{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa620bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: /Users/max/Repos/KTH/pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip subdirectories from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == (\"src\",):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == (\"airquality\",):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == (\"notebooks\",):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir)\n",
    "    print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH`\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from src import config\n",
    "\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3714a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "import hopsworks\n",
    "from src.airquality import util\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74338348",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3fe8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:24:24,049 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-11-16 13:24:24,051 INFO: Initializing external client\n",
      "2025-11-16 13:24:24,051 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:24:25,585 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279179\n",
      "Found AQICN_API_KEY: 17a266deb78b8d8e7b8ad87076443a4e7ee32801\n",
      "Replacing existing AQICN_API_KEY\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    }
   ],
   "source": [
    "today = datetime.date.today()\n",
    "project = hopsworks.login(engine=\"python\")\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# taken from ~/.env. You can also replace settings.AQICN_API_KEY with the api key value as a string \"....\"\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"You need to set AQICN_API_KEY either in this cell or in ~/.env\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "print(f\"Found AQICN_API_KEY: {AQICN_API_KEY}\")\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "# Replace any existing secret with the new value\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "        print(\"Replacing existing AQICN_API_KEY\")\n",
    "except hopsworks.RestAPIError as e:\n",
    "    # Only log the RestAPIError if it is for \"not found\", otherwise re-raise\n",
    "    if hasattr(e, \"error_code\") and getattr(e, \"error_code\", None) == 160048:\n",
    "        # Secret does not exist; proceed to create it.\n",
    "        pass\n",
    "    elif \"Could not find Secret\" in str(e):\n",
    "        # Fallback check if error does not attach error_code attribute\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)\n",
    "\n",
    "\n",
    "# Optional single sensor mode parameters (set in .env or leave None for batch mode)\n",
    "csv_file = getattr(settings, 'SENSOR_CSV_FILE', None)\n",
    "sensor_id_param = getattr(settings, 'SENSOR_ID', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9195e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": 500.0,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "\n",
    "def expect_greater_than_zero(col):\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": 1000.0,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "expect_greater_than_zero(\"precipitation_sum\")\n",
    "expect_greater_than_zero(\"wind_speed_10m_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714bb11",
   "metadata": {},
   "source": [
    "# Backfill Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91460cb4",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sensor_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads the sensor data from the CSV file. The first three rows contains metadata.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        street, city, country = [\n",
    "            s.strip()\n",
    "            for s in f.readline()\n",
    "            .strip()\n",
    "            .lstrip(\"# Sensor \")\n",
    "            .split(\"(\")[0]\n",
    "            .strip()\n",
    "            .split(\",\")\n",
    "        ]\n",
    "        url_line = f.readline().strip().lstrip(\"# \").strip()\n",
    "        sensor_id = url_line.split(\"@\")[1].split(\"/\")[0]\n",
    "        _ = f.readline().strip()\n",
    "    df = pd.read_csv(file_path, skiprows=3)\n",
    "    feed_url = f\"https://api.waqi.info/feed/A{sensor_id}/\"\n",
    "    return df, street, city, country, feed_url, sensor_id\n",
    "\n",
    "\n",
    "def clean_and_append_data(df, street, city, country, feed_url, sensor_id):\n",
    "    \"\"\"\n",
    "    Remove any unused columns, set the daily median value to pm25. Remove NaN's and append the metadata.\n",
    "    \"\"\"\n",
    "    clean_df = pd.DataFrame()\n",
    "    clean_df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    clean_df[\"pm25\"] = df[\"median\"]\n",
    "    clean_df = clean_df.dropna(subset=[\"pm25\"])\n",
    "    clean_df[\"sensor_id\"] = sensor_id\n",
    "    clean_df[\"street\"] = street\n",
    "    clean_df[\"city\"] = city\n",
    "    clean_df[\"country\"] = country\n",
    "    clean_df[\"feed_url\"] = feed_url\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def get_historical_weather(city, df, today, feed_url, sensor_id):\n",
    "    earliest_aq_date = pd.Series.min(df[\"date\"])\n",
    "    earliest_aq_date = earliest_aq_date.strftime(\"%Y-%m-%d\")\n",
    "    response = requests.get(f\"{feed_url}/?token={AQICN_API_KEY}\")\n",
    "    data = response.json()\n",
    "    latitude = data[\"data\"][\"city\"][\"geo\"][0]\n",
    "    longitude = data[\"data\"][\"city\"][\"geo\"][1]\n",
    "    max_retries = 5\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            weather_df = util.get_historical_weather(\n",
    "                city, earliest_aq_date, str(today), latitude, longitude\n",
    "            )\n",
    "            weather_df[\"sensor_id\"] = sensor_id\n",
    "            weather_df[\"city\"] = city\n",
    "            weather_df[\"latitude\"] = latitude\n",
    "            weather_df[\"longitude\"] = longitude\n",
    "            return weather_df, latitude, longitude\n",
    "        except Exception as e:\n",
    "            if hasattr(e, \"args\") and any(\n",
    "                \"Minutely API request limit exceeded\" in str(a) for a in e.args\n",
    "            ):\n",
    "                wait_time = 70\n",
    "                print(\n",
    "                    f\"OpenMeteo API limit exceeded, retrying in {wait_time} seconds... (Attempt {attempt + 1} of {max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            elif \"Minutely API request limit exceeded\" in str(e):\n",
    "                wait_time = 70\n",
    "                print(\n",
    "                    f\"OpenMeteo API limit exceeded, retrying in {wait_time} seconds... (Attempt {attempt + 1} of {max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(\n",
    "        \"Failed to obtain historical weather after multiple retries due to API rate limits.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def add_rolling_window_feature(df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\"):\n",
    "    df = df.sort_values([\"sensor_id\", \"date\"]).copy()\n",
    "    \n",
    "    df_indexed = df.set_index(\"date\", append=False)\n",
    "    \n",
    "    df[new_column] = (\n",
    "        df_indexed.groupby(\"sensor_id\")[column]\n",
    "        .rolling(window=f\"{window_days}D\", min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .values\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719146c0",
   "metadata": {},
   "source": [
    "## Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a0163bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_air_quality_feature_group():\n",
    "    air_quality_fg = fs.get_or_create_feature_group(\n",
    "        name=\"air_quality_all\",\n",
    "        description=\"Air Quality characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        event_time=\"date\",\n",
    "        expectation_suite=aq_expectation_suite,\n",
    "    )\n",
    "    return air_quality_fg\n",
    "\n",
    "\n",
    "def update_air_quality_description(air_quality_fg):\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"date\", \"Date of measurement of air quality\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"country\",\n",
    "        \"Country where the air quality was measured (sometimes a city in aqicn.org)\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"city\", \"City where the air quality was measured\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"street\", \"Street in the city where the air quality was measured\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25\",\n",
    "        \"Particles less than 2.5 micrometers in diameter (fine particles) pose health risk\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_rolling_3d\",\n",
    "        \"3-day rolling mean of PM2.5, backward filled when insufficient history available\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_and_insert_air_quality_data(df):\n",
    "    air_quality_fg = create_air_quality_feature_group()\n",
    "    air_quality_fg.insert(df)\n",
    "    update_air_quality_description(air_quality_fg)\n",
    "\n",
    "\n",
    "def create_weather_feature_group():\n",
    "    weather_fg = fs.get_or_create_feature_group(\n",
    "        name=\"weather_all\",\n",
    "        description=\"Weather characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        event_time=\"date\",\n",
    "        expectation_suite=weather_expectation_suite,\n",
    "    )\n",
    "    return weather_fg\n",
    "\n",
    "\n",
    "def update_weather_description(weather_fg):\n",
    "    weather_fg.update_feature_description(\"date\", \"Date of measurement of weather\")\n",
    "    weather_fg.update_feature_description(\n",
    "        \"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"city\", \"City where weather is measured/forecast for\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"temperature_2m_mean\", \"Temperature in Celsius\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"precipitation_sum\", \"Precipitation (rain/snow) in mm\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"wind_speed_10m_max\", \"Wind speed at 10m abouve ground\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"wind_direction_10m_dominant\", \"Dominant Wind direction over the dayd\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"latitude\", \"Latitude of sensor location used for weather retrieval\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"longitude\", \"Longitude of sensor location used for weather retrieval\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_and_insert_weather_data(df):\n",
    "    weather_fg = create_weather_feature_group()\n",
    "    weather_fg.insert(df)\n",
    "    update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc836b",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7151ca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 20.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 12.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 76.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.75043869018555°N 12.051836013793945°E\n",
      "Elevation 51.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 21.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.609840393066406°N 12.193548202514648°E\n",
      "Elevation 55.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.75043869018555°N 11.857451438903809°E\n",
      "Elevation 7.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 24.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 14.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 72.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 15.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 95.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.75043869018555°N 12.051836013793945°E\n",
      "Elevation 19.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 21.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "Coordinates 57.680137634277344°N 12.025862693786621°E\n",
      "Elevation 27.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid on specified as date, must be a column (of DataFrame), an Index or None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m aq_df_all = pd.concat(all_aq_dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m weather_df_all = pd.concat(all_weather_dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m aq_df_all = \u001b[43madd_rolling_window_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43maq_df_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_days\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpm25\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpm25_rolling_3d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36madd_rolling_window_feature\u001b[39m\u001b[34m(df, window_days, column, new_column)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_rolling_window_feature\u001b[39m(df, window_days=\u001b[32m3\u001b[39m, column=\u001b[33m\"\u001b[39m\u001b[33mpm25\u001b[39m\u001b[33m\"\u001b[39m, new_column=\u001b[33m\"\u001b[39m\u001b[33mpm25_rolling_3d\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     83\u001b[39m     df = df.sort_values([\u001b[33m\"\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m]).copy()\n\u001b[32m     85\u001b[39m     df[new_column] = (\n\u001b[32m     86\u001b[39m         \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msensor_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_days\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m         .mean()\n\u001b[32m     89\u001b[39m         .reset_index(level=\u001b[32m0\u001b[39m, drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     90\u001b[39m     )\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/KTH/pm25-forecast-openmeteo-aqicn/.venv/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:3879\u001b[39m, in \u001b[36mGroupBy.rolling\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3749\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3750\u001b[39m \u001b[33;03mReturn a rolling grouper, providing rolling functionality per group.\u001b[39;00m\n\u001b[32m   3751\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3875\u001b[39m \u001b[33;03m  3  4  0.705\u001b[39;00m\n\u001b[32m   3876\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3877\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwindow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RollingGroupby\n\u001b[32m-> \u001b[39m\u001b[32m3879\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRollingGroupby\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3880\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3881\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3882\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3883\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_as_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3884\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3885\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/KTH/pm25-forecast-openmeteo-aqicn/.venv/lib/python3.11/site-packages/pandas/core/window/rolling.py:713\u001b[39m, in \u001b[36mBaseWindowGroupby.__init__\u001b[39m\u001b[34m(self, obj, _grouper, _as_index, *args, **kwargs)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstep not implemented for groupby\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/KTH/pm25-forecast-openmeteo-aqicn/.venv/lib/python3.11/site-packages/pandas/core/window/rolling.py:164\u001b[39m, in \u001b[36mBaseWindow.__init__\u001b[39m\u001b[34m(self, obj, window, min_periods, center, win_type, axis, on, closed, step, method, selection)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m._on = Index(\u001b[38;5;28mself\u001b[39m.obj[\u001b[38;5;28mself\u001b[39m.on])\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    165\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid on specified as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.on\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmust be a column (of DataFrame), an Index or None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m._selection = selection\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m._validate()\n",
      "\u001b[31mValueError\u001b[39m: invalid on specified as date, must be a column (of DataFrame), an Index or None"
     ]
    }
   ],
   "source": [
    "all_aq_dfs = []\n",
    "all_weather_dfs = []\n",
    "locations = {}\n",
    "\n",
    "if csv_file and os.path.exists(csv_file):\n",
    "    # Single sensor mode if csv_file provided\n",
    "    file_path = csv_file\n",
    "    aq_df_raw, street, city, country, feed_url, sensor_id = read_sensor_data(file_path)\n",
    "    aq_df = clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "    weather_df, latitude, longitude = get_historical_weather(\n",
    "        city, aq_df, today, feed_url, sensor_id\n",
    "    )\n",
    "    all_aq_dfs.append(aq_df)\n",
    "    all_weather_dfs.append(weather_df)\n",
    "    locations[sensor_id] = {\n",
    "        \"country\": country,\n",
    "        \"city\": city,\n",
    "        \"street\": street,\n",
    "        \"aqicn_url\": feed_url,\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "    }\n",
    "else:\n",
    "    # Process all files in data directory\n",
    "    data_dir = os.path.join(root_dir, \"data\")\n",
    "    dir_list = os.listdir(data_dir)\n",
    "    for file in dir_list:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        aq_df_raw, street, city, country, feed_url, sensor_id = read_sensor_data(file_path)\n",
    "        aq_df = clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "        weather_df, latitude, longitude = get_historical_weather(\n",
    "            city, aq_df, today, feed_url, sensor_id\n",
    "        )\n",
    "        all_aq_dfs.append(aq_df)\n",
    "        all_weather_dfs.append(weather_df)\n",
    "        locations[sensor_id] = {\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"street\": street,\n",
    "            \"aqicn_url\": feed_url,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "        }\n",
    "\n",
    "# Concatenate into single, uniform dfs\n",
    "aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "aq_df_all = add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f109bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28200 entries, 14524 to 4050\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype              \n",
      "---  ------           --------------  -----              \n",
      " 0   date             28200 non-null  datetime64[ns, UTC]\n",
      " 1   pm25             28200 non-null  float64            \n",
      " 2   sensor_id        28200 non-null  object             \n",
      " 3   street           28200 non-null  object             \n",
      " 4   city             28200 non-null  object             \n",
      " 5   country          28200 non-null  object             \n",
      " 6   feed_url         28200 non-null  object             \n",
      " 7   pm25_rolling_3d  28200 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(2), object(5)\n",
      "memory usage: 1.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30406 entries, 0 to 30405\n",
      "Data columns (total 9 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         30406 non-null  datetime64[ns]\n",
      " 1   temperature_2m_mean          30406 non-null  float32       \n",
      " 2   precipitation_sum            30406 non-null  float32       \n",
      " 3   wind_speed_10m_max           30406 non-null  float32       \n",
      " 4   wind_direction_10m_dominant  30406 non-null  float32       \n",
      " 5   city                         30406 non-null  object        \n",
      " 6   sensor_id                    30406 non-null  object        \n",
      " 7   latitude                     30406 non-null  float64       \n",
      " 8   longitude                    30406 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float32(4), float64(2), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "aq_df_all.info()\n",
    "weather_df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_59893\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_88372\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_69628\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_61714\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_60541\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_404209\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_194215\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_60535\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_112672\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_154549\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_60853\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_69724\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_65146\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_79750\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Created secret: SENSOR_LOCATION_JSON_59095\n"
     ]
    }
   ],
   "source": [
    "# Create individual secrets for each sensor\n",
    "for sensor_id, location in locations.items():\n",
    "    secret_name = f\"SENSOR_LOCATION_JSON_{sensor_id}\"\n",
    "    location_str = json.dumps(location)\n",
    "    \n",
    "    try:\n",
    "        secret = secrets.get_secret(secret_name)\n",
    "        if secret is not None:\n",
    "            secret.delete()\n",
    "            print(f\"Replacing existing {secret_name}\")\n",
    "    except hopsworks.RestAPIError as e:\n",
    "        if hasattr(e, \"error_code\") and getattr(e, \"error_code\", None) == 160048:\n",
    "            pass\n",
    "        elif \"Could not find Secret\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    secrets.create_secret(secret_name, location_str)\n",
    "    print(f\"Created secret: {secret_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32da907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1279179/fs/1265797/fg/1721709\n",
      "2025-11-16 13:18:09,828 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279179/fs/1265797/fg/1721709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 28200/28200 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_all_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279179/jobs/named/air_quality_all_1_offline_fg_materialization/executions\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1279179/fs/1265797/fg/1721710\n",
      "2025-11-16 13:18:32,747 INFO: \t2 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279179/fs/1265797/fg/1721710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 30406/30406 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_all_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279179/jobs/named/weather_all_1_offline_fg_materialization/executions\n"
     ]
    }
   ],
   "source": [
    "# Insert unified DataFrames into single feature groups\n",
    "create_and_insert_air_quality_data(aq_df_all)\n",
    "create_and_insert_weather_data(weather_df_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
