{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-08 12:13:08,509 INFO: Initializing external client\n",
      "2026-01-08 12:13:08,509 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-08 12:13:09,232 WARNING: UserWarning: The installed hopsworks client version 4.1.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08 12:13:10,276 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in repo at c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, sensor_metadata_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check and Backfill\n",
    "When performed for the first time, might take a long time if many added sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.65s) \n",
      "üìã Found 103 sensors already in feature store\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.56s) \n",
      "üìç Loaded metadata for 103 sensors\n",
      "üìä Total sensors: 103, Already processed: 103, Remaining: 0\n",
      "‚è© Skipping sensor 105325, already in feature store\n",
      "‚è© Skipping sensor 107110, already in feature store\n",
      "‚è© Skipping sensor 112672, already in feature store\n",
      "‚è© ... (suppressing further skip messages)\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "\n",
    "# Get already processed sensors from feature group\n",
    "try:\n",
    "    existing_sensors = set(air_quality_fg.read()[\"sensor_id\"].unique())\n",
    "    print(f\"üìã Found {len(existing_sensors)} sensors already in feature store\")\n",
    "except:\n",
    "    existing_sensors = set()\n",
    "    print(\"üìã No existing sensors found, starting fresh\")\n",
    "\n",
    "# Load all metadata (needed for nearby sensor calculations and location_id lookup)\n",
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) > 0:\n",
    "        # Ensure sensor_id is int type for consistent lookups\n",
    "        metadata_df[\"sensor_id\"] = metadata_df[\"sensor_id\"].astype(int)\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except:\n",
    "    metadata_df = pd.DataFrame()\n",
    "    print(\"üìç No metadata found - please run pipeline 0 first!\")\n",
    "\n",
    "# Count total sensors to process\n",
    "total_sensors = len([f for f in dir_list if f.endswith(\".csv\")])\n",
    "remaining = total_sensors - len(existing_sensors)\n",
    "print(f\"üìä Total sensors: {total_sensors}, Already processed: {len(existing_sensors)}, Remaining: {remaining}\")\n",
    "\n",
    "# Track processing stats\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "failed_sensors = []  # Track which sensors failed and why\n",
    "\n",
    "for file in dir_list:\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    \n",
    "    try:\n",
    "        aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "            file_path, AQICN_API_KEY\n",
    "        )\n",
    "        \n",
    "        # Ensure sensor_id is int for consistent comparison\n",
    "        sensor_id = int(sensor_id)\n",
    "\n",
    "        # Skip if already processed\n",
    "        if sensor_id in existing_sensors:\n",
    "            skipped += 1\n",
    "            if skipped <= 3:  # Only print first 3 to avoid spam\n",
    "                print(f\"‚è© Skipping sensor {sensor_id}, already in feature store\")\n",
    "            elif skipped == 4:\n",
    "                print(f\"‚è© ... (suppressing further skip messages)\")\n",
    "            continue\n",
    "\n",
    "        # Clean AQ\n",
    "        aq_df = cleaning.clean_and_append_data(aq_df_raw, sensor_id)\n",
    "        aq_df = aq_df.sort_values(\"date\")\n",
    "        \n",
    "        # Check for and remove duplicate dates\n",
    "        initial_rows = len(aq_df)\n",
    "        aq_df = aq_df.drop_duplicates(subset=[\"date\"], keep=\"first\")\n",
    "        duplicates_removed = initial_rows - len(aq_df)\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"‚ö†Ô∏è  Sensor {sensor_id}: Removed {duplicates_removed} duplicate date entries\")\n",
    "\n",
    "        aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "        aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "\n",
    "        # Only add nearby sensor feature if we have existing metadata\n",
    "        if len(metadata_df) > 0:\n",
    "            aq_df = feature_engineering.add_nearby_sensor_feature(aq_df, metadata_df, n_closest=3)\n",
    "        else:\n",
    "            # Fill with zeros if no existing sensors to compare against\n",
    "            aq_df[\"pm25_nearby_avg\"] = 0.0\n",
    "\n",
    "        # Compute date range\n",
    "        end_date = aq_df[\"date\"].max().date()\n",
    "        start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "        # Get metadata from sensor_metadata_fg (should always exist now)\n",
    "        if sensor_id in metadata_df.index:\n",
    "            meta = metadata_df.loc[sensor_id]\n",
    "            latitude = meta[\"latitude\"]\n",
    "            longitude = meta[\"longitude\"]\n",
    "            location_id = int(meta[\"location_id\"])\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Sensor {sensor_id} not in metadata. Run pipeline 0 (setup) first to generate metadata.\")\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, \"Missing metadata\"))\n",
    "            continue\n",
    "\n",
    "        # Fetch weather with retry logic\n",
    "        weather_df = None\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_df = fetchers.get_historical_weather(\n",
    "                    location_id, start_date, end_date, latitude, longitude\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except (ConnectionError, ProtocolError, Timeout) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = (attempt + 1) * 5\n",
    "                    print(f\"‚ö†Ô∏è  Network error for sensor {sensor_id}, retrying in {wait_time}s... ({attempt + 1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to fetch weather for sensor {sensor_id} after {max_retries} attempts: {type(e).__name__}\")\n",
    "                    raise\n",
    "\n",
    "        if weather_df is None or len(weather_df) == 0:\n",
    "            print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, \"No weather data\"))\n",
    "            continue\n",
    "\n",
    "        weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "        weather_df[\"location_id\"] = weather_df[\"location_id\"].astype(\"int32\")\n",
    "        weather_fg.insert(weather_df)\n",
    "\n",
    "        # Add location_id to air quality data for joining with weather\n",
    "        aq_df[\"location_id\"] = location_id\n",
    "        aq_df[\"sensor_id\"] = aq_df[\"sensor_id\"].astype(\"int32\")\n",
    "        aq_df[\"location_id\"] = aq_df[\"location_id\"].astype(\"int32\")\n",
    "        aq_columns = [f.name for f in air_quality_fg.features]\n",
    "        aq_df = aq_df[aq_columns]\n",
    "        air_quality_fg.insert(aq_df)\n",
    "\n",
    "        # Add to processed set\n",
    "        existing_sensors.add(sensor_id)\n",
    "        successful += 1\n",
    "\n",
    "        print(f\"‚úÖ Inserted sensor {sensor_id} ({successful}/{remaining} complete, {failed} failed)\")\n",
    "\n",
    "    except (ConnectionError, ProtocolError, Timeout) as e:\n",
    "        failed += 1\n",
    "        error_msg = f\"Network error: {type(e).__name__}\"\n",
    "        failed_sensors.append((sensor_id, error_msg))\n",
    "        print(f\"‚ùå {error_msg} for sensor {sensor_id}\")\n",
    "        print(f\"   Continuing with next sensor... ({successful} successful, {failed} failed, {skipped} skipped)\")\n",
    "        continue\n",
    "        \n",
    "    except RequestException as e:\n",
    "        failed += 1\n",
    "        error_msg = f\"Request error: {type(e).__name__}\"\n",
    "        failed_sensors.append((sensor_id, error_msg))\n",
    "        print(f\"‚ùå {error_msg} for sensor {sensor_id}\")\n",
    "        print(f\"   Continuing with next sensor... ({successful} successful, {failed} failed, {skipped} skipped)\")\n",
    "        continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)[:50]}\"\n",
    "        failed_sensors.append((sensor_id, error_msg))\n",
    "        print(f\"‚ùå Unexpected error processing sensor {sensor_id}: {type(e).__name__} - {str(e)[:100]}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback: {traceback.format_exc()[:200]}\")\n",
    "        print(f\"   Continuing with next sensor... ({successful} successful, {failed} failed, {skipped} skipped)\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ Backfill complete!\")\n",
    "print(f\"üìä Final Summary:\")\n",
    "print(f\"   ‚úÖ Successfully processed: {successful}\")\n",
    "print(f\"   ‚ùå Failed: {failed}\")\n",
    "print(f\"   ‚è© Skipped (already processed): {skipped}\")\n",
    "print(f\"   üìà Total in feature store: {len(existing_sensors)}/{total_sensors}\")\n",
    "\n",
    "if len(failed_sensors) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed Sensors Detail:\")\n",
    "    for sid, reason in failed_sensors:\n",
    "        print(f\"   ‚Ä¢ Sensor {sid}: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481debe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if data exists\n",
    "# try:\n",
    "#     aq_data = air_quality_fg.read()\n",
    "#     is_first_run = len(aq_data) == 0\n",
    "# except:\n",
    "#     is_first_run = True\n",
    "\n",
    "# # Process and insert data if first run\n",
    "# if is_first_run:\n",
    "#     # all_aq_dfs = []\n",
    "#     # all_weather_dfs = []\n",
    "#     locations = {}\n",
    "\n",
    "#     # Process CSV files in data directory\n",
    "#     data_dir = os.path.join(root_dir, \"data\")\n",
    "#     dir_list = os.listdir(data_dir)\n",
    "#     metadata_df = sensor_metadata_fg.read().set_index(\"sensor_id\")\n",
    "#     for file in dir_list:\n",
    "#         if file.endswith(\".csv\"):\n",
    "#             file_path = os.path.join(data_dir, file)\n",
    "#             aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(file_path, AQICN_API_KEY)\n",
    "            \n",
    "#             # Clean and process\n",
    "#             aq_df = cleaning.clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "#             aq_df[\"date\"] = aq_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "#             # start_date = aq_df[\"date\"].min().date()\n",
    "#             end_date = aq_df[\"date\"].max().date()\n",
    "#             start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "#             meta = metadata_df.loc[sensor_id]\n",
    "#             latitude = meta[\"latitude\"]\n",
    "#             longitude = meta[\"longitude\"]\n",
    "#             city = meta[\"city\"]\n",
    "\n",
    "#             weather_df = fetchers.get_historical_weather(city, start_date, end_date, latitude, longitude)\n",
    "\n",
    "#             if weather_df is None or len(weather_df) == 0:\n",
    "#                 print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "#             weather_fg.insert(weather_df)\n",
    "#             air_quality_fg.insert(aq_df)\n",
    "\n",
    "#             # all_aq_dfs.append(aq_df)\n",
    "#             # all_weather_dfs.append(weather_df)\n",
    "#             locations[sensor_id] = {\n",
    "#                 \"country\": country,\n",
    "#                 \"city\": city,\n",
    "#                 \"street\": street,\n",
    "#                 \"aqicn_url\": feed_url,\n",
    "#                 \"latitude\": latitude,\n",
    "#                 \"longitude\": longitude,\n",
    "#             }\n",
    "\n",
    "#             print(f\"‚úÖ Inserted sensor {sensor_id}\")\n",
    "#     print(\"‚úÖ Backfill complete.\")\n",
    "\n",
    "\n",
    "#     # if all_aq_dfs:\n",
    "#     #     # Combine and engineer features\n",
    "#     #     aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "#     #     weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "\n",
    "#     #     aq_df_all = feature_engineering.add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#     #     aq_df_all = feature_engineering.add_lagged_features(aq_df_all, column=\"pm25\", lags=[1, 2, 3])\n",
    "#     #     aq_df_all = feature_engineering.add_nearby_sensor_feature(aq_df_all, locations, column=\"pm25_lag_1d\", n_closest=3)\n",
    "        \n",
    "#     #     air_quality_fg.insert(aq_df_all)\n",
    "#     #     weather_fg.insert(weather_df_all)\n",
    "\n",
    "#     #     # Insert sensor metadata\n",
    "#     #     metadata_records = []\n",
    "#     #     for sensor_id, loc in locations.items():\n",
    "#     #         metadata_records.append({\n",
    "#     #             \"sensor_id\": sensor_id,\n",
    "#     #             \"country\": loc[\"country\"],\n",
    "#     #             \"city\": loc[\"city\"],\n",
    "#     #             \"street\": loc[\"street\"],\n",
    "#     #             \"aqicn_url\": loc[\"aqicn_url\"],\n",
    "#     #             \"latitude\": loc[\"latitude\"],\n",
    "#     #             \"longitude\": loc[\"longitude\"],\n",
    "#     #         })\n",
    "#     #     sensor_metadata_fg.insert(pd.DataFrame(metadata_records))\n",
    "    \n",
    "#     #     print(f\"‚úÖ Inserted {len(aq_df_all)} air quality records\")\n",
    "#     #     print(f\"‚úÖ Inserted {len(weather_df_all)} weather records\")\n",
    "#     #     print(f\"‚úÖ Inserted {len(metadata_records)} sensor metadata records\")\n",
    "#     # else:\n",
    "#     #     print(\"‚ö†Ô∏è No CSV files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.4. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "hopsworks_admin.update_sensor_metadata_description(sensor_metadata_fg)\n",
    "hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.5. Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\"column\": col, \"type_list\": [\"float\", \"int\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# temperature should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"temperature_2m_mean\", \"min_value\": -80, \"max_value\": 60},\n",
    "    )\n",
    ")\n",
    "\n",
    "# # latitude/longitude must be valid\n",
    "# weather_expectation_suite.add_expectation(\n",
    "#     gx.core.ExpectationConfiguration(\n",
    "#         expectation_type=\"expect_column_values_to_be_between\",\n",
    "#         kwargs={\"column\": \"latitude\", \"min_value\": -90, \"max_value\": 90},\n",
    "#     )\n",
    "# )\n",
    "# weather_expectation_suite.add_expectation(\n",
    "#     gx.core.ExpectationConfiguration(\n",
    "#         expectation_type=\"expect_column_values_to_be_between\",\n",
    "#         kwargs={\"column\": \"longitude\", \"min_value\": -180, \"max_value\": 180},\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# precipitation and wind speed should be >= 0 (but allow nulls)\n",
    "for col in [\"precipitation_sum\", \"wind_speed_10m_max\"]:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": None,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.6. Create Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select_all(), on=[\"location_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca29e7",
   "metadata": {},
   "source": [
    "## 1.7. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No sensor metadata found. Run first-time CSV processing first.\")\n",
    "    else:\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading sensor metadata: {e}\")\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "historical_df = air_quality_fv.get_batch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d7a56",
   "metadata": {},
   "source": [
    "## 1.8. Incremental Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2e9a1",
   "metadata": {},
   "source": [
    "Detect latest timestamp per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_per_sensor = (\n",
    "    historical_df.groupby(\"sensor_id\")[\"date\"]\n",
    "    .max()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "incremental.run_incremental_update(\n",
    "    sensor_metadata_fg,\n",
    "    air_quality_fg,\n",
    "    weather_fg,\n",
    "    latest_per_sensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {aq_df_all.shape}\")\n",
    "print(f\"Date range: {aq_df_all['date'].min().date()} to {aq_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {aq_df_all['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {aq_df_all['country'].unique()}\")\n",
    "print(f\"Cities: {aq_df_all['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(aq_df_all['pm25'].describe())\n",
    "print(f\"Missing values: {aq_df_all['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in aq_df_all.columns:\n",
    "        missing = aq_df_all[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(aq_df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {weather_df_all.shape}\")\n",
    "print(f\"Date range: {weather_df_all['date'].min().date()} to {weather_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {weather_df_all['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df_all.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df_all[col].min():.2f} to {weather_df_all[col].max():.2f}, Mean: {weather_df_all[col].mean():.2f}, Missing: {weather_df_all[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {weather_df_all['latitude'].min():.3f} to {weather_df_all['latitude'].max():.3f}, Longitude range: {weather_df_all['longitude'].min():.3f} to {weather_df_all['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = aq_df_all.groupby('sensor_id')['date'].count()\n",
    "total_records = len(aq_df_all)\n",
    "data_completeness = (1 - aq_df_all['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (aq_df_all['pm25'] > 100).sum()\n",
    "very_high_count = (aq_df_all['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(aq_df_all) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(aq_df_all['date']).dt.month\n",
    "    monthly_pm25 = aq_df_all.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
