{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae8d7e",
   "metadata": {},
   "source": [
    "## 1.1. Environment Setup\n",
    "Handle repository cloning, dependency installation, and set up Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b485ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import hopsworks\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    repo_dir = Path(\"pm25-forecast-openmeteo-aqicn\")\n",
    "    if repo_dir.exists():\n",
    "        print(f\"Repository already exists at {repo_dir.absolute()}\")\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/KristinaPalmquist/pm25-forecast-openmeteo-aqicn.git\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "for folder in (\"src\", \"airquality\", \"notebooks\"):\n",
    "    if root_dir.parts[-1:] == (folder,):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "root_dir = str(root_dir)\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "from utils import config\n",
    "\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "project = hopsworks.login(engine=\"python\", api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad128c99",
   "metadata": {},
   "source": [
    "## 1.2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3649091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "import hopsworks\n",
    "from utils import airquality\n",
    "importlib.reload(airquality)\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322e26a",
   "metadata": {},
   "source": [
    "## 1.3. Setup\n",
    "Configure Hopsworks connection, feature store access, and AQICN API key handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad12486",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"You need to set AQICN_API_KEY either in this cell or in ~/.env\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "        print(\"Replacing existing AQICN_API_KEY\")\n",
    "except hopsworks.RestAPIError as e:\n",
    "    if hasattr(e, \"error_code\") and getattr(e, \"error_code\", None) == 160048:\n",
    "        pass\n",
    "    elif \"Could not find Secret\" in str(e):\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fff50",
   "metadata": {},
   "source": [
    "## 1.4. Processing Mode\n",
    "This notebook processes all sensors that have CSV files in the `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1641ed",
   "metadata": {},
   "source": [
    "## 1.5. Data Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "aq_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": 500.0,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "def expect_greater_than_zero(col):\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": 1000.0,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "expect_greater_than_zero(\"precipitation_sum\")\n",
    "expect_greater_than_zero(\"wind_speed_10m_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68ea8b",
   "metadata": {},
   "source": [
    "## 1.6. Helper Methods\n",
    "Data processing functions - clean air quality data and fetch historical weather data with API rate limiting and retry logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_append_data(df, street, city, country, feed_url, sensor_id):\n",
    "    \"\"\"\n",
    "    Remove any unused columns, set the daily median value to pm25. Remove NaN's and append the metadata.\n",
    "    \"\"\"\n",
    "    clean_df = pd.DataFrame()\n",
    "    clean_df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    clean_df[\"pm25\"] = df[\"median\"]\n",
    "    clean_df = clean_df.dropna(subset=[\"pm25\"])\n",
    "    clean_df[\"sensor_id\"] = sensor_id\n",
    "    clean_df[\"street\"] = street\n",
    "    clean_df[\"city\"] = city\n",
    "    clean_df[\"country\"] = country\n",
    "    clean_df[\"feed_url\"] = feed_url\n",
    "    return clean_df\n",
    "\n",
    "def get_historical_weather(city, df, today, feed_url, sensor_id):\n",
    "    earliest_aq_date = pd.Series.min(df[\"date\"])\n",
    "    earliest_aq_date = earliest_aq_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    response = requests.get(f\"{feed_url}/?token={AQICN_API_KEY}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Handle AQICN API error status or missing city\n",
    "    if (\"status\" in data.get(\"data\", {}) and data[\"data\"][\"status\"] == \"error\") or \"city\" not in data.get(\"data\", {}):\n",
    "        print(f\"Skipping sensor {sensor_id}: AQICN API error or unknown ID. Response: {data}\")\n",
    "        return None, None, None \n",
    "    try:\n",
    "        latitude, longitude = airquality.get_sensor_coordinates(feed_url, sensor_id, AQICN_API_KEY)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to get coordinates for sensor {sensor_id}: {e}\")\n",
    "    \n",
    "    max_retries = 5\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            weather_df = airquality.get_historical_weather(\n",
    "                city, earliest_aq_date, str(today), latitude, longitude\n",
    "            )\n",
    "            weather_df[\"sensor_id\"] = sensor_id\n",
    "            weather_df[\"city\"] = city\n",
    "            weather_df[\"latitude\"] = latitude\n",
    "            weather_df[\"longitude\"] = longitude\n",
    "            return weather_df, latitude, longitude\n",
    "        except Exception as e:\n",
    "            if hasattr(e, \"args\") and any(\n",
    "                \"Minutely API request limit exceeded\" in str(a) for a in e.args\n",
    "            ):\n",
    "                wait_time = 70\n",
    "                print(\n",
    "                    f\"OpenMeteo API limit exceeded, retrying in {wait_time} seconds... (Attempt {attempt + 1} of {max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            elif \"Minutely API request limit exceeded\" in str(e):\n",
    "                wait_time = 70\n",
    "                print(\n",
    "                    f\"OpenMeteo API limit exceeded, retrying in {wait_time} seconds... (Attempt {attempt + 1} of {max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(\n",
    "        \"Failed to obtain historical weather after multiple retries due to API rate limits.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.7. Hopsworks\n",
    "Feature Group Management - functions to create and manage air quality and weather feature groups in Hopsworks, including schema descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_air_quality_feature_group():\n",
    "    air_quality_fg = fs.get_or_create_feature_group(\n",
    "        name=\"air_quality_all\",\n",
    "        description=\"Air Quality characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        event_time=\"date\",\n",
    "        expectation_suite=aq_expectation_suite,\n",
    "    )\n",
    "    return air_quality_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicates(new_df=None):\n",
    "#     \"\"\"\n",
    "#     Remove duplicate entries from the Hopsworks feature group, keeping only the latest for each (sensor_id, date).\n",
    "#     If new_df is provided, also remove any rows from new_df that would duplicate (sensor_id, date) already present in the feature group.\n",
    "#     Returns a deduplicated DataFrame for upload (if new_df is provided), else None.\n",
    "#     \"\"\"\n",
    "#     # Try to read from the feature group, handle empty case\n",
    "#     try:\n",
    "#         df = air_quality_fg.read()\n",
    "#         feature_group_empty = df.empty\n",
    "#     except Exception as e:\n",
    "#         print(\"Feature group is empty or cannot be read. Skipping FG deduplication.\")\n",
    "#         feature_group_empty = True\n",
    "#         df = None\n",
    "\n",
    "#     # Deduplicate new_df in-memory before upload\n",
    "#     if new_df is not None:\n",
    "#         # Remove duplicates within new_df itself (by sensor_id and date)\n",
    "#         new_df = new_df.sort_values(['sensor_id', 'date'])\n",
    "#         new_df = new_df.drop_duplicates(subset=['sensor_id', 'date'], keep='last')\n",
    "#         if not feature_group_empty:\n",
    "#             # Remove rows from new_df that already exist in the feature group\n",
    "#             df_keys = df[['sensor_id', 'date']].drop_duplicates()\n",
    "#             new_df = new_df.copy()\n",
    "#             df_keys = df_keys.copy()\n",
    "#             # Ensure both are timezone-naive\n",
    "#             new_df['date'] = pd.to_datetime(new_df['date']).dt.tz_localize(None)\n",
    "#             df_keys['date'] = pd.to_datetime(df_keys['date']).dt.tz_localize(None)\n",
    "#             merged = new_df.merge(df_keys, on=['sensor_id', 'date'], how='left', indicator=True)\n",
    "#             deduped_new_df = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "#             print(f\"Filtered out {len(new_df) - len(deduped_new_df)} rows from new data that already exist in feature group.\")\n",
    "#             new_df = deduped_new_df\n",
    "#         # Final check: ensure no duplicates remain in new_df\n",
    "#         final_dupes = new_df.duplicated(subset=['sensor_id', 'date'], keep=False)\n",
    "#         if final_dupes.any():\n",
    "#             print(\"Warning: Duplicates still present in new_df after deduplication. Removing all but the last occurrence.\")\n",
    "#             new_df = new_df.drop_duplicates(subset=['sensor_id', 'date'], keep='last')\n",
    "#         return new_df\n",
    "\n",
    "#     # If feature group is not empty, deduplicate it\n",
    "#     if not feature_group_empty:\n",
    "#         df_sorted = df.sort_values(['sensor_id', 'date'])\n",
    "#         mask = df_sorted.duplicated(subset=['sensor_id', 'date'], keep='last')\n",
    "#         duplicates = df_sorted[mask]\n",
    "#         print(f\"Found {len(duplicates)} duplicate rows to delete in feature group.\")\n",
    "#         for _, row in duplicates.iterrows():\n",
    "#             air_quality_fg.delete_record({\"sensor_id\": row[\"sensor_id\"], \"date\": row[\"date\"]})\n",
    "#         print(\"Duplicate rows deleted from feature group.\")\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_air_quality_description(air_quality_fg):\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"date\", \"Date of measurement of air quality\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"country\",\n",
    "        \"Country where the air quality was measured (sometimes a city in aqicn.org)\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"city\", \"City where the air quality was measured\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"street\", \"Street in the city where the air quality was measured\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25\",\n",
    "        \"Particles less than 2.5 micrometers in diameter (fine particles) pose health risk\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_rolling_3d\",\n",
    "        \"3-day rolling mean of PM2.5 from previous days (lagged by 1 day for point-in-time correctness).\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_lag_1d\",\n",
    "        \"PM2.5 value from 1 day ago.\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_lag_2d\",\n",
    "        \"PM2.5 value from 2 days ago.\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_lag_3d\",\n",
    "        \"PM2.5 value from 3 days ago.\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_nearby_avg\",\n",
    "        \"Average PM2.5 value from the 3 closest sensors.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_insert_air_quality_data(df):\n",
    "    air_quality_fg = create_air_quality_feature_group()\n",
    "    air_quality_fg.insert(df)\n",
    "    update_air_quality_description(air_quality_fg)\n",
    "    # # remove_duplicates()\n",
    "    # deduped_df = remove_duplicates(df)\n",
    "    # # Check for dict-typed columns\n",
    "    # for col in deduped_df.columns:\n",
    "    #     if deduped_df[col].apply(lambda x: isinstance(x, dict)).any():\n",
    "    #         print(f\"Warning: Column '{col}' contains dict values!\")\n",
    "    # # air_quality_fg.insert(deduped_df)\n",
    "    # update_air_quality_description(air_quality_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_feature_group():\n",
    "    weather_fg = fs.get_or_create_feature_group(\n",
    "        name=\"weather_all\",\n",
    "        description=\"Weather characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        event_time=\"date\",\n",
    "        expectation_suite=weather_expectation_suite,\n",
    "    )\n",
    "    return weather_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weather_description(weather_fg):\n",
    "    weather_fg.update_feature_description(\"date\", \"Date of measurement of weather\")\n",
    "    weather_fg.update_feature_description(\n",
    "        \"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"city\", \"City where weather is measured/forecast for\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"temperature_2m_mean\", \"Temperature in Celsius\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"precipitation_sum\", \"Precipitation (rain/snow) in mm\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"wind_speed_10m_max\", \"Wind speed at 10m abouve ground\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"wind_direction_10m_dominant\", \"Dominant Wind direction over the dayd\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"latitude\", \"Latitude of sensor location used for weather retrieval\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"longitude\", \"Longitude of sensor location used for weather retrieval\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_insert_weather_data(df):\n",
    "    weather_fg = create_weather_feature_group()\n",
    "    weather_fg.insert(df)\n",
    "    update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60347210",
   "metadata": {},
   "source": [
    "## 1.8. Script\n",
    "Main processing logic - processes all sensors in the data folder, cleans data, fetches weather data, adds rolling averages and lagged features, and combines all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aq_dfs = []\n",
    "all_weather_dfs = []\n",
    "locations = {}\n",
    "\n",
    "# Process all files in data directory\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "for file in dir_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        aq_df_raw, street, city, country, feed_url, sensor_id = airquality.read_sensor_data(file_path, AQICN_API_KEY)\n",
    "        aq_df = clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "        weather_df, latitude, longitude = get_historical_weather(\n",
    "            city, aq_df, today, feed_url, sensor_id\n",
    "        )\n",
    "        all_aq_dfs.append(aq_df)\n",
    "        all_weather_dfs.append(weather_df)\n",
    "        locations[sensor_id] = {\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"street\": street,\n",
    "            \"aqicn_url\": feed_url,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "        }\n",
    "\n",
    "# Concatenate into single, uniform dfs\n",
    "aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "aq_df_all = airquality.add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "aq_df_all = airquality.add_lagged_features(aq_df_all, column=\"pm25\", lags=[1, 2, 3])\n",
    "aq_df_all = airquality.add_nearby_sensor_feature(aq_df_all, locations, column=\"pm25_lag_1d\", n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {aq_df_all.shape}\")\n",
    "print(f\"Date range: {aq_df_all['date'].min().date()} to {aq_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {aq_df_all['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {aq_df_all['country'].unique()}\")\n",
    "print(f\"Cities: {aq_df_all['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(aq_df_all['pm25'].describe())\n",
    "print(f\"Missing values: {aq_df_all['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in aq_df_all.columns:\n",
    "        missing = aq_df_all[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(aq_df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {weather_df_all.shape}\")\n",
    "print(f\"Date range: {weather_df_all['date'].min().date()} to {weather_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {weather_df_all['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df_all.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df_all[col].min():.2f} to {weather_df_all[col].max():.2f}, Mean: {weather_df_all[col].mean():.2f}, Missing: {weather_df_all[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {weather_df_all['latitude'].min():.3f} to {weather_df_all['latitude'].max():.3f}, Longitude range: {weather_df_all['longitude'].min():.3f} to {weather_df_all['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = aq_df_all.groupby('sensor_id')['date'].count()\n",
    "total_records = len(aq_df_all)\n",
    "data_completeness = (1 - aq_df_all['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (aq_df_all['pm25'] > 100).sum()\n",
    "very_high_count = (aq_df_all['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(aq_df_all) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(aq_df_all['date']).dt.month\n",
    "    monthly_pm25 = aq_df_all.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3415e",
   "metadata": {},
   "source": [
    "## 1.10. Store Sensor Location\n",
    "Create Hopsworks secrets for each sensor's location metadata (coordinates, address, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfef0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id, location in locations.items():\n",
    "    secret_name = f\"SENSOR_LOCATION_JSON_{sensor_id}\"\n",
    "    location_str = json.dumps(location)\n",
    "    \n",
    "    try:\n",
    "        secret = secrets.get_secret(secret_name)\n",
    "        if secret is not None:\n",
    "            secret.delete()\n",
    "    except hopsworks.RestAPIError as e:\n",
    "        if hasattr(e, \"error_code\") and getattr(e, \"error_code\", None) == 160048:\n",
    "            pass\n",
    "        elif \"Could not find Secret\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    secrets.create_secret(secret_name, location_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82031a",
   "metadata": {},
   "source": [
    "## 1.11. Upload to Hopsworks\n",
    "Insert the processed air quality and weather data into Hopsworks feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f08d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_insert_air_quality_data(aq_df_all)\n",
    "create_and_insert_weather_data(weather_df_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
