{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-08 07:36:49,680 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-08 07:36:49,686 INFO: Initializing external client\n",
      "2026-01-08 07:36:49,687 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-08 07:36:50,328 WARNING: UserWarning: The installed hopsworks client version 4.1.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08 07:36:51,228 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in repo at c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, sensor_metadata_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check and Backfill\n",
    "Only performed when done for the first time. \n",
    "\n",
    "For 100 sensors and 5 years this will take approximately 150 minutes = 2.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Hopsworks, using Hopsworks Feature Query Service...   \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(root_dir, \"checkpoint.json\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    processed_sensors = set(json.load(open(checkpoint_path)))\n",
    "else:\n",
    "    processed_sensors = set()\n",
    "\n",
    "# Check if data exists\n",
    "try:\n",
    "    aq_data = air_quality_fg.read()\n",
    "    is_first_run = len(aq_data) == 0\n",
    "except:\n",
    "    is_first_run = True\n",
    "\n",
    "if is_first_run:\n",
    "    locations = {}\n",
    "\n",
    "    data_dir = os.path.join(root_dir, \"data\")\n",
    "    dir_list = os.listdir(data_dir)\n",
    "    metadata_df = sensor_metadata_fg.read().set_index(\"sensor_id\")\n",
    "\n",
    "    for file in dir_list:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "            file_path, AQICN_API_KEY\n",
    "        )\n",
    "\n",
    "        # Skip if already processed\n",
    "        if sensor_id in processed_sensors:\n",
    "            print(f\"‚è© Skipping sensor {sensor_id}, already processed\")\n",
    "            continue\n",
    "\n",
    "        # Clean AQ\n",
    "        aq_df = cleaning.clean_and_append_data(aq_df_raw, sensor_id)\n",
    "        aq_df = aq_df.sort_values(\"date\")\n",
    "\n",
    "        aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "        aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "\n",
    "        aq_df = feature_engineering.add_nearby_sensor_feature(aq_df, metadata_df, n_closest=3)\n",
    "\n",
    "\n",
    "        # Compute date range\n",
    "        end_date = aq_df[\"date\"].max().date()\n",
    "        start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "        # Metadata\n",
    "        meta = metadata_df.loc[sensor_id]\n",
    "        latitude = meta[\"latitude\"]\n",
    "        longitude = meta[\"longitude\"]\n",
    "        city = meta[\"city\"]\n",
    "        location_id = meta[\"location_id\"]\n",
    "\n",
    "        # Fetch weather\n",
    "        weather_df = fetchers.get_historical_weather(\n",
    "            location_id, start_date, end_date, latitude, longitude\n",
    "        )\n",
    "\n",
    "        if weather_df is None or len(weather_df) == 0:\n",
    "            print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "        weather_df[\"location_id\"] = weather_df[\"location_id\"].astype(\"int32\")\n",
    "        weather_fg.insert(weather_df)\n",
    "\n",
    "\n",
    "        aq_df[\"sensor_id\"] = aq_df[\"sensor_id\"].astype(\"int32\")\n",
    "        aq_columns = [f.name for f in air_quality_fg.features]\n",
    "        aq_df = aq_df[aq_columns]\n",
    "        air_quality_fg.insert(aq_df)\n",
    "\n",
    "        sensor_metadata_fg.insert(pd.DataFrame([{\n",
    "            \"sensor_id\": sensor_id,\n",
    "            \"location_id\": location_id,\n",
    "            \"country\": country,\n",
    "            \"city\": city,\n",
    "            \"street\": street,\n",
    "            \"aqicn_url\": feed_url,\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "        }]))\n",
    "\n",
    "        # Save checkpoint\n",
    "        processed_sensors.add(sensor_id)\n",
    "        json.dump(list(processed_sensors), open(checkpoint_path, \"w\"))\n",
    "\n",
    "        print(f\"‚úÖ Inserted sensor {sensor_id}\")\n",
    "\n",
    "    print(\"üéâ Backfill complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481debe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if data exists\n",
    "# try:\n",
    "#     aq_data = air_quality_fg.read()\n",
    "#     is_first_run = len(aq_data) == 0\n",
    "# except:\n",
    "#     is_first_run = True\n",
    "\n",
    "# # Process and insert data if first run\n",
    "# if is_first_run:\n",
    "#     # all_aq_dfs = []\n",
    "#     # all_weather_dfs = []\n",
    "#     locations = {}\n",
    "\n",
    "#     # Process CSV files in data directory\n",
    "#     data_dir = os.path.join(root_dir, \"data\")\n",
    "#     dir_list = os.listdir(data_dir)\n",
    "#     metadata_df = sensor_metadata_fg.read().set_index(\"sensor_id\")\n",
    "#     for file in dir_list:\n",
    "#         if file.endswith(\".csv\"):\n",
    "#             file_path = os.path.join(data_dir, file)\n",
    "#             aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(file_path, AQICN_API_KEY)\n",
    "            \n",
    "#             # Clean and process\n",
    "#             aq_df = cleaning.clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "#             aq_df[\"date\"] = aq_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "#             # start_date = aq_df[\"date\"].min().date()\n",
    "#             end_date = aq_df[\"date\"].max().date()\n",
    "#             start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "#             meta = metadata_df.loc[sensor_id]\n",
    "#             latitude = meta[\"latitude\"]\n",
    "#             longitude = meta[\"longitude\"]\n",
    "#             city = meta[\"city\"]\n",
    "\n",
    "#             weather_df = fetchers.get_historical_weather(city, start_date, end_date, latitude, longitude)\n",
    "\n",
    "#             if weather_df is None or len(weather_df) == 0:\n",
    "#                 print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "#             weather_fg.insert(weather_df)\n",
    "#             air_quality_fg.insert(aq_df)\n",
    "\n",
    "#             # all_aq_dfs.append(aq_df)\n",
    "#             # all_weather_dfs.append(weather_df)\n",
    "#             locations[sensor_id] = {\n",
    "#                 \"country\": country,\n",
    "#                 \"city\": city,\n",
    "#                 \"street\": street,\n",
    "#                 \"aqicn_url\": feed_url,\n",
    "#                 \"latitude\": latitude,\n",
    "#                 \"longitude\": longitude,\n",
    "#             }\n",
    "\n",
    "#             print(f\"‚úÖ Inserted sensor {sensor_id}\")\n",
    "#     print(\"‚úÖ Backfill complete.\")\n",
    "\n",
    "\n",
    "#     # if all_aq_dfs:\n",
    "#     #     # Combine and engineer features\n",
    "#     #     aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "#     #     weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "\n",
    "#     #     aq_df_all = feature_engineering.add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#     #     aq_df_all = feature_engineering.add_lagged_features(aq_df_all, column=\"pm25\", lags=[1, 2, 3])\n",
    "#     #     aq_df_all = feature_engineering.add_nearby_sensor_feature(aq_df_all, locations, column=\"pm25_lag_1d\", n_closest=3)\n",
    "        \n",
    "#     #     air_quality_fg.insert(aq_df_all)\n",
    "#     #     weather_fg.insert(weather_df_all)\n",
    "\n",
    "#     #     # Insert sensor metadata\n",
    "#     #     metadata_records = []\n",
    "#     #     for sensor_id, loc in locations.items():\n",
    "#     #         metadata_records.append({\n",
    "#     #             \"sensor_id\": sensor_id,\n",
    "#     #             \"country\": loc[\"country\"],\n",
    "#     #             \"city\": loc[\"city\"],\n",
    "#     #             \"street\": loc[\"street\"],\n",
    "#     #             \"aqicn_url\": loc[\"aqicn_url\"],\n",
    "#     #             \"latitude\": loc[\"latitude\"],\n",
    "#     #             \"longitude\": loc[\"longitude\"],\n",
    "#     #         })\n",
    "#     #     sensor_metadata_fg.insert(pd.DataFrame(metadata_records))\n",
    "    \n",
    "#     #     print(f\"‚úÖ Inserted {len(aq_df_all)} air quality records\")\n",
    "#     #     print(f\"‚úÖ Inserted {len(weather_df_all)} weather records\")\n",
    "#     #     print(f\"‚úÖ Inserted {len(metadata_records)} sensor metadata records\")\n",
    "#     # else:\n",
    "#     #     print(\"‚ö†Ô∏è No CSV files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.4. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "hopsworks_admin.update_sensor_metadata_description(sensor_metadata_fg)\n",
    "hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.5. Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\"column\": col, \"type_list\": [\"float\", \"int\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# temperature should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"temperature_2m_mean\", \"min_value\": -80, \"max_value\": 60},\n",
    "    )\n",
    ")\n",
    "\n",
    "# latitude/longitude must be valid\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"latitude\", \"min_value\": -90, \"max_value\": 90},\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"longitude\", \"min_value\": -180, \"max_value\": 180},\n",
    "    )\n",
    ")\n",
    "\n",
    "# precipitation and wind speed should be >= 0 (but allow nulls)\n",
    "for col in [\"precipitation_sum\", \"wind_speed_10m_max\"]:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": None,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.6. Create Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select_all(), on=[\"sensor_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca29e7",
   "metadata": {},
   "source": [
    "## 1.7. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No sensor metadata found. Run first-time CSV processing first.\")\n",
    "    else:\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading sensor metadata: {e}\")\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "historical_df = air_quality_fv.get_batch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d7a56",
   "metadata": {},
   "source": [
    "## 1.8. Incremental Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2e9a1",
   "metadata": {},
   "source": [
    "Detect latest timestamp per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_per_sensor = (\n",
    "    historical_df.groupby(\"sensor_id\")[\"date\"]\n",
    "    .max()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "incremental.run_incremental_update(\n",
    "    sensor_metadata_fg,\n",
    "    air_quality_fg,\n",
    "    weather_fg,\n",
    "    latest_per_sensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {aq_df_all.shape}\")\n",
    "print(f\"Date range: {aq_df_all['date'].min().date()} to {aq_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {aq_df_all['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {aq_df_all['country'].unique()}\")\n",
    "print(f\"Cities: {aq_df_all['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(aq_df_all['pm25'].describe())\n",
    "print(f\"Missing values: {aq_df_all['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in aq_df_all.columns:\n",
    "        missing = aq_df_all[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(aq_df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {weather_df_all.shape}\")\n",
    "print(f\"Date range: {weather_df_all['date'].min().date()} to {weather_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {weather_df_all['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df_all.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df_all[col].min():.2f} to {weather_df_all[col].max():.2f}, Mean: {weather_df_all[col].mean():.2f}, Missing: {weather_df_all[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {weather_df_all['latitude'].min():.3f} to {weather_df_all['latitude'].max():.3f}, Longitude range: {weather_df_all['longitude'].min():.3f} to {weather_df_all['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = aq_df_all.groupby('sensor_id')['date'].count()\n",
    "total_records = len(aq_df_all)\n",
    "data_completeness = (1 - aq_df_all['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (aq_df_all['pm25'] > 100).sum()\n",
    "very_high_count = (aq_df_all['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(aq_df_all) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(aq_df_all['date']).dt.month\n",
    "    monthly_pm25 = aq_df_all.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
