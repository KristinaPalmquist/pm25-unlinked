{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-07 11:25:21,385 INFO: Initializing external client\n",
      "2026-01-07 11:25:21,385 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-07 11:25:22,135 WARNING: UserWarning: The installed hopsworks client version 4.1.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-07 11:25:23,201 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5d0ef",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in repo at c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "def clone_or_update_repo(username: str):\n",
    "    repo_name = \"pm25-forecast-openmeteo-aqicn\"\n",
    "\n",
    "    # 1. Detect if already inside the repo\n",
    "    cwd = Path().absolute()\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        if (parent / \".git\").exists() and parent.name == repo_name:\n",
    "            print(f\"Already in repo at {parent}\")\n",
    "            return parent\n",
    "\n",
    "    # 2. Detect if the repo exists in the current directory\n",
    "    repo_dir = Path(repo_name)\n",
    "    if repo_dir.exists():\n",
    "        print(f\"Repository exists at {repo_dir.absolute()}\")\n",
    "        os.system(f\"git -C {repo_dir} pull\")\n",
    "        return repo_dir\n",
    "\n",
    "    # 3. Otherwise clone it\n",
    "    print(\"Cloning repository...\")\n",
    "    url = f\"https://github.com/{username}/{repo_name}.git\"\n",
    "    exit_code = os.system(f\"git clone {url}\")\n",
    "\n",
    "    if exit_code != 0:\n",
    "        raise RuntimeError(\"Git clone failed.\")\n",
    "\n",
    "    print(\"Clone successful.\")\n",
    "    return repo_dir\n",
    "\n",
    "repo_dir = clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, sensor_metadata_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check and Backfill\n",
    "Only performed when done for the first time. \n",
    "\n",
    "For 100 sensors and 5 years this will take approximately 150 minutes = 2.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481debe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.74s) \n"
     ]
    },
    {
     "ename": "OpenMeteoRequestsError",
     "evalue": "failed to request 'https://archive-api.open-meteo.com/v1/archive': {'error': True, 'reason': 'Daily API request limit exceeded. Please try again tomorrow.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenMeteoRequestsError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\openmeteo_requests\\Client.py:88\u001b[39m, in \u001b[36mClient.weather_api\u001b[39m\u001b[34m(self, url, params, method, verify, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\openmeteo_requests\\Client.py:72\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, url, method, params, verify, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m     response_body = response.json()\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenMeteoRequestsError(response_body)\n\u001b[32m     74\u001b[39m response.raise_for_status()\n",
      "\u001b[31mOpenMeteoRequestsError\u001b[39m: {'error': True, 'reason': 'Daily API request limit exceeded. Please try again tomorrow.'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOpenMeteoRequestsError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m longitude = meta[\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     33\u001b[39m city = meta[\u001b[33m\"\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m weather_df = \u001b[43mfetchers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_historical_weather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weather_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weather_df) == \u001b[32m0\u001b[39m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è No historical weather for sensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msensor_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\utils\\fetchers.py:78\u001b[39m, in \u001b[36mget_historical_weather\u001b[39m\u001b[34m(city, start_date, end_date, latitude, longitude)\u001b[39m\n\u001b[32m     63\u001b[39m params = {\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m: latitude,\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m: longitude,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimezone\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mUTC\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m }\n\u001b[32m     77\u001b[39m rate_limited_request()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m responses = \u001b[43mopenmeteo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweather_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m response = responses[\u001b[32m0\u001b[39m]\n\u001b[32m     81\u001b[39m daily = response.Daily()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\openmeteo_requests\\Client.py:97\u001b[39m, in \u001b[36mClient.weather_api\u001b[39m\u001b[34m(self, url, params, method, verify, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     96\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed to request \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenMeteoRequestsError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOpenMeteoRequestsError\u001b[39m: failed to request 'https://archive-api.open-meteo.com/v1/archive': {'error': True, 'reason': 'Daily API request limit exceeded. Please try again tomorrow.'}"
     ]
    }
   ],
   "source": [
    "# Check if data exists\n",
    "try:\n",
    "    aq_data = air_quality_fg.read()\n",
    "    is_first_run = len(aq_data) == 0\n",
    "except:\n",
    "    is_first_run = True\n",
    "\n",
    "# Process and insert data if first run\n",
    "if is_first_run:\n",
    "    all_aq_dfs = []\n",
    "    all_weather_dfs = []\n",
    "    locations = {}\n",
    "\n",
    "    # Process CSV files in data directory\n",
    "    data_dir = os.path.join(root_dir, \"data\")\n",
    "    dir_list = os.listdir(data_dir)\n",
    "    metadata_df = sensor_metadata_fg.read().set_index(\"sensor_id\")\n",
    "    for file in dir_list:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(file_path, AQICN_API_KEY)\n",
    "            \n",
    "            # Clean and process\n",
    "            aq_df = cleaning.clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "            aq_df[\"date\"] = aq_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "            # start_date = aq_df[\"date\"].min().date()\n",
    "            start_date = end_date - timedelta(days=365 * 3)\n",
    "            end_date = aq_df[\"date\"].max().date()\n",
    "\n",
    "            meta = metadata_df.loc[sensor_id]\n",
    "            latitude = meta[\"latitude\"]\n",
    "            longitude = meta[\"longitude\"]\n",
    "            city = meta[\"city\"]\n",
    "\n",
    "            weather_df = fetchers.get_historical_weather(city, start_date, end_date, latitude, longitude)\n",
    "\n",
    "            if weather_df is None or len(weather_df) == 0:\n",
    "                print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "            all_aq_dfs.append(aq_df)\n",
    "            all_weather_dfs.append(weather_df)\n",
    "            locations[sensor_id] = {\n",
    "                \"country\": country,\n",
    "                \"city\": city,\n",
    "                \"street\": street,\n",
    "                \"aqicn_url\": feed_url,\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude,\n",
    "            }\n",
    "\n",
    "    if all_aq_dfs:\n",
    "        # Combine and engineer features\n",
    "        aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "        weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "\n",
    "        aq_df_all = feature_engineering.add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "        aq_df_all = feature_engineering.add_lagged_features(aq_df_all, column=\"pm25\", lags=[1, 2, 3])\n",
    "        aq_df_all = feature_engineering.add_nearby_sensor_feature(aq_df_all, locations, column=\"pm25_lag_1d\", n_closest=3)\n",
    "        \n",
    "        air_quality_fg.insert(aq_df_all)\n",
    "        weather_fg.insert(weather_df_all)\n",
    "\n",
    "        # Insert sensor metadata\n",
    "        metadata_records = []\n",
    "        for sensor_id, loc in locations.items():\n",
    "            metadata_records.append({\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"country\": loc[\"country\"],\n",
    "                \"city\": loc[\"city\"],\n",
    "                \"street\": loc[\"street\"],\n",
    "                \"aqicn_url\": loc[\"aqicn_url\"],\n",
    "                \"latitude\": loc[\"latitude\"],\n",
    "                \"longitude\": loc[\"longitude\"],\n",
    "            })\n",
    "        sensor_metadata_fg.insert(pd.DataFrame(metadata_records))\n",
    "    \n",
    "        print(f\"‚úÖ Inserted {len(aq_df_all)} air quality records\")\n",
    "        print(f\"‚úÖ Inserted {len(weather_df_all)} weather records\")\n",
    "        print(f\"‚úÖ Inserted {len(metadata_records)} sensor metadata records\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No CSV files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.4. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "hopsworks_admin.update_sensor_metadata_description(sensor_metadata_fg)\n",
    "hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.5. Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\"column\": col, \"type_list\": [\"float\", \"int\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# temperature should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"temperature_2m_mean\", \"min_value\": -80, \"max_value\": 60},\n",
    "    )\n",
    ")\n",
    "\n",
    "# latitude/longitude must be valid\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"latitude\", \"min_value\": -90, \"max_value\": 90},\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"longitude\", \"min_value\": -180, \"max_value\": 180},\n",
    "    )\n",
    ")\n",
    "\n",
    "# precipitation and wind speed should be >= 0 (but allow nulls)\n",
    "for col in [\"precipitation_sum\", \"wind_speed_10m_max\"]:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": None,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.6. Create Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select_all(), on=[\"sensor_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca29e7",
   "metadata": {},
   "source": [
    "## 1.7. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No sensor metadata found. Run first-time CSV processing first.\")\n",
    "    else:\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading sensor metadata: {e}\")\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "historical_df = air_quality_fv.get_batch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d7a56",
   "metadata": {},
   "source": [
    "## 1.8. Incremental Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2e9a1",
   "metadata": {},
   "source": [
    "Detect latest timestamp per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_per_sensor = (\n",
    "    historical_df.groupby(\"sensor_id\")[\"date\"]\n",
    "    .max()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "incremental.run_incremental_update(\n",
    "    sensor_metadata_fg,\n",
    "    air_quality_fg,\n",
    "    weather_fg,\n",
    "    latest_per_sensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {aq_df_all.shape}\")\n",
    "print(f\"Date range: {aq_df_all['date'].min().date()} to {aq_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {aq_df_all['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {aq_df_all['country'].unique()}\")\n",
    "print(f\"Cities: {aq_df_all['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(aq_df_all['pm25'].describe())\n",
    "print(f\"Missing values: {aq_df_all['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in aq_df_all.columns:\n",
    "        missing = aq_df_all[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(aq_df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {weather_df_all.shape}\")\n",
    "print(f\"Date range: {weather_df_all['date'].min().date()} to {weather_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {weather_df_all['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df_all.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df_all[col].min():.2f} to {weather_df_all[col].max():.2f}, Mean: {weather_df_all[col].mean():.2f}, Missing: {weather_df_all[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {weather_df_all['latitude'].min():.3f} to {weather_df_all['latitude'].max():.3f}, Longitude range: {weather_df_all['longitude'].min():.3f} to {weather_df_all['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = aq_df_all.groupby('sensor_id')['date'].count()\n",
    "total_records = len(aq_df_all)\n",
    "data_completeness = (1 - aq_df_all['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (aq_df_all['pm25'] > 100).sum()\n",
    "very_high_count = (aq_df_all['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(aq_df_all) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(aq_df_all['date']).dt.month\n",
    "    monthly_pm25 = aq_df_all.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
