{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ae8d7e",
   "metadata": {},
   "source": [
    "## 1.1. Environment Setup\n",
    "Detect if running in Google Colab or local environment, handle repository cloning, dependency installation, numpy compatibility fixes, and set up Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b485ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "Added the following directory to the PYTHONPATH: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import hopsworks\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "# def is_google_colab() -> bool:\n",
    "#     try:\n",
    "#         if \"google.colab\" in str(get_ipython()):\n",
    "#             return True\n",
    "#     except:\n",
    "#         pass\n",
    "#     return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    repo_dir = Path(\"pm25-forecast-openmeteo-aqicn\")\n",
    "    if repo_dir.exists():\n",
    "        print(f\"Repository already exists at {repo_dir.absolute()}\")\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/KristinaPalmquist/pm25-forecast-openmeteo-aqicn.git\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "def fix_numpy_compatibility():\n",
    "    print(\"Fixing numpy compatibility for hopsworks/pandas...\")\n",
    "    try:\n",
    "        !pip install --force-reinstall numpy==1.26.4 pandas==2.0.3\n",
    "        print(\"Numpy and pandas fixed. Please restart runtime and run again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fix attempt failed: {e}\")\n",
    "        print(\"Please manually restart runtime and try again.\")\n",
    "\n",
    "# if is_google_colab():\n",
    "#     try:\n",
    "#         import numpy\n",
    "#         numpy.array([1, 2, 3])\n",
    "#         import pandas as pd\n",
    "#         print(\"Basic packages working correctly\")\n",
    "\n",
    "#         clone_repository()\n",
    "#         install_dependencies()\n",
    "\n",
    "#         import hopsworks\n",
    "#         print(\"All packages working correctly\")\n",
    "\n",
    "#         root_dir = str(Path().absolute())\n",
    "#         print(\"Google Colab environment\")\n",
    "        \n",
    "#     except (ValueError, ImportError) as e:\n",
    "#         if \"numpy.dtype size changed\" in str(e) or \"numpy.strings\" in str(e) or \"numpy\" in str(e).lower():\n",
    "#             fix_numpy_compatibility()\n",
    "#             raise SystemExit(\"Please restart runtime (Runtime > Restart runtime) and run the notebook again.\")\n",
    "#         else:\n",
    "#             raise\n",
    "\n",
    "# else:\n",
    "root_dir = Path().absolute()\n",
    "if root_dir.parts[-1:] == (\"src\",):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "if root_dir.parts[-1:] == (\"airquality\",):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "if root_dir.parts[-1:] == (\"notebooks\",):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "root_dir = str(root_dir)\n",
    "    # print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# if root_dir not in sys.path:\n",
    "#     sys.path.append(root_dir)\n",
    "#     print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "# if is_google_colab():\n",
    "#     from google.colab import userdata\n",
    "#     import hopsworks\n",
    "#     project = hopsworks.login(\n",
    "#         api_key_value=userdata.get('HOPSWORKS_API_KEY'),\n",
    "#         engine=\"python\"\n",
    "#     )\n",
    "#     AQICN_API_KEY = userdata.get('AQICN_API_KEY')\n",
    "    \n",
    "# else:\n",
    "    # from utils import config\n",
    "    # settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "\n",
    "from utils import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "project = hopsworks.login(engine=\"python\", api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad128c99",
   "metadata": {},
   "source": [
    "## 1.2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3649091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "import hopsworks\n",
    "from utils import airquality\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322e26a",
   "metadata": {},
   "source": [
    "## 1.3. Setup\n",
    "Configure Hopsworks connection, feature store access, and AQICN API key handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad12486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-12 08:58:09,555 INFO: Initializing external client\n",
      "2025-12-12 08:58:09,556 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-12 08:58:11,352 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "Replacing existing AQICN_API_KEY\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    }
   ],
   "source": [
    "today = datetime.date.today()\n",
    "\n",
    "if is_google_colab():\n",
    "    fs = project.get_feature_store()\n",
    "    secrets = hopsworks.get_secrets_api()\n",
    "else:\n",
    "    HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "    project = hopsworks.login(engine=\"python\", api_key_value=HOPSWORKS_API_KEY)\n",
    "    fs = project.get_feature_store()\n",
    "    \n",
    "    if settings.AQICN_API_KEY is None:\n",
    "        print(\"You need to set AQICN_API_KEY either in this cell or in ~/.env\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "    secrets = hopsworks.get_secrets_api()\n",
    "    try:\n",
    "        secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "        if secret is not None:\n",
    "            secret.delete()\n",
    "            print(\"Replacing existing AQICN_API_KEY\")\n",
    "    except hopsworks.RestAPIError as e:\n",
    "        if hasattr(e, \"error_code\") and getattr(e, \"error_code\", None) == 160048:\n",
    "            pass\n",
    "        elif \"Could not find Secret\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fff50",
   "metadata": {},
   "source": [
    "## 1.4. Processing Mode\n",
    "This notebook processes all sensors that have CSV files in the `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1641ed",
   "metadata": {},
   "source": [
    "## 1.5. Data Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef87eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": 500.0,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "def expect_greater_than_zero(col):\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": 1000.0,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "expect_greater_than_zero(\"precipitation_sum\")\n",
    "expect_greater_than_zero(\"wind_speed_10m_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68ea8b",
   "metadata": {},
   "source": [
    "## 1.6. Helper Methods\n",
    "Data processing functions - clean air quality data and fetch historical weather data with API rate limiting and retry logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcc2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_append_data(df, street, city, country, feed_url, sensor_id):\n",
    "    \"\"\"\n",
    "    Remove any unused columns, set the daily median value to pm25. Remove NaN's and append the metadata.\n",
    "    \"\"\"\n",
    "    clean_df = pd.DataFrame()\n",
    "    clean_df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    clean_df[\"pm25\"] = df[\"median\"]\n",
    "    clean_df = clean_df.dropna(subset=[\"pm25\"])\n",
    "    clean_df[\"sensor_id\"] = sensor_id\n",
    "    clean_df[\"street\"] = street\n",
    "    clean_df[\"city\"] = city\n",
    "    clean_df[\"country\"] = country\n",
    "    clean_df[\"feed_url\"] = feed_url\n",
    "    return clean_df\n",
    "\n",
    "def get_historical_weather(city, df, today, feed_url, sensor_id):\n",
    "    earliest_aq_date = pd.Series.min(df[\"date\"])\n",
    "    earliest_aq_date = earliest_aq_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    try:\n",
    "        latitude, longitude, working_feed_url = airquality.get_sensor_coordinates_with_fallback(sensor_id, AQICN_API_KEY)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to get coordinates for sensor {sensor_id}: {e}\")\n",
    "    \n",
    "    max_retries = 5\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            weather_df = airquality.get_historical_weather(\n",
    "                city, earliest_aq_date, str(today), latitude, longitude\n",
    "            )\n",
    "            weather_df[\"sensor_id\"] = sensor_id\n",
    "            weather_df[\"city\"] = city\n",
    "            weather_df[\"latitude\"] = latitude\n",
    "            weather_df[\"longitude\"] = longitude\n",
    "            return weather_df, latitude, longitude\n",
    "        except Exception as e:\n",
    "            if hasattr(e, \"args\") and any(\n",
    "                \"Minutely API request limit exceeded\" in str(a) for a in e.args\n",
    "            ):\n",
    "                wait_time = 70\n",
    "                print(\n",
    "                    f\"OpenMeteo API limit exceeded, retrying in {wait_time} seconds... (Attempt {attempt + 1} of {max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            elif \"Minutely API request limit exceeded\" in str(e):\n",
    "                wait_time = 70\n",
    "                print(\n",
    "                    f\"OpenMeteo API limit exceeded, retrying in {wait_time} seconds... (Attempt {attempt + 1} of {max_retries})\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(\n",
    "        \"Failed to obtain historical weather after multiple retries due to API rate limits.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.7. Hopsworks\n",
    "Feature Group Management - functions to create and manage air quality and weather feature groups in Hopsworks, including schema descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aaa236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_air_quality_feature_group():\n",
    "    air_quality_fg = fs.get_or_create_feature_group(\n",
    "        name=\"air_quality_all\",\n",
    "        description=\"Air Quality characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        event_time=\"date\",\n",
    "        expectation_suite=aq_expectation_suite,\n",
    "    )\n",
    "    return air_quality_fg\n",
    "\n",
    "def update_air_quality_description(air_quality_fg):\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"date\", \"Date of measurement of air quality\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"country\",\n",
    "        \"Country where the air quality was measured (sometimes a city in aqicn.org)\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"city\", \"City where the air quality was measured\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"street\", \"Street in the city where the air quality was measured\"\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25\",\n",
    "        \"Particles less than 2.5 micrometers in diameter (fine particles) pose health risk\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_rolling_3d\",\n",
    "        \"3-day rolling mean of PM2.5 from previous days (lagged by 1 day for point-in-time correctness).\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_lag_1d\",\n",
    "        \"PM2.5 value from 1 day ago.\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_lag_2d\",\n",
    "        \"PM2.5 value from 2 days ago.\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_lag_3d\",\n",
    "        \"PM2.5 value from 3 days ago.\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_nearby_avg\",\n",
    "        \"Average PM2.5 value from the 3 closest sensors.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_and_insert_air_quality_data(df):\n",
    "    air_quality_fg = create_air_quality_feature_group()\n",
    "    air_quality_fg.insert(df)\n",
    "    update_air_quality_description(air_quality_fg)\n",
    "\n",
    "\n",
    "def create_weather_feature_group():\n",
    "    weather_fg = fs.get_or_create_feature_group(\n",
    "        name=\"weather_all\",\n",
    "        description=\"Weather characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        event_time=\"date\",\n",
    "        expectation_suite=weather_expectation_suite,\n",
    "    )\n",
    "    return weather_fg\n",
    "\n",
    "\n",
    "def update_weather_description(weather_fg):\n",
    "    weather_fg.update_feature_description(\"date\", \"Date of measurement of weather\")\n",
    "    weather_fg.update_feature_description(\n",
    "        \"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"city\", \"City where weather is measured/forecast for\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"temperature_2m_mean\", \"Temperature in Celsius\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"precipitation_sum\", \"Precipitation (rain/snow) in mm\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"wind_speed_10m_max\", \"Wind speed at 10m above ground\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"wind_direction_10m_dominant\", \"Dominant Wind direction over the days\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"latitude\", \"Latitude of sensor location used for weather retrieval\"\n",
    "    )\n",
    "    weather_fg.update_feature_description(\n",
    "        \"longitude\", \"Longitude of sensor location used for weather retrieval\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_and_insert_weather_data(df):\n",
    "    weather_fg = create_weather_feature_group()\n",
    "    weather_fg.insert(df)\n",
    "    update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60347210",
   "metadata": {},
   "source": [
    "## 1.8. Script\n",
    "Main processing logic - processes all sensors in the data folder, cleans data, fetches weather data, adds rolling averages and lagged features, and combines all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee25cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenMeteo API limit exceeded, retrying in 70 seconds... (Attempt 1 of 5)\n",
      "OpenMeteo API limit exceeded, retrying in 70 seconds... (Attempt 1 of 5)\n",
      "OpenMeteo API limit exceeded, retrying in 70 seconds... (Attempt 1 of 5)\n",
      "OpenMeteo API limit exceeded, retrying in 70 seconds... (Attempt 1 of 5)\n",
      "OpenMeteo API limit exceeded, retrying in 70 seconds... (Attempt 1 of 5)\n",
      "OpenMeteo API limit exceeded, retrying in 70 seconds... (Attempt 1 of 5)\n",
      "\n",
      "==================================================\n",
      "üìã PROCESSING SUMMARY\n",
      "==================================================\n",
      "‚úÖ Successfully processed: 103 sensors\n",
      "‚úÖ No sensors were skipped!\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_aq_dfs = []\n",
    "all_weather_dfs = []\n",
    "locations = {}\n",
    "skipped_sensors = []\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "for file in dir_list:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.isdir(file_path) or not file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        aq_df_raw, street, city, country, feed_url, sensor_id = airquality.read_sensor_data(file_path)\n",
    "        aq_df = clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "        weather_df, latitude, longitude = get_historical_weather(\n",
    "            city, aq_df, today, feed_url, sensor_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        file_sensor_id = file.replace('.csv', '')\n",
    "        \n",
    "        error_type = type(e).__name__\n",
    "        if \"OpenMeteoRequestsError\" in error_type:\n",
    "            if \"Hourly API request limit exceeded\" in str(e):\n",
    "                reason = \"OpenMeteo hourly API limit exceeded\"\n",
    "            elif \"Minutely API request limit exceeded\" in str(e):\n",
    "                reason = \"OpenMeteo minutely API limit exceeded\"\n",
    "            else:\n",
    "                reason = \"OpenMeteo API error\"\n",
    "        elif \"ValueError\" in error_type:\n",
    "            reason = \"Coordinate/data error\"\n",
    "        else:\n",
    "            reason = error_type\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è  Skipping sensor {file_sensor_id}: {reason}\")\n",
    "        skipped_sensors.append((file_sensor_id, reason))\n",
    "        continue\n",
    "\n",
    "    all_aq_dfs.append(aq_df)\n",
    "    all_weather_dfs.append(weather_df)\n",
    "    locations[sensor_id] = {\n",
    "        \"country\": country,\n",
    "        \"city\": city,\n",
    "        \"street\": street,\n",
    "        \"aqicn_url\": feed_url,\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "    }\n",
    "\n",
    "# Print processing summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìã PROCESSING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Successfully processed: {len(locations)} sensors\")\n",
    "if skipped_sensors:\n",
    "    print(f\"‚ùå Skipped sensors ({len(skipped_sensors)}): {', '.join([s[0] for s in skipped_sensors])}\")\n",
    "    \n",
    "    # Group by reason for clean summary\n",
    "    from collections import Counter\n",
    "    reason_counts = Counter([reason for _, reason in skipped_sensors])\n",
    "    for reason, count in reason_counts.items():\n",
    "        print(f\"   - {count} sensors: {reason}\")\n",
    "else:\n",
    "    print(\"‚úÖ No sensors were skipped!\")\n",
    "\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Concatenate into single, uniform dfs\n",
    "aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "aq_df_all = airquality.add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "aq_df_all = airquality.add_lagged_features(aq_df_all, column=\"pm25\", lags=[1, 2, 3])\n",
    "aq_df_all = airquality.add_nearby_sensor_feature(aq_df_all, locations, column=\"pm25_lag_1d\", n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79536f30",
   "metadata": {},
   "source": [
    "Air quality data info - display information about the processed air quality DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9222740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AIR QUALITY DATA EXPLORATION\n",
      "========================================\n",
      "Shape: (165655, 12)\n",
      "Date range: 2019-12-09 to 2025-12-04\n",
      "Number of unique sensors: 103\n",
      "Countries: ['Sweden']\n",
      "Cities: 85 unique cities\n",
      "\n",
      "üìä PM2.5 Statistics:\n",
      "count    165655.000000\n",
      "mean          3.202608\n",
      "std          11.877357\n",
      "min           0.000000\n",
      "25%           0.900000\n",
      "50%           1.800000\n",
      "75%           3.500000\n",
      "max         999.900000\n",
      "Name: pm25, dtype: float64\n",
      "Missing values: 0\n",
      "\n",
      "üìà Engineered Features Statistics:\n",
      "pm25_rolling_3d: 103 missing values (0.1%)\n",
      "pm25_lag_1d: 103 missing values (0.1%)\n",
      "pm25_lag_2d: 206 missing values (0.1%)\n",
      "pm25_lag_3d: 309 missing values (0.2%)\n",
      "pm25_nearby_avg: 3506 missing values (2.1%)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {aq_df_all.shape}\")\n",
    "print(f\"Date range: {aq_df_all['date'].min().date()} to {aq_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {aq_df_all['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {aq_df_all['country'].unique()}\")\n",
    "print(f\"Cities: {aq_df_all['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(aq_df_all['pm25'].describe())\n",
    "print(f\"Missing values: {aq_df_all['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in aq_df_all.columns:\n",
    "        missing = aq_df_all[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(aq_df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95b3fd",
   "metadata": {},
   "source": [
    "Weather data info - displays information about the processed weather DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è WEATHER DATA EXPLORATION\n",
      "========================================\n",
      "Shape: (191423, 9)\n",
      "Date range: 2019-12-09 to 2025-12-12\n",
      "Number of unique sensors: 103\n",
      "\n",
      "üå°Ô∏è Weather Statistics:\n",
      "temperature_2m_mean:\n",
      "  Range: -31.30 to 27.15, Mean: 7.94, Missing: 0\n",
      "precipitation_sum:\n",
      "  Range: 0.00 to 105.10, Mean: 2.23, Missing: 0\n",
      "wind_speed_10m_max:\n",
      "  Range: 3.05 to 74.15, Mean: 18.52, Missing: 0\n",
      "wind_direction_10m_dominant:\n",
      "  Range: 0.00 to 360.00, Mean: 199.22, Missing: 0\n",
      "\n",
      "üìç Geographic Coverage:\n",
      "Latitude range: 55.476 to 64.944, Longitude range: 11.166 to 20.874\n"
     ]
    }
   ],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {weather_df_all.shape}\")\n",
    "print(f\"Date range: {weather_df_all['date'].min().date()} to {weather_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {weather_df_all['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df_all.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df_all[col].min():.2f} to {weather_df_all[col].max():.2f}, Mean: {weather_df_all[col].mean():.2f}, Missing: {weather_df_all[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {weather_df_all['latitude'].min():.3f} to {weather_df_all['latitude'].max():.3f}, Longitude range: {weather_df_all['longitude'].min():.3f} to {weather_df_all['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d531812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó DATA QUALITY & RELATIONSHIPS\n",
      "========================================\n",
      "üìä Overall Data Quality:\n",
      "Total records: 165,655\n",
      "Data completeness: 100.0%\n",
      "Days per sensor - Min: 86, Median: 1872, Max: 2184\n",
      "Sensors with <30 days: 0, >365 days: 100\n",
      "\n",
      "‚ö†Ô∏è Air Quality Levels:\n",
      "Extreme readings (>100 Œºg/m¬≥): 38 (0.0%)\n",
      "Very high readings (>50 Œºg/m¬≥): 142 (0.1%)\n",
      "\n",
      "üóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\n",
      "  Winter: 3.8\n",
      "  Spring: 2.7\n",
      "  Summer: 2.9\n",
      "  Autumn: 3.5\n"
     ]
    }
   ],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = aq_df_all.groupby('sensor_id')['date'].count()\n",
    "total_records = len(aq_df_all)\n",
    "data_completeness = (1 - aq_df_all['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (aq_df_all['pm25'] > 100).sum()\n",
    "very_high_count = (aq_df_all['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(aq_df_all) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(aq_df_all['date']).dt.month\n",
    "    monthly_pm25 = aq_df_all.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3415e",
   "metadata": {},
   "source": [
    "## 1.10. Store Sensor Location\n",
    "Create Hopsworks secrets for each sensor's location metadata (coordinates, address, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ae81b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n",
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    }
   ],
   "source": [
    "for sensor_id, location in locations.items():\n",
    "    secret_name = f\"SENSOR_LOCATION_JSON_{sensor_id}\"\n",
    "    location_str = json.dumps(location)\n",
    "    \n",
    "    try:\n",
    "        secret = secrets.get_secret(secret_name)\n",
    "        if secret is not None:\n",
    "            secret.delete()\n",
    "    except hopsworks.RestAPIError as e:\n",
    "        if hasattr(e, \"error_code\") and getattr(e, \"error_code\", None) == 160048:\n",
    "            pass\n",
    "        elif \"Could not find Secret\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    secrets.create_secret(secret_name, location_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82031a",
   "metadata": {},
   "source": [
    "## 1.11. Upload to Hopsworks\n",
    "Insert the processed air quality and weather data into Hopsworks feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7504ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-12 09:57:35,959 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1774972\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\tmp\\\\c.app.hopsworks.ai\\\\kristina_titanic\\\\keyStore.jks'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcreate_and_insert_air_quality_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43maq_df_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m create_and_insert_weather_data(weather_df_all)  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcreate_and_insert_air_quality_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_and_insert_air_quality_data\u001b[39m(df):\n\u001b[32m     56\u001b[39m     air_quality_fg = create_air_quality_feature_group()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mair_quality_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     update_air_quality_description(air_quality_fg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\feature_group.py:3153\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3151\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33moffline_backfill_every_hr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr\n\u001b[32m-> \u001b[39m\u001b[32m3153\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3156\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3163\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   3166\u001b[39m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[32m   3167\u001b[39m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[32m   3168\u001b[39m     \u001b[38;5;28mself\u001b[39m.compute_statistics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\core\\feature_group_engine.py:245\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m._feature_group_api.delete_content(feature_group)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbulk_insert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    254\u001b[39m     ge_report,\n\u001b[32m    255\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\engine\\python.py:819\u001b[39m, in \u001b[36mEngine.save_dataframe\u001b[39m\u001b[34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_dataframe\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    806\u001b[39m     feature_group: FeatureGroup,\n\u001b[32m   (...)\u001b[39m\u001b[32m    813\u001b[39m     validation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    814\u001b[39m ) -> Optional[job.Job]:\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    816\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(feature_group, \u001b[33m\"\u001b[39m\u001b[33mEXTERNAL_FEATURE_GROUP\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    817\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m feature_group.online_enabled\n\u001b[32m    818\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group.stream:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    823\u001b[39m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[32m    824\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy_save_dataframe(\n\u001b[32m    825\u001b[39m             feature_group,\n\u001b[32m    826\u001b[39m             dataframe,\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m             validation_id,\n\u001b[32m    833\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\engine\\python.py:1499\u001b[39m, in \u001b[36mEngine._write_dataframe_kafka\u001b[39m\u001b[34m(self, feature_group, dataframe, offline_write_options)\u001b[39m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write_dataframe_kafka\u001b[39m(\n\u001b[32m   1493\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1494\u001b[39m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[32m   1495\u001b[39m     dataframe: Union[pd.DataFrame, pl.DataFrame],\n\u001b[32m   1496\u001b[39m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1497\u001b[39m ) -> Optional[job.Job]:\n\u001b[32m   1498\u001b[39m     initial_check_point = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1499\u001b[39m     producer, headers, feature_writers, writer = \u001b[43mkafka_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1505\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group._multi_part_insert:\n\u001b[32m   1506\u001b[39m         \u001b[38;5;66;03m# set initial_check_point to the current offset\u001b[39;00m\n\u001b[32m   1507\u001b[39m         initial_check_point = kafka_engine.kafka_get_offsets(\n\u001b[32m   1508\u001b[39m             topic_name=feature_group._online_topic_name,\n\u001b[32m   1509\u001b[39m             feature_store_id=feature_group.feature_store_id,\n\u001b[32m   1510\u001b[39m             offline_write_options=offline_write_options,\n\u001b[32m   1511\u001b[39m             high=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1512\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\core\\kafka_engine.py:86\u001b[39m, in \u001b[36minit_kafka_resources\u001b[39m\u001b[34m(feature_group, offline_write_options, num_entries)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_group._multi_part_insert \u001b[38;5;129;01mand\u001b[39;00m feature_group._kafka_producer:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     81\u001b[39m         feature_group._kafka_producer,\n\u001b[32m     82\u001b[39m         feature_group._kafka_headers,\n\u001b[32m     83\u001b[39m         feature_group._feature_writers,\n\u001b[32m     84\u001b[39m         feature_group._writer,\n\u001b[32m     85\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m producer, headers, feature_writers, writer = \u001b[43m_init_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_entries\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_group._multi_part_insert:\n\u001b[32m     90\u001b[39m     feature_group._kafka_producer = producer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\core\\kafka_engine.py:105\u001b[39m, in \u001b[36m_init_kafka_resources\u001b[39m\u001b[34m(feature_group, offline_write_options, num_entries)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_kafka_resources\u001b[39m(\n\u001b[32m     98\u001b[39m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[32m     99\u001b[39m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m ]:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     producer = \u001b[43minit_kafka_producer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# setup headers\u001b[39;00m\n\u001b[32m    109\u001b[39m     headers = get_headers(feature_group, num_entries)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hopsworks_common\\decorators.py:138\u001b[39m, in \u001b[36muses_confluent_kafka.<locals>.g\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m HAS_CONFLUENT_KAFKA:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(confluent_kafka_not_installed_message)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\core\\kafka_engine.py:158\u001b[39m, in \u001b[36minit_kafka_producer\u001b[39m\u001b[34m(feature_store_id, offline_write_options)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;129m@uses_confluent_kafka\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_kafka_producer\u001b[39m(\n\u001b[32m    154\u001b[39m     feature_store_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    155\u001b[39m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    156\u001b[39m ) -> Producer:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Producer(\u001b[43mget_kafka_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\core\\kafka_engine.py:284\u001b[39m, in \u001b[36mget_kafka_config\u001b[39m\u001b[34m(feature_store_id, write_options, engine)\u001b[39m\n\u001b[32m    282\u001b[39m     config.update(write_options)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mconfluent\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     config = \u001b[43mstorage_connector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfluent_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     config.update(write_options.get(\u001b[33m\"\u001b[39m\u001b[33mkafka_producer_config\u001b[39m\u001b[33m\"\u001b[39m, {}))\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hsfs\\storage_connector.py:1284\u001b[39m, in \u001b[36mKafkaConnector.confluent_options\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1269\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kafka_options.items():\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1271\u001b[39m         key\n\u001b[32m   1272\u001b[39m         \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m   (...)\u001b[39m\u001b[32m   1278\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pem_files_created\n\u001b[32m   1279\u001b[39m     ):\n\u001b[32m   1280\u001b[39m         (\n\u001b[32m   1281\u001b[39m             ca_chain_path,\n\u001b[32m   1282\u001b[39m             client_cert_path,\n\u001b[32m   1283\u001b[39m             client_key_path,\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         ) = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_write_pem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkafka_options\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mssl.keystore.location\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkafka_options\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mssl.keystore.password\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkafka_options\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mssl.truststore.location\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkafka_options\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mssl.truststore.password\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka_sc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_project_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1291\u001b[39m         \u001b[38;5;28mself\u001b[39m._pem_files_created = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1292\u001b[39m         config[\u001b[33m\"\u001b[39m\u001b[33mssl.ca.location\u001b[39m\u001b[33m\"\u001b[39m] = ca_chain_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hopsworks_common\\client\\base.py:224\u001b[39m, in \u001b[36mClient._write_pem\u001b[39m\u001b[34m(self, keystore_path, keystore_pw, truststore_path, truststore_pw, prefix)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write_pem\u001b[39m(\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mself\u001b[39m, keystore_path, keystore_pw, truststore_path, truststore_pw, prefix\n\u001b[32m    223\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     ks = \u001b[43mjks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mKeyStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeystore_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeystore_pw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_decrypt_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     ts = jks.KeyStore.load(\n\u001b[32m    226\u001b[39m         Path(truststore_path), truststore_pw, try_decrypt_keys=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    229\u001b[39m     ca_chain_path = os.path.join(\u001b[33m\"\u001b[39m\u001b[33m/tmp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_ca_chain.pem\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\jks\\util.py:88\u001b[39m, in \u001b[36mAbstractKeystore.load\u001b[39m\u001b[34m(cls, filename, store_password, try_decrypt_keys)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, store_password, try_decrypt_keys=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    Convenience wrapper function; reads the contents of the given file\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    and passes it through to :func:`loads`. See :func:`loads`.\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     89\u001b[39m         input_bytes = file.read()\n\u001b[32m     90\u001b[39m         ret = \u001b[38;5;28mcls\u001b[39m.loads(input_bytes,\n\u001b[32m     91\u001b[39m                         store_password,\n\u001b[32m     92\u001b[39m                         try_decrypt_keys=try_decrypt_keys)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '\\\\tmp\\\\c.app.hopsworks.ai\\\\kristina_titanic\\\\keyStore.jks'"
     ]
    }
   ],
   "source": [
    "create_and_insert_air_quality_data(aq_df_all)\n",
    "create_and_insert_weather_data(weather_df_all)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
