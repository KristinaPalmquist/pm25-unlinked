{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-09 10:28:48,243 INFO: Initializing external client\n",
      "2026-01-09 10:28:48,243 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-09 10:28:49,091 WARNING: UserWarning: The installed hopsworks client version 4.1.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 10:28:50,087 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in repo at c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, sensor_metadata_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check and Backfill\n",
    "When performed for the first time, might take a long time if many added sensors (about 1.5 minutes per sensor).\n",
    "\n",
    "Thereafter about 3 seconds per sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (22.56s) \n",
      "üìã Found 103 sensors already in feature store\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.61s) \n",
      "üìç Loaded metadata for 103 sensors\n",
      "üìä Total sensors: 103, Already processed: 103, Remaining: 0\n",
      "‚è© Skipping sensor 105325, already in feature store\n",
      "‚è© Skipping sensor 107110, already in feature store\n",
      "‚è© Skipping sensor 112672, already in feature store\n",
      "‚è© ... (suppressing further skip messages)\n",
      "\n",
      "üéâ Backfill complete!\n",
      "üìä Final Summary:\n",
      "   ‚úÖ Successfully processed: 0\n",
      "   ‚ùå Failed: 0\n",
      "   ‚è© Skipped (already processed): 103\n",
      "   üìà Total in feature store: 103/103\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "\n",
    "# Get already processed sensors from feature group\n",
    "try:\n",
    "    existing_sensors = set(air_quality_fg.read()[\"sensor_id\"].unique())\n",
    "    print(f\"üìã Found {len(existing_sensors)} sensors already in feature store\")\n",
    "except:\n",
    "    existing_sensors = set()\n",
    "    print(\"üìã No existing sensors found, starting fresh\")\n",
    "\n",
    "# Load all metadata (needed for nearby sensor calculations and location_id lookup)\n",
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) > 0:\n",
    "        # Ensure sensor_id is int type for consistent lookups\n",
    "        metadata_df[\"sensor_id\"] = metadata_df[\"sensor_id\"].astype(int)\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except:\n",
    "    metadata_df = pd.DataFrame()\n",
    "    print(\"üìç No metadata found - please run pipeline 0 first!\")\n",
    "\n",
    "# Count total sensors to process\n",
    "total_sensors = len([f for f in dir_list if f.endswith(\".csv\")])\n",
    "remaining = total_sensors - len(existing_sensors)\n",
    "print(f\"üìä Total sensors: {total_sensors}, Already processed: {len(existing_sensors)}, Remaining: {remaining}\")\n",
    "\n",
    "# Track processing stats\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "failed_sensors = []  # Track which sensors failed and why\n",
    "\n",
    "for file in dir_list:\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    \n",
    "    try:\n",
    "        aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "            file_path, AQICN_API_KEY\n",
    "        )\n",
    "        \n",
    "        # Ensure sensor_id is int for consistent comparison\n",
    "        sensor_id = int(sensor_id)\n",
    "\n",
    "        # Skip if already processed\n",
    "        if sensor_id in existing_sensors:\n",
    "            skipped += 1\n",
    "            if skipped <= 3:  # Only print first 3 to avoid spam\n",
    "                print(f\"‚è© Skipping sensor {sensor_id}, already in feature store\")\n",
    "            elif skipped == 4:\n",
    "                print(f\"‚è© ... (suppressing further skip messages)\")\n",
    "            continue\n",
    "\n",
    "        # Clean AQ\n",
    "        aq_df = cleaning.clean_and_append_data(aq_df_raw, sensor_id)\n",
    "        aq_df = aq_df.sort_values(\"date\")\n",
    "        \n",
    "        # Check for and remove duplicate dates\n",
    "        initial_rows = len(aq_df)\n",
    "        aq_df = aq_df.drop_duplicates(subset=[\"date\"], keep=\"first\")\n",
    "        duplicates_removed = initial_rows - len(aq_df)\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"‚ö†Ô∏è  Sensor {sensor_id}: Removed {duplicates_removed} duplicate date entries\")\n",
    "\n",
    "        aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "        aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "\n",
    "        # Only add nearby sensor feature if we have existing metadata\n",
    "        if len(metadata_df) > 0:\n",
    "            aq_df = feature_engineering.add_nearby_sensor_feature(aq_df, metadata_df, n_closest=3)\n",
    "        else:\n",
    "            # Fill with zeros if no existing sensors to compare against\n",
    "            aq_df[\"pm25_nearby_avg\"] = 0.0\n",
    "\n",
    "        # Compute date range\n",
    "        end_date = aq_df[\"date\"].max().date()\n",
    "        start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "        # Get metadata from sensor_metadata_fg (should always exist now)\n",
    "        if sensor_id in metadata_df.index:\n",
    "            meta = metadata_df.loc[sensor_id]\n",
    "            latitude = meta[\"latitude\"]\n",
    "            longitude = meta[\"longitude\"]\n",
    "            location_id = int(meta[\"location_id\"])\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Sensor {sensor_id} not in metadata. Run pipeline 0 (setup) first to generate metadata.\")\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, \"Missing metadata\"))\n",
    "            continue\n",
    "\n",
    "        # Fetch weather with retry logic\n",
    "        weather_df = None\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_df = fetchers.get_historical_weather(\n",
    "                    location_id, start_date, end_date, latitude, longitude\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except (ConnectionError, ProtocolError, Timeout) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = (attempt + 1) * 5\n",
    "                    print(f\"‚ö†Ô∏è  Network error for sensor {sensor_id}, retrying in {wait_time}s... ({attempt + 1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to fetch weather for sensor {sensor_id} after {max_retries} attempts: {type(e).__name__}\")\n",
    "                    raise\n",
    "\n",
    "        if weather_df is None or len(weather_df) == 0:\n",
    "            print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, \"No weather data\"))\n",
    "            continue\n",
    "\n",
    "        weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "        \"\"\" CHECK TYPES \"\"\"\n",
    "\n",
    "        weather_df[\"location_id\"] = weather_df[\"location_id\"].astype(\"int32\")\n",
    "        weather_df = weather_df.astype({\n",
    "            \"location_id\": \"int32\",\n",
    "            \"temperature_2m_mean\": \"float64\",\n",
    "            \"precipitation_sum\": \"float64\",\n",
    "            \"wind_speed_10m_max\": \"float64\",\n",
    "            \"wind_direction_10m_dominant\": \"float64\",\n",
    "        })\n",
    "\n",
    "        weather_fg.insert(weather_df)\n",
    "\n",
    "        # Add location_id to air quality data for joining with weather\n",
    "        aq_df[\"location_id\"] = location_id\n",
    "\n",
    "\n",
    "        \"\"\" CHECK TYPES \"\"\"\n",
    "\n",
    "        aq_df[\"sensor_id\"] = aq_df[\"sensor_id\"].astype(\"int32\")\n",
    "        aq_df[\"location_id\"] = aq_df[\"location_id\"].astype(\"int32\")\n",
    "        aq_columns = [f.name for f in air_quality_fg.features]\n",
    "        aq_df = aq_df[aq_columns]\n",
    "        aq_df = aq_df.astype({\n",
    "            \"sensor_id\": \"int32\",\n",
    "            \"location_id\": \"int32\",\n",
    "            \"pm25\": \"float64\",\n",
    "            \"pm25_lag_1d\": \"float64\",\n",
    "            \"pm25_lag_2d\": \"float64\",\n",
    "            \"pm25_lag_3d\": \"float64\",\n",
    "            \"pm25_rolling_3d\": \"float64\",\n",
    "            \"pm25_nearby_avg\": \"float64\",\n",
    "        })\n",
    "\n",
    "        air_quality_fg.insert(aq_df)\n",
    "\n",
    "        # Add to processed set\n",
    "        existing_sensors.add(sensor_id)\n",
    "        successful += 1\n",
    "\n",
    "        print(f\"‚úÖ Inserted sensor {sensor_id} ({successful}/{remaining} complete, {failed} failed)\")\n",
    "\n",
    "    except (ConnectionError, ProtocolError, Timeout) as e:\n",
    "        failed += 1\n",
    "        error_msg = f\"Network error: {type(e).__name__}\"\n",
    "        failed_sensors.append((sensor_id, error_msg))\n",
    "        print(f\"‚ùå {error_msg} for sensor {sensor_id}\")\n",
    "        print(f\"   Continuing with next sensor... ({successful} successful, {failed} failed, {skipped} skipped)\")\n",
    "        continue\n",
    "        \n",
    "    except RequestException as e:\n",
    "        failed += 1\n",
    "        error_msg = f\"Request error: {type(e).__name__}\"\n",
    "        failed_sensors.append((sensor_id, error_msg))\n",
    "        print(f\"‚ùå {error_msg} for sensor {sensor_id}\")\n",
    "        print(f\"   Continuing with next sensor... ({successful} successful, {failed} failed, {skipped} skipped)\")\n",
    "        continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)[:50]}\"\n",
    "        failed_sensors.append((sensor_id, error_msg))\n",
    "        print(f\"‚ùå Unexpected error processing sensor {sensor_id}: {type(e).__name__} - {str(e)[:100]}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback: {traceback.format_exc()[:200]}\")\n",
    "        print(f\"   Continuing with next sensor... ({successful} successful, {failed} failed, {skipped} skipped)\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ Backfill complete!\")\n",
    "print(f\"üìä Final Summary:\")\n",
    "print(f\"   ‚úÖ Successfully processed: {successful}\")\n",
    "print(f\"   ‚ùå Failed: {failed}\")\n",
    "print(f\"   ‚è© Skipped (already processed): {skipped}\")\n",
    "print(f\"   üìà Total in feature store: {len(existing_sensors)}/{total_sensors}\")\n",
    "\n",
    "if len(failed_sensors) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed Sensors Detail:\")\n",
    "    for sid, reason in failed_sensors:\n",
    "        print(f\"   ‚Ä¢ Sensor {sid}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.4. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "hopsworks_admin.update_sensor_metadata_description(sensor_metadata_fg)\n",
    "hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.5. Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing expectation suite for FG 'air_quality'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1911228\n",
      "Saved expectation suite for FG 'air_quality'.\n",
      "Deleted existing expectation suite for FG 'weather'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1893863\n",
      "Saved expectation suite for FG 'weather'.\n"
     ]
    }
   ],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"type_list\": [\"float\", \"int\"],\n",
    "                \"mostly\": 1.0\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "# Date column - should be dateutil parseable\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"date\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# Temperature column - allow nulls, should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"min_value\": -80,\n",
    "            \"max_value\": 60,\n",
    "            \"mostly\": 1.0,\n",
    "            \"allow_cross_type_comparisons\": True\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Precipitation column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"min_value\": 0,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "            \"allow_cross_type_comparisons\": True\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wind column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"min_value\": 0,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "            \"allow_cross_type_comparisons\": True\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.6. Create Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select_all(), on=[\"location_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca29e7",
   "metadata": {},
   "source": [
    "## 1.7. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0b5c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.33s) \n",
      "üìç Loaded metadata for 103 sensors\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (10.85s) \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No sensor metadata found. Run first-time CSV processing first.\")\n",
    "    else:\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading sensor metadata: {e}\")\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "historical_df = air_quality_fv.get_batch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.8. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb2796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.90s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.57s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (10.33s) \n"
     ]
    }
   ],
   "source": [
    "air_quality_df = air_quality_fg.read()\n",
    "metadata_df = sensor_metadata_fg.read()\n",
    "weather_df = weather_fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9222740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AIR QUALITY DATA EXPLORATION\n",
      "========================================\n",
      "Shape: (165910, 9)\n",
      "Date range: 2019-12-09 to 2026-01-09\n",
      "Number of unique sensors: 103\n",
      "Countries: ['Sweden']\n",
      "Cities: 85 unique cities\n",
      "\n",
      "üìä PM2.5 Statistics:\n",
      "count    165910.000000\n",
      "mean          3.216124\n",
      "std          11.894608\n",
      "min           0.000000\n",
      "25%           0.900000\n",
      "50%           1.800000\n",
      "75%           3.500000\n",
      "max         999.900000\n",
      "Name: pm25, dtype: float64\n",
      "Missing values: 0\n",
      "\n",
      "üìà Engineered Features Statistics:\n",
      "pm25_rolling_3d: 165910 missing values (100.0%)\n",
      "pm25_lag_1d: 349 missing values (0.2%)\n",
      "pm25_lag_2d: 452 missing values (0.3%)\n",
      "pm25_lag_3d: 555 missing values (0.3%)\n",
      "pm25_nearby_avg: 165910 missing values (100.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"Shape: {air_quality_df.shape}\")\n",
    "print(f\"Date range: {air_quality_df['date'].min().date()} to {air_quality_df['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {air_quality_df['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {metadata_df['country'].unique()}\")\n",
    "print(f\"Cities: {metadata_df['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(air_quality_df['pm25'].describe())\n",
    "print(f\"Missing values: {air_quality_df['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in air_quality_df.columns:\n",
    "        missing = air_quality_df[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(air_quality_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è WEATHER DATA EXPLORATION\n",
      "========================================\n",
      "Shape: (118330, 6)\n",
      "Date range: 2018-06-01 to 2026-01-16\n",
      "Number of unique sensors: 103\n",
      "\n",
      "üå°Ô∏è Weather Statistics:\n",
      "temperature_2m_mean:\n",
      "  Range: -31.06 to 26.12, Mean: 6.99, Missing: 0\n",
      "precipitation_sum:\n",
      "  Range: 0.00 to 105.10, Mean: 2.12, Missing: 0\n",
      "wind_speed_10m_max:\n",
      "  Range: 0.20 to 63.46, Mean: 18.40, Missing: 0\n",
      "wind_direction_10m_dominant:\n",
      "  Range: 0.00 to 360.00, Mean: 184.89, Missing: 0\n",
      "\n",
      "üìç Geographic Coverage:\n",
      "Latitude range: 55.477 to 64.944, Longitude range: 11.175 to 20.881\n"
     ]
    }
   ],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"Shape: {weather_df.shape}\")\n",
    "print(f\"Date range: {weather_df['date'].min().date()} to {weather_df['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {metadata_df['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df[col].min():.2f} to {weather_df[col].max():.2f}, Mean: {weather_df[col].mean():.2f}, Missing: {weather_df[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {metadata_df['latitude'].min():.3f} to {metadata_df['latitude'].max():.3f}, Longitude range: {metadata_df['longitude'].min():.3f} to {metadata_df['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129aa662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó DATA QUALITY & RELATIONSHIPS\n",
      "========================================\n",
      "üìä Overall Data Quality:\n",
      "Total records: 165,910\n",
      "Data completeness: 100.0%\n",
      "Days per sensor - Min: 88, Median: 1873, Max: 2187\n",
      "Sensors with <30 days: 0, >365 days: 100\n",
      "\n",
      "‚ö†Ô∏è Air Quality Levels:\n",
      "Extreme readings (>100 Œºg/m¬≥): 41 (0.0%)\n",
      "Very high readings (>50 Œºg/m¬≥): 150 (0.1%)\n",
      "\n",
      "üóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\n",
      "  Winter: 3.8\n",
      "  Spring: 2.7\n",
      "  Summer: 2.9\n",
      "  Autumn: 3.5\n"
     ]
    }
   ],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = air_quality_df.groupby('sensor_id')['date'].count()\n",
    "total_records = len(air_quality_df)\n",
    "data_completeness = (1 - air_quality_df['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (air_quality_df['pm25'] > 100).sum()\n",
    "very_high_count = (air_quality_df['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(air_quality_df) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(air_quality_df['date']).dt.month\n",
    "    monthly_pm25 = air_quality_df.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
