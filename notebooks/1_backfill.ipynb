{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-05 16:35:11,836 INFO: Initializing external client\n",
      "2026-01-05 16:35:11,837 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-05 16:35:12,555 WARNING: UserWarning: The installed hopsworks client version 4.1.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05 16:35:13,635 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone, date\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5d0ef",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already inside repo at c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "def clone_or_update_repo(username: str):\n",
    "    repo_name = \"pm25-forecast-openmeteo-aqicn\"\n",
    "\n",
    "    # 1. Detect if already inside the repo\n",
    "    cwd = Path().absolute()\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        if (parent / \".git\").exists() and parent.name == repo_name:\n",
    "            print(f\"Already inside repo at {parent}\")\n",
    "            return parent\n",
    "\n",
    "    # 2. Detect if the repo exists in the current directory\n",
    "    repo_dir = Path(repo_name)\n",
    "    if repo_dir.exists():\n",
    "        print(f\"Repository exists at {repo_dir.absolute()}\")\n",
    "        os.system(f\"git -C {repo_dir} pull\")\n",
    "        return repo_dir\n",
    "\n",
    "    # 3. Otherwise clone it\n",
    "    print(\"Cloning repository...\")\n",
    "    url = f\"https://github.com/{username}/{repo_name}.git\"\n",
    "    exit_code = os.system(f\"git clone {url}\")\n",
    "\n",
    "    if exit_code != 0:\n",
    "        raise RuntimeError(\"Git clone failed.\")\n",
    "\n",
    "    print(\"Clone successful.\")\n",
    "    return repo_dir\n",
    "\n",
    "repo_dir = clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_groups(fs):\n",
    "    \"\"\"\n",
    "    Create all feature groups needed for the project.\n",
    "    \"\"\"\n",
    "    air_quality_fg = fs.get_or_create_feature_group(\n",
    "        name=\"air_quality\",\n",
    "        description=\"Air Quality characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\", \"datetime\"],\n",
    "        event_time=\"datetime\",\n",
    "        expectation_suite=None,\n",
    "    )\n",
    "\n",
    "    sensor_metadata_fg = fs.get_or_create_feature_group(\n",
    "        name=\"sensor_metadata\",\n",
    "        description=\"Metadata for each air quality sensor\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\"],\n",
    "        expectation_suite=None,\n",
    "    )\n",
    "\n",
    "    weather_fg = fs.get_or_create_feature_group(\n",
    "        name=\"weather\",\n",
    "        description=\"Weather characteristics of each day for all sensors\",\n",
    "        version=1,\n",
    "        primary_key=[\"sensor_id\", \"datetime\"],\n",
    "        event_time=\"datetime\",\n",
    "        expectation_suite=None,\n",
    "    )\n",
    "\n",
    "    return air_quality_fg, sensor_metadata_fg, weather_fg\n",
    "\n",
    "\n",
    "air_quality_fg, sensor_metadata_fg, weather_fg = create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check and Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481debe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (5.00s) \n"
     ]
    }
   ],
   "source": [
    "# Check if data exists\n",
    "try:\n",
    "    aq_data = air_quality_fg.read()\n",
    "    is_first_run = len(aq_data) == 0\n",
    "except:\n",
    "    is_first_run = True\n",
    "\n",
    "# Process and insert data if first run\n",
    "if is_first_run:\n",
    "    all_aq_dfs = []\n",
    "    all_weather_dfs = []\n",
    "    locations = {}\n",
    "\n",
    "    # Process CSV files in data directory\n",
    "    data_dir = os.path.join(root_dir, \"data\")\n",
    "    dir_list = os.listdir(data_dir)\n",
    "    for file in dir_list:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(file_path, AQICN_API_KEY)\n",
    "            \n",
    "            # Clean and process\n",
    "            aq_df = cleaning.clean_and_append_data(aq_df_raw, street, city, country, feed_url, sensor_id)\n",
    "            aq_df[\"datetime\"] = aq_df[\"datetime\"].dt.tz_localize(None)\n",
    "\n",
    "            # Fetch historical weather\n",
    "            weather_df, latitude, longitude = fetchers.get_historical_weather(\n",
    "                city, aq_df, today, feed_url, sensor_id, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            if weather_df is None or len(weather_df) == 0:\n",
    "                print(f\"‚ö†Ô∏è No historical weather for sensor {sensor_id}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            weather_df[\"datetime\"] = weather_df[\"datetime\"].dt.tz_localize(None)\n",
    "\n",
    "            all_aq_dfs.append(aq_df)\n",
    "            all_weather_dfs.append(weather_df)\n",
    "            locations[sensor_id] = {\n",
    "                \"country\": country,\n",
    "                \"city\": city,\n",
    "                \"street\": street,\n",
    "                \"aqicn_url\": feed_url,\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude,\n",
    "            }\n",
    "\n",
    "    if all_aq_dfs:\n",
    "        # Combine and engineer features\n",
    "        aq_df_all = pd.concat(all_aq_dfs, ignore_index=True)\n",
    "        weather_df_all = pd.concat(all_weather_dfs, ignore_index=True)\n",
    "\n",
    "        aq_df_all = feature_engineering.add_rolling_window_feature(aq_df_all, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "        aq_df_all = feature_engineering.add_lagged_features(aq_df_all, column=\"pm25\", lags=[1, 2, 3])\n",
    "        aq_df_all = feature_engineering.add_nearby_sensor_feature(aq_df_all, locations, column=\"pm25_lag_1d\", n_closest=3)\n",
    "        \n",
    "        # Rename and insert\n",
    "        aq_df_all = aq_df_all.rename(columns={\"date\": \"datetime\"})\n",
    "        weather_df_all = weather_df_all.rename(columns={\"date\": \"datetime\"})\n",
    "    \n",
    "        air_quality_fg.insert(aq_df_all)\n",
    "        weather_fg.insert(weather_df_all)\n",
    "\n",
    "        # Insert sensor metadata\n",
    "        metadata_records = []\n",
    "        for sensor_id, loc in locations.items():\n",
    "            metadata_records.append({\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"country\": loc[\"country\"],\n",
    "                \"city\": loc[\"city\"],\n",
    "                \"street\": loc[\"street\"],\n",
    "                \"aqicn_url\": loc[\"aqicn_url\"],\n",
    "                \"latitude\": loc[\"latitude\"],\n",
    "                \"longitude\": loc[\"longitude\"],\n",
    "            })\n",
    "        sensor_metadata_fg.insert(pd.DataFrame(metadata_records))\n",
    "    \n",
    "        print(f\"‚úÖ Inserted {len(aq_df_all)} air quality records\")\n",
    "        print(f\"‚úÖ Inserted {len(weather_df_all)} weather records\")\n",
    "        print(f\"‚úÖ Inserted {len(metadata_records)} sensor metadata records\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No CSV files processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.4. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_air_quality_description(air_quality_fg):\n",
    "    air_quality_fg.update_feature_description(\"datetime\", \"Date and time of measurement of air quality\")\n",
    "    air_quality_fg.update_feature_description(\"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\")\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25\",\n",
    "        \"Particles less than 2.5 micrometers in diameter (fine particles) pose health risk\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\n",
    "        \"pm25_rolling_3d\",\n",
    "        \"3-day rolling mean of PM2.5 from previous days (lagged by 1 day for point-in-time correctness).\",\n",
    "    )\n",
    "    air_quality_fg.update_feature_description(\"pm25_lag_1d\", \"PM2.5 value from 1 day ago.\")\n",
    "    air_quality_fg.update_feature_description(\"pm25_lag_2d\", \"PM2.5 value from 2 days ago.\")\n",
    "    air_quality_fg.update_feature_description(\"pm25_lag_3d\", \"PM2.5 value from 3 days ago.\")\n",
    "\n",
    "\n",
    "def update_sensor_metadata_description(sensor_metadata_fg):\n",
    "    sensor_metadata_fg.update_feature_description(\"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\")\n",
    "    sensor_metadata_fg.update_feature_description(\"city\", \"City where the air quality was measured\")\n",
    "    sensor_metadata_fg.update_feature_description(\"street\", \"Street in the city where the air quality was measured\")\n",
    "    sensor_metadata_fg.update_feature_description(\n",
    "        \"country\",\n",
    "        \"Country where the air quality was measured (sometimes a city in aqicn.org)\",\n",
    "    )\n",
    "    sensor_metadata_fg.update_feature_description(\"aqicn_url\", \"URL to the AQICN feed for this sensor\")\n",
    "    sensor_metadata_fg.update_feature_description(\"latitude\", \"Latitude of the sensor location\")\n",
    "    sensor_metadata_fg.update_feature_description(\"longitude\", \"Longitude of the sensor location\")\n",
    "\n",
    "\n",
    "def update_weather_description(weather_fg):\n",
    "    weather_fg.update_feature_description(\"datetime\", \"Date and time of measurement of weather\")\n",
    "    weather_fg.update_feature_description(\"sensor_id\", \"AQICN sensor identifier (e.g., 59893)\")\n",
    "    weather_fg.update_feature_description(\"city\", \"City where weather is measured/forecast for\")\n",
    "    weather_fg.update_feature_description(\"temperature_2m_mean\", \"Temperature in Celsius\")\n",
    "    weather_fg.update_feature_description(\"precipitation_sum\", \"Precipitation (rain/snow) in mm\")\n",
    "    weather_fg.update_feature_description(\"wind_speed_10m_max\", \"Wind speed at 10m above ground\")\n",
    "    weather_fg.update_feature_description(\"wind_direction_10m_dominant\", \"Dominant wind direction over the day\")\n",
    "    weather_fg.update_feature_description(\"latitude\", \"Latitude of sensor location used for weather retrieval\")\n",
    "    weather_fg.update_feature_description(\"longitude\", \"Longitude of sensor location used for weather retrieval\")\n",
    "\n",
    "\n",
    "update_air_quality_description(air_quality_fg)\n",
    "update_sensor_metadata_description(sensor_metadata_fg)\n",
    "update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.5. Validation Setup\n",
    "Creates Great Expectations validation suites for air quality and weather data with column value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing expectation suite for FG 'air_quality'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1908050\n",
      "Saved expectation suite for FG 'air_quality'.\n",
      "Deleted existing expectation suite for FG 'weather'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1893800\n",
      "Saved expectation suite for FG 'weather'.\n"
     ]
    }
   ],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": True,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"datetime\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"datetime\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\"column\": col, \"type_list\": [\"float\", \"int\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_dateutil_parseable\",\n",
    "        kwargs={\"column\": \"datetime\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# temperature should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"temperature_2m_mean\", \"min_value\": -80, \"max_value\": 60},\n",
    "    )\n",
    ")\n",
    "\n",
    "# latitude/longitude must be valid\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"latitude\", \"min_value\": -90, \"max_value\": 90},\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"longitude\", \"min_value\": -180, \"max_value\": 180},\n",
    "    )\n",
    ")\n",
    "\n",
    "# precipitation and wind speed should be >= 0 (but allow nulls)\n",
    "for col in [\"precipitation_sum\", \"wind_speed_10m_max\"]:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": -0.1,\n",
    "                \"max_value\": None,\n",
    "                \"strict_min\": True,\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.6. Create Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fv/air_quality_complete_fv/version/1\n"
     ]
    }
   ],
   "source": [
    "air_quality_fv = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_complete_fv\",\n",
    "    version=1,\n",
    "    query=air_quality_fg.select_all()\n",
    "        .join(weather_fg.select_all(), on=[\"sensor_id\", \"datetime\"])\n",
    "        .join(sensor_metadata_fg.select_all(), on=\"sensor_id\"),\n",
    "    labels=[\"pm25\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca29e7",
   "metadata": {},
   "source": [
    "## 1.7. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b5c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Hopsworks, using Hopsworks Feature Query Service..    \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    metadata_df = sensor_metadata_fg.read()\n",
    "    if len(metadata_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No sensor metadata found. Run first-time CSV processing first.\")\n",
    "    else:\n",
    "        metadata_df = metadata_df.set_index(\"sensor_id\")\n",
    "        print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading sensor metadata: {e}\")\n",
    "    metadata_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = air_quality_fv.get_batch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d7a56",
   "metadata": {},
   "source": [
    "## 1.8. Incremental Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2e9a1",
   "metadata": {},
   "source": [
    "Detect latest timestamp per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_per_sensor = (\n",
    "    historical_df.groupby(\"sensor_id\")[\"date\"]\n",
    "    .max()\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bfc17",
   "metadata": {},
   "source": [
    "Incremental air quality fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_latest_aq_data(sensor_id, feed_url, since):\n",
    "    response = requests.get(feed_url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"status\") != \"ok\":\n",
    "        print(f\"[WARN] AQICN returned error for {sensor_id}: {data.get('data')}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    time_info = data[\"data\"].get(\"time\")\n",
    "\n",
    "    if isinstance(time_info, dict):\n",
    "        ts_str = time_info.get(\"s\")\n",
    "    elif isinstance(time_info, str):\n",
    "        ts_str = time_info\n",
    "    else:\n",
    "        print(f\"[WARN] Unexpected time format for {sensor_id}: {time_info}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ts = pd.to_datetime(ts_str).tz_localize(None)\n",
    "\n",
    "    if since is not None:\n",
    "        since = since.tz_localize(None)\n",
    "\n",
    "    if since is not None and ts <= since:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    pm25 = (\n",
    "        data[\"data\"]\n",
    "        .get(\"iaqi\", {})\n",
    "        .get(\"pm25\", {})\n",
    "        .get(\"v\", None)\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"sensor_id\": sensor_id,\n",
    "        \"date\": ts,\n",
    "        \"pm25\": pm25,\n",
    "        \"aqicn_url\": feed_url\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7140905",
   "metadata": {},
   "source": [
    "Incremental weather fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_weather(latitude, longitude, since):\n",
    "    if since is None:\n",
    "        since = datetime.now(timezone.utc) - pd.Timedelta(days=7)\n",
    "\n",
    "    since = since.replace(tzinfo=None)\n",
    "\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m\",\n",
    "        \"start_date\": since.strftime(\"%Y-%m-%d\"),\n",
    "        \"end_date\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"),\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "\n",
    "    if \"hourly\" not in data or \"time\" not in data[\"hourly\"]:\n",
    "        print(f\"[WARN] No weather data returned for lat={latitude}, lon={longitude}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(data[\"hourly\"])\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.tz_localize(None)\n",
    "\n",
    "    df = df[df[\"date\"] > since]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43c02d",
   "metadata": {},
   "source": [
    "Incremental ingestion loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg.schema\n",
    "weather_fg.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental.run_incremental_update(\n",
    "    sensor_metadata_fg,\n",
    "    air_quality_fg,\n",
    "    weather_fg,\n",
    "    latest_per_sensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd26786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_df = sensor_metadata_fg.read().set_index(\"sensor_id\")\n",
    "\n",
    "# if len(metadata_df) > 0:\n",
    "#     for sensor_id, meta in metadata_df.iterrows():\n",
    "\n",
    "#         last_ts = latest_per_sensor.get(sensor_id)\n",
    "\n",
    "#         aq_new = fetch_latest_aq_data(\n",
    "#             sensor_id=sensor_id,\n",
    "#             feed_url=meta[\"aqicn_url\"],\n",
    "#             since=last_ts\n",
    "#         )\n",
    "\n",
    "#         if not aq_new.empty:\n",
    "#             aq_new[\"date\"] = aq_new[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "#             # Feature engineering\n",
    "#             aq_new = feature_engineering.add_rolling_window_feature(aq_new, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#             aq_new = feature_engineering.add_lagged_features(aq_new, column=\"pm25\", lags=[1, 2, 3])\n",
    "\n",
    "#             # Clean schema\n",
    "#             aq_new = aq_new.drop(columns=[\"aqicn_url\"], errors=\"ignore\")\n",
    "#             aq_new[\"sensor_id\"] = aq_new[\"sensor_id\"].astype(\"int64\")\n",
    "#             aq_new[\"pm25\"] = aq_new[\"pm25\"].astype(float)\n",
    "\n",
    "#             for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "#                 if col in aq_new.columns:\n",
    "#                     aq_new[col] = aq_new[col].astype(float)\n",
    "            \n",
    "#             air_quality_fg.insert(aq_new)\n",
    "\n",
    "#         weather_new = get_latest_weather(\n",
    "#             latitude=meta[\"latitude\"],\n",
    "#             longitude=meta[\"longitude\"],\n",
    "#             since=last_ts\n",
    "#         )\n",
    "\n",
    "#         weather_new = weather_new.rename(columns={\n",
    "#         \"time\": \"date\",\n",
    "#         \"temperature_2m\": \"temperature_2m_mean\",\n",
    "#         \"wind_speed_10m\": \"wind_speed_10m_max\",\n",
    "#         \"wind_direction_10m\": \"wind_direction_10m_dominant\",\n",
    "#     })\n",
    "#     weather_new[\"date\"] = pd.to_datetime(weather_new[\"date\"], errors=\"coerce\")\n",
    "\n",
    "#     bad_dates = weather_new[\"date\"].isna().sum()\n",
    "#     if bad_dates > 0:\n",
    "#         print(f\"‚ö†Ô∏è Warning: {bad_dates} rows had invalid dates and were dropped.\")\n",
    "        \n",
    "#     weather_new = weather_new.dropna(subset=[\"date\"])\n",
    "# else:\n",
    "#     print(\"‚è≠Ô∏è Skipping incremental updates - no sensors configured yet\")\n",
    "\n",
    "# # Ensure required columns exist\n",
    "# if \"precipitation_sum\" not in weather_new.columns:\n",
    "#     weather_new[\"precipitation_sum\"] = 0.0\n",
    "\n",
    "# if \"wind_direction_10m_dominant\" not in weather_new.columns:\n",
    "#     weather_new[\"wind_direction_10m_dominant\"] = weather_new.get(\n",
    "#         \"wind_direction_10m_dominant\", 0.0\n",
    "# )\n",
    "\n",
    "#     # weather_new[\"wind_direction_10m_dominant\"] = np.nan\n",
    "\n",
    "# # Add metadata\n",
    "# weather_new[\"city\"] = meta[\"city\"]\n",
    "# weather_new[\"latitude\"] = meta[\"latitude\"]\n",
    "# weather_new[\"longitude\"] = meta[\"longitude\"]\n",
    "# weather_new[\"sensor_id\"] = sensor_id\n",
    "\n",
    "# # Cast types\n",
    "# weather_new[\"sensor_id\"] = weather_new[\"sensor_id\"].astype(\"int64\")\n",
    "# weather_new[\"latitude\"] = weather_new[\"latitude\"].astype(\"float64\")\n",
    "# weather_new[\"longitude\"] = weather_new[\"longitude\"].astype(\"float64\")\n",
    "# weather_new[\"temperature_2m_mean\"] = weather_new[\"temperature_2m_mean\"].astype(\"float64\")\n",
    "# weather_new[\"precipitation_sum\"] = weather_new[\"precipitation_sum\"].astype(\"float64\")\n",
    "# weather_new[\"wind_speed_10m_max\"] = weather_new[\"wind_speed_10m_max\"].astype(\"float64\")\n",
    "# weather_new[\"wind_direction_10m_dominant\"] = weather_new[\"wind_direction_10m_dominant\"].astype(\"float64\")\n",
    "\n",
    "# # Final schema selection\n",
    "# weather_new = weather_new[[\n",
    "#     \"sensor_id\",\n",
    "#     \"date\",\n",
    "#     \"temperature_2m_mean\",\n",
    "#     \"precipitation_sum\",\n",
    "#     \"wind_speed_10m_max\",\n",
    "#     \"wind_direction_10m_dominant\",\n",
    "#     \"city\",\n",
    "#     \"latitude\",\n",
    "#     \"longitude\",\n",
    "# ]]\n",
    "\n",
    "# if not weather_new.empty:\n",
    "#     weather_new[\"date\"] = weather_new[\"date\"].dt.tz_localize(None)\n",
    "#     weather_fg.insert(weather_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13eb2f",
   "metadata": {},
   "source": [
    "rebuild feature view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {aq_df_all.shape}\")\n",
    "print(f\"Date range: {aq_df_all['date'].min().date()} to {aq_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {aq_df_all['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {aq_df_all['country'].unique()}\")\n",
    "print(f\"Cities: {aq_df_all['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(aq_df_all['pm25'].describe())\n",
    "print(f\"Missing values: {aq_df_all['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in aq_df_all.columns:\n",
    "        missing = aq_df_all[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(aq_df_all)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "print(f\"Shape: {weather_df_all.shape}\")\n",
    "print(f\"Date range: {weather_df_all['date'].min().date()} to {weather_df_all['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {weather_df_all['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df_all.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df_all[col].min():.2f} to {weather_df_all[col].max():.2f}, Mean: {weather_df_all[col].mean():.2f}, Missing: {weather_df_all[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {weather_df_all['latitude'].min():.3f} to {weather_df_all['latitude'].max():.3f}, Longitude range: {weather_df_all['longitude'].min():.3f} to {weather_df_all['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = aq_df_all.groupby('sensor_id')['date'].count()\n",
    "total_records = len(aq_df_all)\n",
    "data_completeness = (1 - aq_df_all['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (aq_df_all['pm25'] > 100).sum()\n",
    "very_high_count = (aq_df_all['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(aq_df_all) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(aq_df_all['date']).dt.month\n",
    "    monthly_pm25 = aq_df_all.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
