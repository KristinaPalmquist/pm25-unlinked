{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1f60",
   "metadata": {},
   "source": [
    "### 1.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-22 09:05:49,370 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-22 09:05:49,378 INFO: Initializing external client\n",
      "2026-01-22 09:05:49,379 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-22 09:05:50,929 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "2026-01-22 09:05:52,249 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-22 09:05:52,265 INFO: Initializing external client\n",
      "2026-01-22 09:05:52,267 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-22 09:05:53,842 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "current = Path().absolute()\n",
    "for parent in [current] + list(current.parents):\n",
    "    if (parent / \"pyproject.toml\").exists():\n",
    "        root_dir = parent\n",
    "        break\n",
    "else:\n",
    "    root_dir = current\n",
    "\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from confluent_kafka import KafkaException\n",
    "\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings()\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c087c8",
   "metadata": {},
   "source": [
    "### 1.1.2. Load settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db481a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopsworksSettings initialized!\n",
      "2026-01-22 09:00:33,021 INFO: Initializing external client\n",
      "2026-01-22 09:00:33,022 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-22 09:00:34,786 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "2026-01-22 09:00:36,052 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-22 09:00:36,071 INFO: Initializing external client\n",
      "2026-01-22 09:00:36,071 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-22 09:00:37,586 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "#  Load settings \n",
    "settings = config.HopsworksSettings()\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "dataset_api = project.get_dataset_api()\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "### 1.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists at c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f82663",
   "metadata": {},
   "source": [
    "### 1.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check what backfill is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (8.43s) \n",
      "üìã Found 103 sensors already in feature store\n",
      "üìç Loaded locations for 103 existing sensors\n",
      "üìä Total sensors: 103, Already processed: 103, Remaining: 0\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "\n",
    "# Get already processed sensors from feature group\n",
    "existing_sensors = set()\n",
    "sensor_locations = {}\n",
    "\n",
    "# Only try to read if feature group has commits\n",
    "try:\n",
    "    commits = air_quality_fg.commit_details()\n",
    "    \n",
    "    if commits is not None and len(commits) > 0:\n",
    "        # Feature group has data\n",
    "        existing_aq_data = air_quality_fg.read()\n",
    "        existing_sensors = set(existing_aq_data[\"sensor_id\"].unique())\n",
    "        print(f\"üìã Found {len(existing_sensors)} sensors already in feature store\")\n",
    "        \n",
    "        # Build location dict\n",
    "        for _, row in existing_aq_data[[\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\"]].drop_duplicates(subset=[\"sensor_id\"]).iterrows():\n",
    "            sensor_locations[row[\"sensor_id\"]] = (\n",
    "                row[\"latitude\"], \n",
    "                row[\"longitude\"], \n",
    "                row[\"city\"], \n",
    "                row[\"street\"], \n",
    "                row[\"country\"]\n",
    "            )\n",
    "        print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")\n",
    "    else:\n",
    "        # No commits yet, feature group is empty\n",
    "        print(\"üìã No existing sensors found, starting fresh\")\n",
    "        print(\"üìç No existing sensors found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    # Feature group is brand new or error checking commits\n",
    "    print(\"üìã No existing sensors found, starting fresh\")\n",
    "    print(\"üìç No existing sensors found\")\n",
    "\n",
    "total_sensors = len([f for f in dir_list if f.endswith(\".csv\")])\n",
    "remaining = total_sensors - len(existing_sensors)\n",
    "print(f\"üìä Total sensors: {total_sensors}, Already processed: {len(existing_sensors)}, Remaining: {remaining}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75457765",
   "metadata": {},
   "source": [
    "## 1.4. Backfill\n",
    "When performed for the first time, might take a long time if many added sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0609e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All sensors already processed. No backfill needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if total_sensors != len(existing_sensors):\n",
    "    print(\"\\nüöÄ Starting backfill process...\\n\")\n",
    "    # Track processing stats\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    failed_sensors = []  # Track which sensors failed and why\n",
    "\n",
    "    for file in dir_list:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        try:\n",
    "            aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "                file_path, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            sensor_id = int(sensor_id)\n",
    "\n",
    "            # Skip if already processed\n",
    "            if sensor_id in existing_sensors:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Get working feed URL using sensor ID and API token\n",
    "            try:\n",
    "                working_feed_url = fetchers.get_working_feed_url(sensor_id, AQICN_API_KEY)\n",
    "            except Exception as url_err:\n",
    "                print(f\"‚ö†Ô∏è Sensor {sensor_id}: Could not resolve feed URL - {url_err}\")\n",
    "                working_feed_url = feed_url  # Fallback to CSV feed_url if resolution fails\n",
    "\n",
    "            # Get coordinates for this sensor\n",
    "            lat, lon = metadata.get_coordinates(city, street, country)\n",
    "            \n",
    "            if lat is None or lon is None:\n",
    "                print(f\"‚ö†Ô∏è Sensor {sensor_id}: cannot geocode location\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"Geocoding failed\"))\n",
    "                continue\n",
    "\n",
    "            # Clean and prepare air quality data \n",
    "            aq_df = cleaning.clean_and_append_data(\n",
    "                aq_df_raw, sensor_id, \n",
    "                city=city, street=street, country=country,\n",
    "                latitude=lat, longitude=lon, aqicn_url=working_feed_url\n",
    "            )\n",
    "            aq_df = aq_df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"first\")\n",
    "            \n",
    "            # Add features\n",
    "            aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "            aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "            \n",
    "            # Calculate nearby sensor feature using location dict\n",
    "            if len(sensor_locations) > 0:\n",
    "                aq_df = feature_engineering.add_nearby_sensor_feature(\n",
    "                    aq_df, \n",
    "                    sensor_locations,  # Pass dict instead of DataFrame\n",
    "                    n_closest=3\n",
    "                )\n",
    "            else:\n",
    "                aq_df[\"pm25_nearby_avg\"] = 0.0\n",
    "            \n",
    "            # Date range for weather\n",
    "            end_date = aq_df[\"date\"].max().date()\n",
    "            start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "            # Fetch weather\n",
    "            weather_df = fetchers.get_historical_weather(\n",
    "                sensor_id, start_date, end_date, lat, lon\n",
    "            )\n",
    "            \n",
    "            if weather_df is None or len(weather_df) == 0:\n",
    "                print(f\"‚ö†Ô∏è No weather data for sensor {sensor_id}\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"No weather data\"))\n",
    "                continue\n",
    "\n",
    "            # Prepare weather data\n",
    "            weather_df[\"date\"] = weather_df[\"date\"].dt.tz_localize(None)\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df = weather_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"temperature_2m_mean\": \"float64\",\n",
    "                \"precipitation_sum\": \"float64\",\n",
    "                \"wind_speed_10m_max\": \"float64\",\n",
    "                \"wind_direction_10m_dominant\": \"float64\",\n",
    "            })\n",
    "            # Insert without triggering materialization\n",
    "            weather_fg.insert(weather_df, write_options={\"start_offline_materialization\": False})\n",
    "\n",
    "            # Prepare air quality data\n",
    "            aq_df[\"sensor_id\"] = aq_df[\"sensor_id\"].astype(\"int32\")\n",
    "            aq_columns = [f.name for f in air_quality_fg.features]\n",
    "            aq_df = aq_df[aq_columns].astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"pm25\": \"float64\",\n",
    "                \"pm25_lag_1d\": \"float64\",\n",
    "                \"pm25_lag_2d\": \"float64\",\n",
    "                \"pm25_lag_3d\": \"float64\",\n",
    "                \"pm25_rolling_3d\": \"float64\",\n",
    "                \"pm25_nearby_avg\": \"float64\",\n",
    "                \"city\": \"string\",\n",
    "                \"street\": \"string\",\n",
    "                \"country\": \"string\",\n",
    "                \"aqicn_url\": \"string\",\n",
    "                \"latitude\": \"float64\",\n",
    "                \"longitude\": \"float64\",\n",
    "            })\n",
    "            # Insert without triggering materialization\n",
    "            air_quality_fg.insert(aq_df, write_options={\"start_offline_materialization\": False})\n",
    "\n",
    "            existing_sensors.add(sensor_id)\n",
    "            \n",
    "            # Add this sensor's location to dict for subsequent nearby calculations\n",
    "            sensor_locations[sensor_id] = (lat, lon, city, street, country)\n",
    "            \n",
    "            successful += 1\n",
    "            print(f\"‚úÖ Sensor {sensor_id} ({successful}/{remaining} complete)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, f\"{type(e).__name__}: {str(e)[:100]}\"))\n",
    "            print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüéâ Backfill complete!\")\n",
    "    print(f\"üìä Final Summary:\")\n",
    "    print(f\"   ‚úÖ Successfully processed: {successful}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}\")\n",
    "    print(f\"   ‚è© Skipped (already processed): {skipped}\")\n",
    "    print(f\"   üìà Total in feature store: {len(existing_sensors)}/{total_sensors}\")\n",
    "\n",
    "    if len(failed_sensors) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed Sensors Detail:\")\n",
    "        for sid, reason in failed_sensors:\n",
    "            print(f\"   ‚Ä¢ Sensor {sid}: {reason}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ All sensors already processed. No backfill needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.5. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.6. Add Validation to Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing expectation suite for FG 'air_quality'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n",
      "Saved expectation suite for FG 'air_quality'.\n",
      "Deleted existing expectation suite for FG 'weather'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n",
      "Saved expectation suite for FG 'weather'.\n"
     ]
    }
   ],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": 0.0,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": False,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "        kwargs={\"min_value\": 1, \"max_value\": None}\n",
    "    )\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(   \n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Temperature column - allow nulls, should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"min_value\": -80,\n",
    "            \"max_value\": 60,\n",
    "            \"mostly\": 1.0,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Precipitation column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wind column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"min_value\": 0,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "gx.core.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "    kwargs={\"min_value\": 1, \"max_value\": None}\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.7. Create Complete Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select_all(), on=[\"sensor_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e22dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_quality_fg.materialization()\n",
    "# weather_fg.materialization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37ccb4",
   "metadata": {},
   "source": [
    "How to perform materialization??? Started manually for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736295c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# td = air_quality_fv.create_training_data(\n",
    "#     description=\"Initial materialization\",\n",
    "#     data_format=\"parquet\",\n",
    "#     write_options={\"use_spark\": True}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748aa594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trigger materialization job to populate offline feature store\n",
    "# try:\n",
    "#     materialization_job = air_quality_fv.create_training_data(\n",
    "#         description=\"Initial materialization after backfill\",\n",
    "#         data_format=\"parquet\"\n",
    "#     )\n",
    "#     print(\"‚úÖ Materialization job started\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ÑπÔ∏è Materialization will occur automatically when feature view is used: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca29e7",
   "metadata": {},
   "source": [
    "## 1.8. Load Historical Data from Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453f879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (15.89s) \n",
      "üìä Loaded 165668 records from feature view\n"
     ]
    }
   ],
   "source": [
    "air_quality_df = air_quality_fv.get_batch_data()\n",
    "print(f\"üìä Loaded {len(air_quality_df)} records from feature view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fb2796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (22.69s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (3.82s) \n",
      "üìç Extracted metadata for 103 unique sensors\n"
     ]
    }
   ],
   "source": [
    "air_quality_df = air_quality_fg.read()\n",
    "weather_df = weather_fg.read()\n",
    "\n",
    "# Extract unique sensor metadata from air quality feature group\n",
    "metadata_df = air_quality_df[[\"sensor_id\", \"city\", \"street\", \"country\", \"latitude\", \"longitude\"]].drop_duplicates(subset=[\"sensor_id\"])\n",
    "print(f\"üìç Extracted metadata for {len(metadata_df)} unique sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9222740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AIR QUALITY DATA EXPLORATION\n",
      "========================================\n",
      "Shape: (165668, 14)\n",
      "Date range: 2019-12-09 to 2026-01-14\n",
      "Number of unique sensors: 103\n",
      "Countries: ['Sweden']\n",
      "Cities: 85 unique cities\n",
      "\n",
      "üìä PM2.5 Statistics:\n",
      "count    165668.000000\n",
      "mean          3.202714\n",
      "std          11.876914\n",
      "min           0.000000\n",
      "25%           0.900000\n",
      "50%           1.800000\n",
      "75%           3.500000\n",
      "max         999.900000\n",
      "Name: pm25, dtype: float64\n",
      "Missing values: 0\n",
      "\n",
      "üìà Engineered Features Statistics:\n",
      "pm25_rolling_3d: 103 missing values (0.1%)\n",
      "pm25_lag_1d: 103 missing values (0.1%)\n",
      "pm25_lag_2d: 206 missing values (0.1%)\n",
      "pm25_lag_3d: 309 missing values (0.2%)\n",
      "pm25_nearby_avg: 163978 missing values (99.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"Shape: {air_quality_df.shape}\")\n",
    "print(f\"Date range: {air_quality_df['date'].min().date()} to {air_quality_df['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {air_quality_df['sensor_id'].nunique()}\")\n",
    "print(f\"Countries: {metadata_df['country'].unique()}\")\n",
    "print(f\"Cities: {metadata_df['city'].nunique()} unique cities\")\n",
    "\n",
    "print(\"\\nüìä PM2.5 Statistics:\")\n",
    "print(air_quality_df['pm25'].describe())\n",
    "print(f\"Missing values: {air_quality_df['pm25'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìà Engineered Features Statistics:\")\n",
    "for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "    if col in air_quality_df.columns:\n",
    "        missing = air_quality_df[col].isna().sum()\n",
    "        print(f\"{col}: {missing} missing values ({missing/len(air_quality_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è WEATHER DATA EXPLORATION\n",
      "========================================\n",
      "Shape: (112842, 6)\n",
      "Date range: 2018-06-01 to 2026-01-27\n",
      "Number of unique sensors: 103\n",
      "\n",
      "üå°Ô∏è Weather Statistics:\n",
      "temperature_2m_mean:\n",
      "  Range: -26.83 to 26.34, Mean: 6.35, Missing: 0\n",
      "precipitation_sum:\n",
      "  Range: 0.00 to 105.10, Mean: 2.24, Missing: 0\n",
      "wind_speed_10m_max:\n",
      "  Range: 3.05 to 63.46, Mean: 17.72, Missing: 0\n",
      "wind_direction_10m_dominant:\n",
      "  Range: 0.00 to 360.00, Mean: 203.19, Missing: 0\n",
      "\n",
      "üìç Geographic Coverage:\n",
      "Latitude range: 55.474 to 64.751, Longitude range: 11.171 to 20.953\n"
     ]
    }
   ],
   "source": [
    "print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"Shape: {weather_df.shape}\")\n",
    "print(f\"Date range: {weather_df['date'].min().date()} to {weather_df['date'].max().date()}\")\n",
    "print(f\"Number of unique sensors: {metadata_df['sensor_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "    if col in weather_df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Range: {weather_df[col].min():.2f} to {weather_df[col].max():.2f}, Mean: {weather_df[col].mean():.2f}, Missing: {weather_df[col].isna().sum()}\")\n",
    "\n",
    "print(\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"Latitude range: {metadata_df['latitude'].min():.3f} to {metadata_df['latitude'].max():.3f}, Longitude range: {metadata_df['longitude'].min():.3f} to {metadata_df['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "129aa662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó DATA QUALITY & RELATIONSHIPS\n",
      "========================================\n",
      "üìä Overall Data Quality:\n",
      "Total records: 165,668\n",
      "Data completeness: 100.0%\n",
      "Days per sensor - Min: 86, Median: 1872, Max: 2184\n",
      "Sensors with <30 days: 0, >365 days: 100\n",
      "\n",
      "‚ö†Ô∏è Air Quality Levels:\n",
      "Extreme readings (>100 Œºg/m¬≥): 38 (0.0%)\n",
      "Very high readings (>50 Œºg/m¬≥): 142 (0.1%)\n",
      "\n",
      "üóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\n",
      "  Winter: 3.8\n",
      "  Spring: 2.7\n",
      "  Summer: 2.9\n",
      "  Autumn: 3.5\n"
     ]
    }
   ],
   "source": [
    "print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall data completeness\n",
    "sensor_day_counts = air_quality_df.groupby('sensor_id')['date'].count()\n",
    "total_records = len(air_quality_df)\n",
    "data_completeness = (1 - air_quality_df['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "print(f\"üìä Overall Data Quality:\")\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# Extreme values summary\n",
    "extreme_count = (air_quality_df['pm25'] > 100).sum()\n",
    "very_high_count = (air_quality_df['pm25'] > 50).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# Seasonal patterns\n",
    "if len(air_quality_df) > 0:\n",
    "    # Create temporary month column without modifying original DataFrame\n",
    "    temp_months = pd.to_datetime(air_quality_df['date']).dt.month\n",
    "    monthly_pm25 = air_quality_df.groupby(temp_months)['pm25'].mean()\n",
    "    print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "    seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "    for months, season in seasons.items():\n",
    "        season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "        print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
