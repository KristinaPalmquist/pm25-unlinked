{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd7db2c",
   "metadata": {},
   "source": [
    "# 5. Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65abd3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydantic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m     sys.path.append(root_dir)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdded the following directory to the PYTHONPATH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_google_colab():\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\utils\\config.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SecretStr\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseSettings, SettingsConfigDict\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pydantic'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    # Check if repository already exists\n",
    "    repo_dir = Path(\"pm25-forecast-openmeteo-aqicn\")\n",
    "    if repo_dir.exists():\n",
    "        print(f\"Repository already exists at {repo_dir.absolute()}\")\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/KristinaPalmquist/pm25-forecast-openmeteo-aqicn.git\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "def fix_numpy_compatibility():\n",
    "    print(\"Fixing numpy compatibility for hopsworks/pandas...\")\n",
    "    try:\n",
    "        # Use precompiled wheels with compatible versions\n",
    "        !pip install --force-reinstall numpy==1.24.4 pandas==2.0.3\n",
    "        print(\"Numpy and pandas fixed. Please restart runtime and run again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fix attempt failed: {e}\")\n",
    "        print(\"Please manually restart runtime and try again.\")\n",
    "\n",
    "if is_google_colab():\n",
    "    try:\n",
    "        import numpy\n",
    "        numpy.array([1, 2, 3])\n",
    "        import pandas as pd\n",
    "        print(\"Basic packages working correctly\")\n",
    "\n",
    "        clone_repository()\n",
    "        install_dependencies()\n",
    "\n",
    "        import hopsworks\n",
    "        print(\"All packages working correctly\")\n",
    "\n",
    "        root_dir = str(Path().absolute())\n",
    "        print(\"Google Colab environment\")\n",
    "        \n",
    "    except (ValueError, ImportError) as e:\n",
    "        if \"numpy.dtype size changed\" in str(e) or \"numpy.strings\" in str(e) or \"numpy\" in str(e).lower():\n",
    "            fix_numpy_compatibility()\n",
    "            raise SystemExit(\"Please restart runtime (Runtime > Restart runtime) and run the notebook again.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    if root_dir.parts[-1:] == (\"src\",):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == (\"airquality\",):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == (\"notebooks\",):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir)\n",
    "    print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "from utils import config\n",
    "\n",
    "if is_google_colab():\n",
    "    from google.colab import userdata\n",
    "    import hopsworks\n",
    "    project = hopsworks.login(\n",
    "        api_key_value=userdata.get('HOPSWORKS_API_KEY'),\n",
    "        engine=\"python\"\n",
    "    )\n",
    "    AQICN_API_KEY = userdata.get('AQICN_API_KEY')\n",
    "    \n",
    "else:\n",
    "    # Local development - use .env file\n",
    "    settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ef94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python path:\", sys.path[:3])  # First few paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import hopsworks\n",
    "from openai import OpenAI\n",
    "from utils.llm_chain import (\n",
    "    load_model, \n",
    "    get_llm_chain, \n",
    "    generate_response, \n",
    "    generate_response_openai,\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from utils import config\n",
    "from utils import airquality\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_google_colab():\n",
    "    fs = project.get_feature_store()\n",
    "    secrets = hopsworks.get_secrets_api()\n",
    "else:\n",
    "    HOPSWORKS_API_KEY = getattr(settings, 'HOPSWORKS_API_KEY', None)\n",
    "\n",
    "    if HOPSWORKS_API_KEY is not None and hasattr(HOPSWORKS_API_KEY, 'get_secret_value'):\n",
    "        HOPSWORKS_API_KEY = HOPSWORKS_API_KEY.get_secret_value()\n",
    "\n",
    "    project = hopsworks.login(engine=\"python\", api_key_value=HOPSWORKS_API_KEY)\n",
    "\n",
    "    fs = project.get_feature_store()\n",
    "\n",
    "    secrets = hopsworks.get_secrets_api()\n",
    "    AQICN_API_KEY = secrets.get_secret(\"AQICN_API_KEY\").value\n",
    "\n",
    "\n",
    "today = datetime.today().date()\n",
    "\n",
    "# Retrieve feature groups\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name=\"air_quality_all\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name=\"weather_all\",\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b6e57",
   "metadata": {},
   "source": [
    "Set SENSOR_CSV_FILE in .env with the relative path to a sensor to process it, or leave it unset to process all sensors in the `data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d512129",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_google_colab():\n",
    "    sensor_csv_file = None\n",
    "else:\n",
    "    sensor_csv_file = getattr(settings, 'SENSOR_CSV_FILE', None)\n",
    "\n",
    "if sensor_csv_file:\n",
    "    # Read one secret for single sensor mode\n",
    "    _, _, _, _, _, sensor_id = airquality.read_sensor_data(sensor_csv_file)\n",
    "    secret_name = f\"SENSOR_LOCATION_JSON_{sensor_id}\"\n",
    "    location_str = secrets.get_secret(secret_name).value\n",
    "    locations = {sensor_id: json.loads(location_str)}\n",
    "else:\n",
    "    # Read all individual secrets in batch mode\n",
    "    all_secrets = secrets.get_secrets()\n",
    "    locations = {}\n",
    "    for secret in all_secrets:\n",
    "        if secret.name.startswith(\"SENSOR_LOCATION_JSON_\"):\n",
    "            sensor_id = secret.name.replace(\"SENSOR_LOCATION_JSON_\", \"\")\n",
    "            location_str = secrets.get_secret(secret.name).value\n",
    "            if location_str:\n",
    "                locations[sensor_id] = json.loads(location_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1002da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "MODEL_NAME_TEMPLATE = \"air_quality_xgboost_model_{sensor_id}\"\n",
    "\n",
    "retrieved_models = {}\n",
    "\n",
    "for sensor_id in locations.keys():\n",
    "    if len(retrieved_models) > 0:\n",
    "        continue\n",
    "    model_name = MODEL_NAME_TEMPLATE.format(sensor_id=sensor_id)\n",
    "    retrieved_model = None\n",
    "\n",
    "    available_models = mr.get_models(name=model_name)\n",
    "    if available_models:\n",
    "        retrieved_model = max(available_models, key=lambda model: model.version)\n",
    "\n",
    "    saved_model_dir = retrieved_model.download()\n",
    "    model_air_quality = XGBRegressor()\n",
    "    model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
    "    fv = retrieved_model.get_feature_view()\n",
    "    \n",
    "    retrieved_models[sensor_id] = fv, model_air_quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e59fae",
   "metadata": {},
   "source": [
    "## LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad038daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the LLM and its corresponding tokenizer\n",
    "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095652c",
   "metadata": {},
   "source": [
    "## LangChain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "llm_chain = get_llm_chain(\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"The code execution took {duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a6e53",
   "metadata": {},
   "source": [
    "## Domain Specific Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d74be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view, model_air_quality = retrieved_models[list(retrieved_models.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4247d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"Hi!\"\n",
    "response7 = generate_response(\n",
    "    QUESTION7,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b67873",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response1 = generate_response(\n",
    "    QUESTION1, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION11 = \"When and what was the air quality like last week?\"\n",
    "\n",
    "response11 = generate_response(\n",
    "    QUESTION11, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
    "\n",
    "response12 = generate_response(\n",
    "    QUESTION12, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2a = \"What was the air quality like last week?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2a,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION2 = \"What was the air quality like yesterday?\"\n",
    "\n",
    "response2 = generate_response(\n",
    "    QUESTION2,\n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32535b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
    "\n",
    "response3 = generate_response(\n",
    "    QUESTION3, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d20f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
    "\n",
    "response4 = generate_response(\n",
    "    QUESTION4, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
    "\n",
    "response5 = generate_response(\n",
    "    QUESTION5, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
    "\n",
    "response7 = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION7, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view,\n",
    "    weather_fg,\n",
    "    model_air_quality,\n",
    "    model_llm, \n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ffa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecb3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from functions.llm_chain import load_model, get_llm_chain, generate_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ASR pipeline\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    if y.ndim > 1 and y.shape[1] > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "    y /= np.max(np.abs(y))\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "def generate_query_response(user_query, method, openai_api_key=None):\n",
    "    if method == 'Hermes LLM':        \n",
    "        response = generate_response(\n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            model_llm,\n",
    "            tokenizer,\n",
    "            llm_chain,\n",
    "            verbose=False,\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    elif method == 'OpenAI API' and openai_api_key:\n",
    "        client = OpenAI(\n",
    "            api_key=openai_api_key\n",
    "        )\n",
    "        \n",
    "        response = generate_response_openai(   \n",
    "            user_query,\n",
    "            feature_view,\n",
    "            weather_fg,\n",
    "            model_air_quality,\n",
    "            client=client,\n",
    "            verbose=True,\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    else:\n",
    "        return \"Invalid method or missing API key.\"\n",
    "\n",
    "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
    "    if audio_input is not None:\n",
    "        user_query = transcribe(audio_input)\n",
    "    else:\n",
    "        user_query = text_input\n",
    "    \n",
    "    # Check if OpenAI API key is required but not provided\n",
    "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
    "        return \"OpenAI API key is required for this method.\"\n",
    "\n",
    "    if user_query:\n",
    "        return generate_query_response(user_query, method, openai_api_key)\n",
    "    else:\n",
    "        return \"Please provide input either via text or voice.\"\n",
    "    \n",
    "\n",
    "# Setting up the Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=handle_input,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Type here or use voice input...\"), \n",
    "        gr.Audio(), \n",
    "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
    "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
    "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
