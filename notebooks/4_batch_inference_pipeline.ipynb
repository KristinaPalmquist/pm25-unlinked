{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0807c7e3",
   "metadata": {},
   "source": [
    "# 4. Batch Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e540c",
   "metadata": {},
   "source": [
    "## 4.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62eb402",
   "metadata": {},
   "source": [
    "### 4.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e036d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4a169",
   "metadata": {},
   "source": [
    "### 4.1.2. Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a9d05",
   "metadata": {},
   "source": [
    "### 4.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7551d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bff40",
   "metadata": {},
   "source": [
    "### 4.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4450bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fe744",
   "metadata": {},
   "source": [
    "### 4.1.5. Get Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f1d0b",
   "metadata": {},
   "source": [
    "## 4.2. Get Feature Groups and Sensor Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)\n",
    "\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4c59a",
   "metadata": {},
   "source": [
    "## 4.3. Load Data from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530feda4",
   "metadata": {},
   "source": [
    "### 4.3.1. Set Inference Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32127435",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_date = today - timedelta(days=7)  # Get 7 days of historical data for feature engineering\n",
    "future_date = today + timedelta(days=7)  # Get 7 days of future weather forecasts\n",
    "today_short = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Inference period: {past_date} to {future_date}\")\n",
    "print(f\"Today: {today_short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66556865",
   "metadata": {},
   "source": [
    "### 4.3.2. Load Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dced6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batch_weather = weather_fg.filter(\n",
    "        (weather_fg.date >= past_date) & (weather_fg.date <= future_date)\n",
    "    ).read()\n",
    "except Exception:\n",
    "    batch_weather = weather_fg.read()\n",
    "    batch_weather = batch_weather[\n",
    "        (batch_weather[\"date\"] >= past_date) & (batch_weather[\"date\"] <= future_date)\n",
    "    ]\n",
    "\n",
    "batch_weather[\"date\"] = pd.to_datetime(batch_weather[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_weather)} weather records from {past_date} to {future_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca24aa",
   "metadata": {},
   "source": [
    "### 4.3.3. Load Air Quality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batch_airquality = air_quality_fg.filter(\n",
    "        air_quality_fg.date >= past_date\n",
    "    ).read()\n",
    "except Exception:\n",
    "    # batch_airquality = pd.DataFrame()\n",
    "    batch_airquality = air_quality_fg.read()\n",
    "    batch_airquality = batch_airquality[\n",
    "        batch_airquality[\"date\"] >= past_date\n",
    "    ]\n",
    "\n",
    "batch_airquality[\"date\"] = pd.to_datetime(batch_airquality[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_airquality)} air quality records from {past_date} to {today}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f8fb",
   "metadata": {},
   "source": [
    "## 4.4. Model Retrieval\n",
    "Download trained XGBoost models from Hopsworks model registry for each sensor and extract feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c861de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_TEMPLATE = \"air_quality_xgboost_model_{sensor_id}\"\n",
    "\n",
    "retrieved_models = {}\n",
    "\n",
    "for sensor_id in sensor_locations.keys():\n",
    "    model_name = MODEL_NAME_TEMPLATE.format(sensor_id=sensor_id)\n",
    "    \n",
    "    try:\n",
    "        available_models = mr.get_models(name=model_name)\n",
    "        if not available_models:\n",
    "            print(f\"‚ö†Ô∏è No model found for sensor {sensor_id}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_model = max(available_models, key=lambda model: model.version)\n",
    "        saved_model_dir = retrieved_model.download()\n",
    "        \n",
    "        xgb_model = XGBRegressor()\n",
    "        xgb_model.load_model(saved_model_dir + \"/model.json\")\n",
    "        booster = xgb_model.get_booster()\n",
    "        \n",
    "        retrieved_models[sensor_id] = retrieved_model, xgb_model, booster.feature_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model for sensor {sensor_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(retrieved_models)} models from registry\")\n",
    "print(f\"   Total sensors in feature store: {len(sensor_locations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieved {len(retrieved_models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88c6a6",
   "metadata": {},
   "source": [
    "## 4.5. Batch Prediction\n",
    "Merge weather and air quality data, iteratively predict PM2.5 values for forecast days, update engineered features after each prediction, and store results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e6f33",
   "metadata": {},
   "source": [
    "### 4.5.1. Batch Prediction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_CAP_MAX = 150.0  # Maximum reasonable PM2.5 value\n",
    "PREDICTION_CAP_MIN = 0.0    # Minimum reasonable PM2.5 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge historical data with weather data\n",
    "batch_data = pd.merge(batch_weather, batch_airquality, on=[\"date\", \"sensor_id\"], how=\"left\")\n",
    "batch_data = batch_data.sort_values([\"sensor_id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"pm25_rolling_3d\",\n",
    "    \"pm25_lag_1d\",\n",
    "    \"pm25_lag_2d\",\n",
    "    \"pm25_lag_3d\",\n",
    "    \"pm25_nearby_avg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new columns, fill with NaN for now\n",
    "batch_data[\"predicted_pm25\"] = np.nan\n",
    "batch_data[\"days_before_forecast_day\"] = np.nan\n",
    "for col in feature_cols:\n",
    "    batch_data[f\"predicted_{col}\"] = np.nan\n",
    "    \n",
    "# Select all rows where pm25 is NaN and date is today or later\n",
    "# drop any NaN date values, sort the dates in ascending order, get unique dates\n",
    "# forecast days will be a list of dates for which pm2.5 predictions are needed\n",
    "forecast_days = (\n",
    "    batch_data.loc[batch_data[\"pm25\"].isna() & \n",
    "                   (batch_data[\"date\"] >= today.strftime(\"%Y-%m-%d\")), \"date\"]\n",
    "    .dropna()\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38101be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_day in forecast_days:\n",
    "    # context with all sensors up to current day\n",
    "    window = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    day_rows = window[(window[\"date\"] == target_day) & window[\"pm25\"].isna()]\n",
    "\n",
    "    for _, row in day_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        try:\n",
    "            _, xgb_model, model_features = retrieved_models[sensor_id]\n",
    "        except KeyError:\n",
    "            print(f\"No model for sensor {sensor_id}, skipping prediction for {target_day}.\")\n",
    "            continue\n",
    "        features = (row.reindex(model_features).to_frame().T.apply(pd.to_numeric, errors=\"coerce\"))\n",
    "        y_hat = xgb_model.predict(features)[0]\n",
    "\n",
    "        idx = batch_data.index[(batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)][0]\n",
    "        batch_data.at[idx, \"pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"predicted_pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"days_before_forecast_day\"] = (target_day - pd.Timestamp(today)).days + 1\n",
    "\n",
    "    # Recompute features for after filling this day\n",
    "    temp_df = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    temp_df = feature_engineering.add_rolling_window_feature(\n",
    "        temp_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\"\n",
    "    )\n",
    "    temp_df = feature_engineering.add_lagged_features(temp_df, column=\"pm25\", lags=[1, 2, 3])\n",
    "    temp_df = feature_engineering.add_nearby_sensor_feature(\n",
    "        temp_df,\n",
    "        sensor_locations,\n",
    "        column=\"pm25_lag_1d\",\n",
    "        n_closest=3,\n",
    "        new_column=\"pm25_nearby_avg\",\n",
    "    )\n",
    "\n",
    "    current_rows = temp_df[temp_df[\"date\"] == target_day]\n",
    "    for _, row in current_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        mask = (batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)\n",
    "        if mask.any():\n",
    "            for col in feature_cols:\n",
    "                batch_data.loc[mask, f\"predicted_{col}\"] = row[col]\n",
    "\n",
    "predictions = batch_data.loc[\n",
    "    batch_data[\"predicted_pm25\"].notna(),\n",
    "    [\"date\", \"sensor_id\", \"predicted_pm25\", \"days_before_forecast_day\"]\n",
    "    + [f\"predicted_{col}\" for col in feature_cols],\n",
    "].reset_index(drop=True)\n",
    "batch_data.loc[batch_data[\"date\"] > pd.Timestamp(today), \"pm25\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6fa61",
   "metadata": {},
   "source": [
    "### 4.5.2. Assemble Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d621f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions.copy()\n",
    "\n",
    "print(f\"‚úÖ Generated {len(predictions_df)} prediction rows\")\n",
    "print(f\"   Date range: {predictions_df['date'].min()} to {predictions_df['date'].max()}\")\n",
    "print(f\"   Sensors: {predictions_df['sensor_id'].nunique()}\")\n",
    "print(f\"   Forecast days: {sorted(predictions_df['days_before_forecast_day'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308fdb6",
   "metadata": {},
   "source": [
    "## 4.6. Save Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef56771",
   "metadata": {},
   "source": [
    "### 4.6.1. Save Predictions to Feature Store / Model Registry / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"üîç Diagnostic Info:\")\n",
    "# print(f\"\\nPredictions DataFrame columns: {predictions.columns.tolist()}\")\n",
    "# print(f\"\\nPredictions DataFrame shape: {predictions.shape}\")\n",
    "# print(f\"\\nSample data:\")\n",
    "# print(predictions.head())\n",
    "\n",
    "# # Check if the feature group already exists and what schema it has\n",
    "# try:\n",
    "#     existing_fg = fs.get_feature_group(\"air_quality_predictions\", version=1)\n",
    "#     print(f\"\\nüìã Existing feature group schema:\")\n",
    "#     for feat in existing_fg.features:\n",
    "#         print(f\"  - {feat.name} ({feat.type})\")\n",
    "# except:\n",
    "#     print(\"\\n‚úÖ Feature group doesn't exist yet - will be created fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMPORARY fix for type mismatch issue\n",
    "\n",
    "\n",
    "# Create or get predictions feature group (same as in training pipeline)\n",
    "predictions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"aq_predictions\",\n",
    "    version=1,\n",
    "    primary_key=[\"sensor_id\", \"date\", \"days_before_forecast_day\"],\n",
    "    description=\"Air Quality prediction monitoring\",\n",
    "    event_time=\"date\"\n",
    ")\n",
    "\n",
    "# Insert predictions\n",
    "print(f\"üìä Inserting {len(predictions)} prediction rows to {predictions_fg.name}\")\n",
    "print(f\"   Primary keys: {predictions_fg.primary_key}\")\n",
    "print(f\"   Columns: {list(predictions.columns)}\")\n",
    "print(f\"   Date range: {predictions['date'].min()} to {predictions['date'].max()}\")\n",
    "\n",
    "if len(predictions) > 0:\n",
    "\n",
    "    ## DIFFERENCE IN TYPE BETWEEN FEATURE STORES???\n",
    "    # Ensure sensor_id is int64 to match feature group schema (bigint)\n",
    "    if env in (\"job\", \"jupyter\"):\n",
    "        predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int64\")\n",
    "    else:\n",
    "       predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int32\")\n",
    "\n",
    "    max_retries = 5\n",
    "    delay = 2  # seconds\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"üü¶ Insert attempt {attempt}/{max_retries}...\")\n",
    "            predictions_fg.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "            print(\"‚úÖ Insert successful!\")\n",
    "            print(f\"   Total predictions: {len(predictions)}\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Insert failed on attempt {attempt}: {e}\")\n",
    "\n",
    "            if attempt == max_retries:\n",
    "                print(\"‚ùå Max retries reached. Insert failed permanently.\")\n",
    "                raise\n",
    "\n",
    "            sleep_time = delay * (2 ** (attempt - 1))\n",
    "            print(f\"‚è≥ Retrying in {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "\n",
    "    # predictions_fg.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "    print(\"‚úÖ Insert successful!\")\n",
    "    print(f\"   Total predictions: {len(predictions)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No predictions to insert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a292fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create or get predictions feature group (same as in training pipeline)\n",
    "# predictions_fg = fs.get_or_create_feature_group(\n",
    "#     name=\"aq_predictions\",\n",
    "#     version=1,\n",
    "#     primary_key=[\"sensor_id\", \"date\", \"days_before_forecast_day\"],\n",
    "#     description=\"Air Quality prediction monitoring\",\n",
    "#     event_time=\"date\"\n",
    "# )\n",
    "\n",
    "# # Insert predictions\n",
    "# print(f\"üìä Inserting {len(predictions)} prediction rows to {predictions_fg.name}\")\n",
    "# print(f\"   Primary keys: {predictions_fg.primary_key}\")\n",
    "# print(f\"   Columns: {list(predictions.columns)}\")\n",
    "# print(f\"   Date range: {predictions['date'].min()} to {predictions['date'].max()}\")\n",
    "\n",
    "# if len(predictions) > 0:\n",
    "#     # Ensure sensor_id is int64 to match feature group schema (bigint)\n",
    "#     predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int64\")\n",
    "    \n",
    "\n",
    "#     ## DIFFERENCE IN TYPE BETWEEN FEATURE STORES???\n",
    "\n",
    "\n",
    "#     predictions_fg.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "#     print(\"‚úÖ Insert successful!\")\n",
    "#     print(f\"   Total predictions: {len(predictions)}\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No predictions to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519157fb",
   "metadata": {},
   "source": [
    "## 4.7. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure visualization for production vs development\n",
    "SKIP_SENSOR_PLOTS = env == \"job\"  # Skip individual sensor plots when running as Hopsworks job\n",
    "\n",
    "if SKIP_SENSOR_PLOTS:\n",
    "    print(\"‚è≠Ô∏è Skipping individual sensor plots (running as Hopsworks job)\")\n",
    "    print(\"   Heatmap interpolations will still be generated for UI\")\n",
    "else:\n",
    "    print(\"üìä Full visualization enabled (running locally/Jupyter)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f7a97",
   "metadata": {},
   "source": [
    "## 4.8. Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574a8ec",
   "metadata": {},
   "source": [
    "### 4.8.1. Generate Forecast Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sensor plots are skipped in jobs\n",
    "if SKIP_SENSOR_PLOTS:\n",
    "    print(\"‚è≠Ô∏è Skipping forecast plot generation (200+ files)\")\n",
    "else:\n",
    "    dataset_api = project.get_dataset_api()\n",
    "    forecast_paths = []\n",
    "\n",
    "    for sensor_id, location in sensor_locations.items():\n",
    "        sensor_forecast = predictions[predictions[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "        city, street = location[\"city\"], location[\"street\"]\n",
    "        forecast_path = f\"{root_dir}/models/{sensor_id}/images/forecast.png\"\n",
    "        Path(forecast_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fig = visualization.plot_air_quality_forecast(\n",
    "            location[\"city\"],\n",
    "            location[\"street\"],\n",
    "            sensor_forecast,\n",
    "            forecast_path,\n",
    "            hindcast=False,\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        forecast_paths.append((sensor_id, forecast_path))\n",
    "\n",
    "    if not dataset_api.exists(\"Resources/airquality\"):\n",
    "        dataset_api.mkdir(\"Resources/airquality\")\n",
    "\n",
    "    # Upload with retry logic and error handling\n",
    "    upload_success = 0\n",
    "    upload_failed = 0\n",
    "    \n",
    "    for i, (sensor_id, forecast_path) in enumerate(forecast_paths):\n",
    "        max_retries = 3\n",
    "        retry_delay = 2  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                dataset_api.upload(\n",
    "                    forecast_path,\n",
    "                    f\"Resources/airquality/{sensor_id}_{today_short}_forecast.png\",\n",
    "                    overwrite=True,\n",
    "                )\n",
    "                upload_success += 1\n",
    "                if (i + 1) % 20 == 0:  # Progress update every 20 uploads\n",
    "                    print(f\"   Uploaded {i + 1}/{len(forecast_paths)} plots...\")\n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except (ConnectionError, ProtocolError, Timeout, RequestException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"‚ö†Ô∏è Upload failed for sensor {sensor_id} (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to upload for sensor {sensor_id} after {max_retries} attempts: {e}\")\n",
    "                    upload_failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Unexpected error uploading for sensor {sensor_id}: {e}\")\n",
    "                upload_failed += 1\n",
    "                break\n",
    "        \n",
    "        # Small delay between uploads to avoid overwhelming the connection\n",
    "        if i < len(forecast_paths) - 1:\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    print(f\"‚úÖ Upload complete: {upload_success} successful, {upload_failed} failed\")\n",
    "    if upload_success > 0:\n",
    "        print(f\"   Forecast plots available in Hopsworks under {project.get_url()}/settings/fb/path/Resources/airquality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3ebe3",
   "metadata": {},
   "source": [
    "### 4.8.2. Hindcast Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e326719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sensor plots are skipped in jobs\n",
    "if SKIP_SENSOR_PLOTS:\n",
    "    print(\"‚è≠Ô∏è Skipping hindcast plot generation (200+ files)\")\n",
    "else:\n",
    "    # Use predictions_fg (same variable name as in training pipeline)\n",
    "\n",
    "    try:\n",
    "        monitoring_df = predictions_fg.filter(predictions_fg.days_before_forecast_day == 1).read()\n",
    "        monitoring_df[\"date\"] = pd.to_datetime(monitoring_df[\"date\"]).dt.tz_localize(None)\n",
    "        print(f\"‚úÖ Successfully read {len(monitoring_df)} hindcast predictions\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not read monitoring data: {e}\")\n",
    "        print(\"Skipping hindcast analysis...\")\n",
    "        monitoring_df = pd.DataFrame()  # Empty dataframe to prevent further errors\n",
    "\n",
    "    if not monitoring_df.empty:\n",
    "        air_quality_df = air_quality_fg.read()[[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        air_quality_df[\"date\"] = pd.to_datetime(air_quality_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "        for sensor_id, location in sensor_locations.items():\n",
    "            try:\n",
    "                sensor_preds = monitoring_df[monitoring_df[\"sensor_id\"] == sensor_id][[\"date\", \"predicted_pm25\"]]\n",
    "                \n",
    "                if sensor_preds.empty:\n",
    "                    continue\n",
    "                    \n",
    "                merged = sensor_preds.merge(\n",
    "                    air_quality_df[air_quality_df[\"sensor_id\"] == sensor_id][[\"date\", \"pm25\"]],\n",
    "                    on=\"date\",\n",
    "                    how=\"inner\",\n",
    "                ).sort_values(\"date\")\n",
    "\n",
    "                city, street = location[\"city\"], location[\"street\"]\n",
    "                hindcast_path = f\"{root_dir}/models/{sensor_id}/images/hindcast_prediction.png\"\n",
    "                Path(hindcast_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                fig = visualization.plot_air_quality_forecast(\n",
    "                    city,\n",
    "                    street,\n",
    "                    merged if not merged.empty else sensor_preds.assign(pm25=np.nan),\n",
    "                    hindcast_path,\n",
    "                    hindcast=True,\n",
    "                )\n",
    "                if fig is not None:\n",
    "                    plt.close(fig)\n",
    "\n",
    "                dataset_api.upload(\n",
    "                    hindcast_path,\n",
    "                    f\"Resources/airquality/{sensor_id}_{today:%Y-%m-%d}_hindcast.png\",\n",
    "                    overwrite=True,\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing hindcast for sensor {sensor_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abb161",
   "metadata": {},
   "source": [
    "### 4.8.3. IDW Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210c028",
   "metadata": {},
   "source": [
    "#### 4.8.3.1. IDW Interpolation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idw_interpolation(points, values, grid_points, lon_mesh, power=2):\n",
    "    # compute distances between grid points and known data points \n",
    "    distances = cdist(grid_points, points)\n",
    "    # replace 0 with a small value to avoid division by zero\n",
    "    distances = np.where(distances == 0, 1e-10, distances)\n",
    "    # compute weights based on inverse distance\n",
    "    weights = 1.0 / (distances ** power)\n",
    "    # sum of weights for normalization\n",
    "    weights_sum = np.sum(weights, axis=1)\n",
    "    # compute interpolated values - weighted average of known values for each grid point\n",
    "    interpolated = np.sum(weights * values, axis=1) / weights_sum\n",
    "    # reshape to the match grid shape\n",
    "    return interpolated.reshape(lon_mesh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a225b",
   "metadata": {},
   "source": [
    "#### 4.8.3.2. Generate Heatmap Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è Generating heatmap interpolation images (required for UI)\")\n",
    "grid_bounds = tuple(list(json.load(open(f\"{root_dir}/frontend/coordinates.json\")).values())[:4])\n",
    "# grid_bounds = map_bounds[1], map_bounds[0], map_bounds[3], map_bounds[2]  # lat_min, lat_max, lon_min, lon_max\n",
    "print(grid_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_dir = f\"{root_dir}/models/interpolation\"\n",
    "os.makedirs(interpolation_dir, exist_ok=True)\n",
    "\n",
    "interpolation_df = predictions.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db61826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pm25_idw_heatmap(\n",
    "    predictions: pd.DataFrame,\n",
    "    sensor_locations: dict,\n",
    "    forecast_date: datetime,\n",
    "    path: str,\n",
    "    grid_bounds=(11.4, 57.15, 12.5, 58.25),\n",
    "    grid_resolution=800,\n",
    "    power=2,\n",
    "):\n",
    "\n",
    "    df_day = predictions[predictions[\"date\"] == forecast_date].copy()\n",
    "\n",
    "    # Build sensor coordinates and PM2.5 values\n",
    "    sensor_coords_list = []\n",
    "    pm25_values_list = []\n",
    "    \n",
    "    pm25_column = \"predicted_pm25\" if not df_day[\"predicted_pm25\"].isna().all() else \"pm25\"\n",
    "    \n",
    "    for sid in df_day[\"sensor_id\"].unique():\n",
    "        if sid in sensor_locations:\n",
    "            sensor_coords_list.append([sensor_locations[sid][\"longitude\"], sensor_locations[sid][\"latitude\"]])\n",
    "            pm25_val = df_day[df_day[\"sensor_id\"] == sid][pm25_column].iloc[0]\n",
    "            pm25_values_list.append(pm25_val)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    sensor_coords = np.array(sensor_coords_list)\n",
    "    pm25_values = np.array(pm25_values_list)\n",
    "    \n",
    "    # Safety check: need at least 1 sensor with data\n",
    "    if len(sensor_coords) == 0 or len(pm25_values) == 0:\n",
    "        print(f\"‚ö†Ô∏è No sensor data available for {forecast_date}, skipping heatmap generation\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Ensure sensor_coords is 2D (required by cdist)\n",
    "    if sensor_coords.ndim == 1:\n",
    "        sensor_coords = sensor_coords.reshape(1, -1)\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = grid_bounds\n",
    "\n",
    "    lon_grid = np.linspace(min_lon, max_lon, grid_resolution)\n",
    "    lat_grid = np.linspace(min_lat, max_lat, grid_resolution)\n",
    "    lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
    "    grid_points = np.column_stack([lon_mesh.ravel(), lat_mesh.ravel()])\n",
    "\n",
    "    idw_result = idw_interpolation(sensor_coords, pm25_values, grid_points, lon_mesh, power=power)\n",
    "\n",
    "    default_levels = np.array([0, 12, 35, 55, 150, 250, 500])\n",
    "    category_colors = [\"#00e400\", \"#7de400\", \"#ffff00\", \"#ffb000\", \"#ff7e00\", \"#ff4000\", \"#ff0000\", \"#c0007f\", \"#8f3f97\", \"#7e0023\"]\n",
    "    vmin, vmax = default_levels[0], 150\n",
    "    \n",
    "    clipped = np.clip(idw_result, vmin, vmax)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(\n",
    "        clipped,\n",
    "        extent=(min_lon, max_lon, min_lat, max_lat),\n",
    "        origin=\"lower\",\n",
    "        cmap=mcolors.LinearSegmentedColormap.from_list(\"aqi\", category_colors, N=512),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.set_xlim(min_lon, max_lon)\n",
    "    ax.set_ylim(min_lat, max_lat)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    fig.savefig(path, dpi=300, bbox_inches=\"tight\", pad_inches=0, transparent=True)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582de454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any actual PM2.5 data from today if available\n",
    "today_actual = batch_data[batch_data[\"date\"] == today_short].copy()\n",
    "\n",
    "if not today_actual.empty:\n",
    "    # Ensure both columns exist for the plotting function\n",
    "    today_actual = today_actual[[col for col in [\"date\", \"sensor_id\", \"pm25\", \"predicted_pm25\"] if col in today_actual.columns]]\n",
    "    interpolation_df = pd.concat([today_actual, interpolation_df], ignore_index=True)\n",
    "\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "for i, forecast_date in enumerate(sorted(interpolation_df[\"date\"].unique())):\n",
    "    forecast_date_short = forecast_date.strftime(\"%Y-%m-%d\")\n",
    "    days_ahead = (forecast_date - pd.Timestamp(today)).days\n",
    "    output_png = f\"{interpolation_dir}/forecast_interpolation_{days_ahead}d.png\"\n",
    "\n",
    "    plot_pm25_idw_heatmap(\n",
    "        interpolation_df,\n",
    "        sensor_locations,\n",
    "        forecast_date,\n",
    "        output_png,\n",
    "    )\n",
    "\n",
    "    dataset_api.upload(\n",
    "        output_png,\n",
    "        f\"Resources/airquality/interpolation_{today_short}_{forecast_date_short}.png\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Generated {len(interpolation_df['date'].unique())} heatmap interpolation images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3639158",
   "metadata": {},
   "source": [
    "## 4.9. Pipeline Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ac922",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ BATCH INFERENCE PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - Predictions generated: {len(predictions)}\")\n",
    "print(f\"   - Sensors processed: {predictions['sensor_id'].nunique()}\")\n",
    "print(f\"   - Forecast days: {sorted(predictions['days_before_forecast_day'].unique())}\")\n",
    "print(f\"   - Date range: {predictions['date'].min()} to {predictions['date'].max()}\")\n",
    "print(f\"\\nüíæ Data saved to:\")\n",
    "print(f\"   - Feature Group: {predictions_fg.name} (version {predictions_fg.version})\")\n",
    "if not SKIP_SENSOR_PLOTS:\n",
    "    print(f\"   - Sensor plots uploaded to Hopsworks Resources/airquality/\")\n",
    "print(f\"   - Heatmap images uploaded to Hopsworks Resources/airquality/\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
