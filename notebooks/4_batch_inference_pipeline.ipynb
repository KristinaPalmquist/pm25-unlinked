{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0807c7e3",
   "metadata": {},
   "source": [
    "# 4. Batch Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e540c",
   "metadata": {},
   "source": [
    "## 4.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62eb402",
   "metadata": {},
   "source": [
    "### 4.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e036d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "import traceback\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4a169",
   "metadata": {},
   "source": [
    "### 4.1.2. Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    env_path = os.path.join(root_dir, '.env')\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"‚úÖ Loaded .env from: {env_path}\")\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "HOPSWORKS_ENDPOINT = os.environ.get('HOPSWORKS_ENDPOINT')\n",
    "if HOPSWORKS_ENDPOINT:\n",
    "    HOPSWORKS_HOST = HOPSWORKS_ENDPOINT.replace('https://', '').replace('http://', '')\n",
    "    print(f\"üîó Using Hopsworks host: {HOPSWORKS_HOST}\")\n",
    "else:\n",
    "    HOPSWORKS_HOST = None\n",
    "    print(\"üîó Using default Hopsworks host\")\n",
    "\n",
    "print(f\"üîë API key loaded: {HOPSWORKS_API_KEY[:20]}...{HOPSWORKS_API_KEY[-10:]}\")\n",
    "\n",
    "if HOPSWORKS_HOST:\n",
    "    project = hopsworks.login(host=HOPSWORKS_HOST, api_key_value=HOPSWORKS_API_KEY)\n",
    "else:\n",
    "    project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n",
    "print(project.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a9d05",
   "metadata": {},
   "source": [
    "### 4.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7551d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bff40",
   "metadata": {},
   "source": [
    "### 4.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4450bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fe744",
   "metadata": {},
   "source": [
    "### 4.1.5. Get Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f1d0b",
   "metadata": {},
   "source": [
    "## 4.2. Get Feature Groups and Sensor Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)\n",
    "\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4c59a",
   "metadata": {},
   "source": [
    "## 4.3. Load Data from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530feda4",
   "metadata": {},
   "source": [
    "### 4.3.1. Set Inference Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32127435",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_date = today - timedelta(days=7)  # Get 7 days of historical data for feature engineering\n",
    "future_date = today + timedelta(days=7)  # Get 7 days of future weather forecasts\n",
    "today_short = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Inference period: {past_date} to {future_date}\")\n",
    "print(f\"Today: {today_short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66556865",
   "metadata": {},
   "source": [
    "### 4.3.2. Load Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dced6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        batch_weather = weather_fg.filter(\n",
    "            (weather_fg.date >= past_date) & (weather_fg.date <= future_date)\n",
    "        ).read()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"‚ö†Ô∏è Weather read attempt {attempt + 1} failed, retrying...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to read weather data after {max_retries} attempts\")\n",
    "            batch_weather = weather_fg.read()\n",
    "            batch_weather = batch_weather[\n",
    "                (batch_weather[\"date\"] >= past_date) & (batch_weather[\"date\"] <= future_date)\n",
    "            ]\n",
    "\n",
    "batch_weather[\"date\"] = pd.to_datetime(batch_weather[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_weather)} weather records from {past_date} to {future_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca24aa",
   "metadata": {},
   "source": [
    "### 4.3.3. Load Air Quality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        batch_airquality = air_quality_fg.filter(\n",
    "            air_quality_fg.date >= past_date\n",
    "        ).read()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"‚ö†Ô∏è Air quality read attempt {attempt + 1} failed, retrying...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to read air quality data after {max_retries} attempts\")\n",
    "            batch_airquality = air_quality_fg.read()\n",
    "            batch_airquality = batch_airquality[\n",
    "                batch_airquality[\"date\"] >= past_date\n",
    "            ]\n",
    "\n",
    "batch_airquality[\"date\"] = pd.to_datetime(batch_airquality[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_airquality)} air quality records from {past_date} to {today}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f8fb",
   "metadata": {},
   "source": [
    "## 4.4. Model Retrieval\n",
    "Download trained XGBoost models from Hopsworks model registry for each sensor and extract feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c861de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_TEMPLATE = \"air_quality_xgboost_model_{sensor_id}\"\n",
    "\n",
    "retrieved_models = {}\n",
    "\n",
    "for sensor_id in sensor_locations.keys():\n",
    "    model_name = MODEL_NAME_TEMPLATE.format(sensor_id=sensor_id)\n",
    "    \n",
    "    try:\n",
    "        available_models = mr.get_models(name=model_name)\n",
    "        if not available_models:\n",
    "            print(f\"‚ö†Ô∏è No model found for sensor {sensor_id}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_model = max(available_models, key=lambda model: model.version)\n",
    "        saved_model_dir = retrieved_model.download()\n",
    "        \n",
    "        xgb_model = XGBRegressor()\n",
    "        xgb_model.load_model(saved_model_dir + \"/model.json\")\n",
    "        booster = xgb_model.get_booster()\n",
    "        \n",
    "        retrieved_models[sensor_id] = retrieved_model, xgb_model, booster.feature_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model for sensor {sensor_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(retrieved_models)} models from registry\")\n",
    "print(f\"   Total sensors in feature store: {len(sensor_locations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieved {len(retrieved_models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88c6a6",
   "metadata": {},
   "source": [
    "## 4.5. Batch Prediction\n",
    "Merge weather and air quality data, iteratively predict PM2.5 values for forecast days, update engineered features after each prediction, and store results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e6f33",
   "metadata": {},
   "source": [
    "### 4.5.1. Batch Prediction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge historical data with weather data\n",
    "batch_data = pd.merge(batch_weather, batch_airquality, on=[\"date\", \"sensor_id\"], how=\"left\")\n",
    "batch_data = batch_data.sort_values([\"sensor_id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"pm25_rolling_3d\",\n",
    "    \"pm25_lag_1d\",\n",
    "    \"pm25_lag_2d\",\n",
    "    \"pm25_lag_3d\",\n",
    "    \"pm25_nearby_avg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new columns, fill with NaN for now\n",
    "batch_data[\"predicted_pm25\"] = np.nan\n",
    "batch_data[\"days_before_forecast_day\"] = np.nan\n",
    "for col in feature_cols:\n",
    "    batch_data[f\"predicted_{col}\"] = np.nan\n",
    "    \n",
    "\n",
    "forecast_days = [pd.Timestamp(today) + pd.Timedelta(days=i) for i in range(7)]\n",
    "\n",
    "# Ensure today is always included for UI display, even if we have some actual data\n",
    "if today.strftime(\"%Y-%m-%d\") not in forecast_days:\n",
    "    forecast_days = np.append([pd.Timestamp(today)], forecast_days)\n",
    "    print(f\"‚ÑπÔ∏è  Added today ({today}) to forecast_days for UI completeness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38101be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_day in forecast_days:\n",
    "    # context with all sensors up to current day\n",
    "    window = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    day_rows = window[window[\"date\"] == target_day]\n",
    "\n",
    "    for _, row in day_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        try:\n",
    "            _, xgb_model, model_features = retrieved_models[sensor_id]\n",
    "        except KeyError:\n",
    "            print(f\"No model for sensor {sensor_id}, skipping prediction for {target_day}.\")\n",
    "            continue\n",
    "        features = (row.reindex(model_features).to_frame().T.apply(pd.to_numeric, errors=\"coerce\"))\n",
    "        y_hat = xgb_model.predict(features)[0]\n",
    "\n",
    "        idx = batch_data.index[(batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)][0]\n",
    "       \n",
    "        if pd.isna(row[\"pm25\"]):\n",
    "            batch_data.at[idx, \"pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"predicted_pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"days_before_forecast_day\"] = (target_day - pd.Timestamp(today)).days\n",
    "\n",
    "    # Recompute features for after filling this day\n",
    "    temp_df = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    temp_df = feature_engineering.add_rolling_window_feature(\n",
    "        temp_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\"\n",
    "    )\n",
    "    temp_df = feature_engineering.add_lagged_features(temp_df, column=\"pm25\", lags=[1, 2, 3])\n",
    "    temp_df = feature_engineering.add_nearby_sensor_feature(\n",
    "        temp_df,\n",
    "        sensor_locations,\n",
    "        column=\"pm25_lag_1d\",\n",
    "        n_closest=3,\n",
    "        new_column=\"pm25_nearby_avg\",\n",
    "    )\n",
    "\n",
    "    current_rows = temp_df[temp_df[\"date\"] == target_day]\n",
    "    for _, row in current_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        mask = (batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)\n",
    "        if mask.any():\n",
    "            for col in feature_cols:\n",
    "                batch_data.loc[mask, f\"predicted_{col}\"] = row[col]\n",
    "\n",
    "predictions = batch_data.loc[\n",
    "    batch_data[\"predicted_pm25\"].notna(),\n",
    "    [\"date\", \"sensor_id\", \"predicted_pm25\", \"days_before_forecast_day\"]\n",
    "    + [f\"predicted_{col}\" for col in feature_cols],\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6fa61",
   "metadata": {},
   "source": [
    "### 4.5.2. Assemble Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d621f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions.copy()\n",
    "\n",
    "print(f\"‚úÖ Generated {len(predictions_df)} prediction rows\")\n",
    "print(f\"   Date range: {predictions_df['date'].min()} to {predictions_df['date'].max()}\")\n",
    "print(f\"   Sensors: {predictions_df['sensor_id'].nunique()}\")\n",
    "print(f\"   Forecast days: {sorted(predictions_df['days_before_forecast_day'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd25d0",
   "metadata": {},
   "source": [
    "### 4.5.3. Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get predictions feature group (same as in training pipeline)\n",
    "predictions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"aq_predictions\",\n",
    "    version=1,\n",
    "    primary_key=[\"sensor_id\", \"date\", \"days_before_forecast_day\"],\n",
    "    description=\"Air Quality prediction monitoring\",\n",
    "    event_time=\"date\"\n",
    ")\n",
    "\n",
    "# Insert predictions\n",
    "if len(predictions) > 0:\n",
    "\n",
    "    # Difference in types between feature stores\n",
    "    if env in (\"job\", \"jupyter\"):\n",
    "        predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int64\")\n",
    "    else:\n",
    "       predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int32\")\n",
    "\n",
    "    max_retries = 5\n",
    "    delay = 2  # seconds\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            predictions_fg.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "            print(f\"‚úÖ Insert successful on attempt {attempt}\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Insert failed on attempt {attempt}: {type(e).__name__}: {e}\")\n",
    "\n",
    "            if attempt == max_retries:\n",
    "                print(\"‚ùå Max retries reached. Insert failed permanently.\")\n",
    "                raise\n",
    "\n",
    "            sleep_time = delay * (2 ** (attempt - 1))\n",
    "            print(f\"‚è≥ Retrying in {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    print(f\"‚úÖ Inserted {len(predictions)} predictions to {predictions_fg.name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No predictions to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f7a97",
   "metadata": {},
   "source": [
    "## 4.6. Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574a8ec",
   "metadata": {},
   "source": [
    "### 4.6.1. Generate Forecast Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_api = project.get_dataset_api()\n",
    "forecast_paths = []\n",
    "\n",
    "for sensor_id, location in sensor_locations.items():\n",
    "    sensor_forecast = predictions[predictions[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "    city, street = location[\"city\"], location[\"street\"]\n",
    "    forecast_dir = Path(root_dir) / \"frontend\" / \"sensor_images\" / str(sensor_id)\n",
    "    forecast_path = forecast_dir / f\"{sensor_id}_{today_short}_forecast.png\"\n",
    "    forecast_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig = visualization.plot_air_quality_forecast(\n",
    "        location[\"city\"],\n",
    "        location[\"street\"],\n",
    "        sensor_forecast,\n",
    "        str(forecast_path),\n",
    "        hindcast=False,\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    forecast_paths.append((sensor_id, str(forecast_path)))\n",
    "\n",
    "if not dataset_api.exists(\"Resources/airquality\"):\n",
    "    dataset_api.mkdir(\"Resources/airquality\")\n",
    "\n",
    "# Upload with retry logic and error handling\n",
    "upload_success = 0\n",
    "upload_failed = 0\n",
    "\n",
    "for i, (sensor_id, forecast_path) in enumerate(forecast_paths):\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            dataset_api.upload(\n",
    "                forecast_path,\n",
    "                f\"Resources/airquality/{sensor_id}_{today_short}_forecast.png\",\n",
    "                overwrite=True,\n",
    "            )\n",
    "            upload_success += 1\n",
    "            if (i + 1) % 20 == 0:  # Progress update every 20 uploads\n",
    "                print(f\"   Uploaded {i + 1}/{len(forecast_paths)} plots...\")\n",
    "            break  # Success, exit retry loop\n",
    "            \n",
    "        except (ConnectionError, ProtocolError, Timeout, RequestException) as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"‚ö†Ô∏è Upload failed for sensor {sensor_id} (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to upload for sensor {sensor_id} after {max_retries} attempts: {e}\")\n",
    "                upload_failed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error uploading for sensor {sensor_id}: {e}\")\n",
    "            upload_failed += 1\n",
    "            break\n",
    "    \n",
    "    # Small delay between uploads to avoid overwhelming the connection\n",
    "    if i < len(forecast_paths) - 1:\n",
    "        time.sleep(0.1)\n",
    "\n",
    "print(f\"‚úÖ Upload complete: {upload_success} successful, {upload_failed} failed\")\n",
    "if upload_success > 0:\n",
    "    print(f\"   Forecast plots available in Hopsworks under {project.get_url()}/settings/fb/path/Resources/airquality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3ebe3",
   "metadata": {},
   "source": [
    "### 4.6.2. Hindcast Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8464c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    monitoring_df = predictions_fg.filter(predictions_fg.days_before_forecast_day == 1).read()\n",
    "    monitoring_df[\"date\"] = pd.to_datetime(monitoring_df[\"date\"]).dt.tz_localize(None)\n",
    "    print(f\"‚úÖ Successfully read {len(monitoring_df)} hindcast predictions\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not read monitoring data: {e}\")\n",
    "    print(\"Skipping hindcast analysis...\")\n",
    "    monitoring_df = pd.DataFrame()  # Empty dataframe to prevent further errors\n",
    "\n",
    "if not monitoring_df.empty:\n",
    "    air_quality_df = air_quality_fg.read()[[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "    air_quality_df[\"date\"] = pd.to_datetime(air_quality_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    for sensor_id, location in sensor_locations.items():\n",
    "        try:\n",
    "            sensor_preds = monitoring_df[monitoring_df[\"sensor_id\"] == sensor_id][[\"date\", \"predicted_pm25\"]]\n",
    "            \n",
    "            if sensor_preds.empty:\n",
    "                continue\n",
    "                \n",
    "            merged = sensor_preds.merge(\n",
    "                air_quality_df[air_quality_df[\"sensor_id\"] == sensor_id][[\"date\", \"pm25\"]],\n",
    "                on=\"date\",\n",
    "                how=\"inner\",\n",
    "            ).sort_values(\"date\")\n",
    "\n",
    "            city, street = location[\"city\"], location[\"street\"]\n",
    "            hindcast_dir = Path(root_dir) / \"frontend\" / \"sensor_images\" / str(sensor_id)\n",
    "            hindcast_path = hindcast_dir / f\"{sensor_id}_{today_short}_hindcast.png\"\n",
    "            hindcast_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            fig = visualization.plot_air_quality_forecast(\n",
    "                city,\n",
    "                street,\n",
    "                merged if not merged.empty else sensor_preds.assign(pm25=np.nan),\n",
    "                str(hindcast_path),\n",
    "                hindcast=True,\n",
    "            )\n",
    "            if fig is not None:\n",
    "                plt.close(fig)\n",
    "\n",
    "            dataset_api.upload(\n",
    "                str(hindcast_path),\n",
    "                f\"Resources/airquality/{sensor_id}_{today_short}_hindcast.png\",\n",
    "                overwrite=True,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing hindcast for sensor {sensor_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abb161",
   "metadata": {},
   "source": [
    "### 4.6.3. IDW Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a225b",
   "metadata": {},
   "source": [
    "#### 4.6.3.1. Load Grid Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüó∫Ô∏è Loading grid bounds for interpolation...\")\n",
    "\n",
    "try:\n",
    "    with open(f\"{root_dir}/frontend/coordinates.json\") as f:\n",
    "        coords_data = json.load(f)\n",
    "    grid_bounds = (\n",
    "        float(coords_data[\"MIN_LONGITUDE\"]),\n",
    "        float(coords_data[\"MIN_LATITUDE\"]),\n",
    "        float(coords_data[\"MAX_LONGITUDE\"]),\n",
    "        float(coords_data[\"MAX_LATITUDE\"])\n",
    "    )\n",
    "\n",
    "    print(f\"   Loaded grid bounds from coordinates.json: {grid_bounds}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load coordinates.json: {e}\")\n",
    "    print(\"   Falling back to sensor-based bounds...\")\n",
    "    \n",
    "    lons = [loc['longitude'] for loc in sensor_locations.values()]\n",
    "    lats = [loc['latitude'] for loc in sensor_locations.values()]\n",
    "    \n",
    "    min_lon, max_lon = min(lons), max(lons)\n",
    "    min_lat, max_lat = min(lats), max(lats)\n",
    "    \n",
    "    # Add 10% padding\n",
    "    lon_padding = (max_lon - min_lon) * 0.1\n",
    "    lat_padding = (max_lat - min_lat) * 0.1\n",
    "    \n",
    "    grid_bounds = (\n",
    "        float(min_lon - lon_padding),\n",
    "        float(min_lat - lat_padding),\n",
    "        float(max_lon + lon_padding),\n",
    "        float(max_lat + lat_padding)\n",
    "    )\n",
    "    \n",
    "    print(f\"   Calculated fallback grid bounds: {grid_bounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be89a30",
   "metadata": {},
   "source": [
    "#### 4.6.3.2. Prepare Interpolation DataFrame\n",
    "Actual Day-0 + Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüßÆ Preparing interpolation DataFrame...\")\n",
    "\n",
    "interpolation_df = predictions.copy()\n",
    "interpolation_df[\"predicted_pm25\"] = pd.to_numeric(interpolation_df[\"predicted_pm25\"], errors='coerce')\n",
    "\n",
    "# Get today's actual measurements\n",
    "today_actual = batch_data[batch_data[\"date\"] == today_short].copy()\n",
    "\n",
    "if not today_actual.empty:\n",
    "    today_actual = today_actual[[\"date\", \"sensor_id\", \"pm25\"]].copy()\n",
    "    today_actual[\"pm25\"] = pd.to_numeric(today_actual[\"pm25\"], errors='coerce')\n",
    "    today_actual = today_actual[today_actual[\"pm25\"].notna()]\n",
    "    \n",
    "    if len(today_actual) > 0:\n",
    "        print(f\"   Using {len(today_actual)} actual sensor measurements for day 0\")\n",
    "        # Remove predicted rows for today\n",
    "        interpolation_df = interpolation_df[interpolation_df[\"date\"] != pd.Timestamp(today)]\n",
    "        # Add actual measurements for today\n",
    "        interpolation_df = pd.concat([today_actual, interpolation_df], ignore_index=True)\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No valid actual pm25 for today ‚Äî using predictions\")\n",
    "        today_preds = interpolation_df[interpolation_df[\"date\"] == pd.Timestamp(today)].copy()\n",
    "        if not today_preds.empty:\n",
    "            today_preds[\"pm25\"] = today_preds[\"predicted_pm25\"]\n",
    "            interpolation_df = interpolation_df[interpolation_df[\"date\"] != pd.Timestamp(today)]\n",
    "            interpolation_df = pd.concat([today_preds, interpolation_df], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No batch_data for today ‚Äî using predictions\")\n",
    "    today_preds = interpolation_df[interpolation_df[\"date\"] == pd.Timestamp(today)].copy()\n",
    "    if not today_preds.empty:\n",
    "        today_preds[\"pm25\"] = today_preds[\"predicted_pm25\"]\n",
    "        interpolation_df = interpolation_df[interpolation_df[\"date\"] != pd.Timestamp(today)]\n",
    "        interpolation_df = pd.concat([today_preds, interpolation_df], ignore_index=True)\n",
    "\n",
    "# Diagnostics\n",
    "print(\"\\nüîç Interpolation DataFrame dtypes:\")\n",
    "print(f\"   predicted_pm25: {interpolation_df['predicted_pm25'].dtype}\")\n",
    "if \"pm25\" in interpolation_df.columns:\n",
    "    print(f\"   pm25: {interpolation_df['pm25'].dtype}\")\n",
    "    print(f\"   Sample pm25 values: {interpolation_df[interpolation_df['pm25'].notna()]['pm25'].head(3).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0e5df",
   "metadata": {},
   "source": [
    "#### 4.6.3.3. Generate IDW Heatmap Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüó∫Ô∏è Generating heatmap interpolation images...\")\n",
    "\n",
    "interpolation_dir = f\"{root_dir}/models/interpolation\"\n",
    "frontend_interpolation_dir = f\"{root_dir}/frontend/interpolation\"\n",
    "\n",
    "os.makedirs(interpolation_dir, exist_ok=True)\n",
    "os.makedirs(frontend_interpolation_dir, exist_ok=True)\n",
    "\n",
    "unique_dates = sorted(interpolation_df[\"date\"].unique())\n",
    "\n",
    "print(f\"   Total forecast dates: {len(unique_dates)}\")\n",
    "print(f\"   Today: {today}\")\n",
    "print(f\"   Dates: {[str(d)[:10] for d in unique_dates]}\")\n",
    "\n",
    "successful_images = 0\n",
    "failed_images = 0\n",
    "\n",
    "for i, forecast_date in enumerate(unique_dates):\n",
    "    forecast_date_short = forecast_date.strftime(\"%Y-%m-%d\")\n",
    "    days_ahead = (forecast_date - pd.Timestamp(today)).days\n",
    "\n",
    "    output_png = f\"{interpolation_dir}/forecast_interpolation_{days_ahead}d.png\"\n",
    "    frontend_png = f\"{frontend_interpolation_dir}/forecast_interpolation_{days_ahead}d.png\"\n",
    "\n",
    "    try:\n",
    "        visualization.plot_pm25_idw_heatmap(\n",
    "            interpolation_df,\n",
    "            sensor_locations,\n",
    "            forecast_date,\n",
    "            output_png,\n",
    "            grid_bounds=grid_bounds,\n",
    "            today=today,\n",
    "        )\n",
    "\n",
    "        shutil.copy2(output_png, frontend_png)\n",
    "\n",
    "        if days_ahead == 0:\n",
    "            from IPython.display import Image, display\n",
    "            display(Image(filename=output_png))\n",
    "\n",
    "        print(\"‚úÖ\")\n",
    "        successful_images += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {type(e).__name__}: {str(e)[:100]}\")\n",
    "        failed_images += 1\n",
    "\n",
    "print(f\"\\nüìä Heatmap generation complete: {successful_images} successful, {failed_images} failed\")\n",
    "\n",
    "if failed_images > 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: {failed_images} images failed to generate\")\n",
    "if successful_images == 0:\n",
    "    raise Exception(\"‚ùå All heatmap images failed ‚Äî pipeline cannot complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3639158",
   "metadata": {},
   "source": [
    "## 4.7. Export Predictions for Frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01eea5",
   "metadata": {},
   "source": [
    "### 4.7.1. Prepare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì¶ Preparing predictions for frontend export...\")\n",
    "\n",
    "# Debug: show available forecast days and prediction count before merging with metadata\n",
    "print(f\"\\nüîç Debug - Available days_before_forecast_day values: {sorted(predictions['days_before_forecast_day'].unique())}\")\n",
    "print(f\"   Total predictions: {len(predictions)}\")\n",
    "\n",
    "frontend_predictions = predictions.copy()\n",
    "\n",
    "\n",
    "# Build metadata dataframe safely\n",
    "sensor_metadata_rows = []\n",
    "for sensor_id, location_data in sensor_locations.items():\n",
    "    sensor_metadata_rows.append({\n",
    "        'sensor_id': int(sensor_id),  # force int\n",
    "        'latitude': float(location_data['latitude']),\n",
    "        'longitude': float(location_data['longitude']),\n",
    "        'city': location_data.get('city', ''),\n",
    "        'street': location_data.get('street', '')\n",
    "    })\n",
    "\n",
    "sensor_metadata_df = pd.DataFrame(sensor_metadata_rows)\n",
    "\n",
    "# Ensure consistent dtypes\n",
    "frontend_predictions['sensor_id'] = frontend_predictions['sensor_id'].astype(int)\n",
    "sensor_metadata_df['sensor_id'] = sensor_metadata_df['sensor_id'].astype(int)\n",
    "\n",
    "print(f\"   Metadata sensors: {sensor_metadata_df['sensor_id'].nunique()}\")\n",
    "print(f\"   Prediction sensors: {frontend_predictions['sensor_id'].nunique()}\")\n",
    "\n",
    "# Merge predictions with metadata\n",
    "merged = frontend_predictions.merge(sensor_metadata_df, on='sensor_id', how='left')\n",
    "\n",
    "# Validate coordinates\n",
    "missing_coords = merged[merged['latitude'].isna() | merged['longitude'].isna()]\n",
    "if not missing_coords.empty:\n",
    "    print(\"\\n‚ùå ERROR: Missing coordinates for these sensor IDs:\")\n",
    "    print(missing_coords['sensor_id'].unique())\n",
    "    raise ValueError(\"Some prediction rows have no matching sensor metadata. Fix required.\")\n",
    "\n",
    "frontend_predictions = merged\n",
    "print(f\"   ‚úÖ Coordinates added for {len(frontend_predictions)} rows\")\n",
    "print(frontend_predictions.head())\n",
    "\n",
    "# Convert datetime to string for JSON\n",
    "frontend_predictions[\"date\"] = frontend_predictions[\"date\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf08af",
   "metadata": {},
   "source": [
    "### 4.7.2. Export Predictions JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15951942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions as JSON for frontend\n",
    "print(\"\\nüì¶ Exporting predictions for frontend...\")\n",
    "\n",
    "# Export to frontend directory\n",
    "predictions_json_path = f\"{root_dir}/frontend/predictions.json\"\n",
    "frontend_predictions.to_json(predictions_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"‚úÖ Exported {len(frontend_predictions)} predictions to frontend/predictions.json\")\n",
    "print(f\"   Sensors: {frontend_predictions['sensor_id'].nunique()}\")\n",
    "if len(frontend_predictions) > 0:\n",
    "    print(f\"   Date range: {frontend_predictions['date'].min()} to {frontend_predictions['date'].max()}\" if len(frontend_predictions) > 0 else \"   No predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f8f35",
   "metadata": {},
   "source": [
    "## 4.8. Commit Frontend Artifacts to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì§ Committing predictions and interpolation images to git...\")\n",
    "\n",
    "try:\n",
    "    # List PNGs for debugging\n",
    "    frontend_pngs = glob.glob(f\"{root_dir}/frontend/interpolation/*.png\")\n",
    "    print(f\"   Found {len(frontend_pngs)} PNGs in frontend/interpolation/\")\n",
    "    if frontend_pngs:\n",
    "        print(f\"   Example: {os.path.basename(frontend_pngs[0])}\")\n",
    "\n",
    "    # Configure git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"Hopsworks Bot\"], cwd=root_dir, check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"bot@hopsworks.ai\"], cwd=root_dir, check=True)\n",
    "\n",
    "    # Stage the generated artifacts\n",
    "    print(\"   Staging frontend artifacts...\")\n",
    "    subprocess.run([\"git\", \"add\", \"-f\", \"frontend/predictions.json\"], cwd=root_dir, check=True)\n",
    "    subprocess.run([\"git\", \"add\", \"-f\", \"frontend/interpolation/\"], cwd=root_dir, check=True)\n",
    "    subprocess.run([\"git\", \"add\", \"-f\", \"frontend/sensor_images/\"], cwd=root_dir, check=True)\n",
    "\n",
    "    # Check if anything changed\n",
    "    status = subprocess.run(\n",
    "        [\"git\", \"status\", \"--porcelain\"],\n",
    "        cwd=root_dir,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if not status.stdout.strip():\n",
    "        print(\"‚ÑπÔ∏è  No changes to commit ‚Äî predictions and heatmaps already up to date\")\n",
    "\n",
    "    else:\n",
    "        print(\"   Changes detected:\")\n",
    "        print(status.stdout)\n",
    "\n",
    "        # Commit ONLY the frontend artifacts\n",
    "        commit = subprocess.run(\n",
    "            [\n",
    "                \"git\", \"commit\",\n",
    "                \"-m\", f\"Update predictions and heatmaps - {today_short}\",\n",
    "                \"--\", \"frontend/predictions.json\", \"frontend/interpolation/\", \"frontend/sensor_images/\"\n",
    "            ],\n",
    "            cwd=root_dir,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if commit.returncode == 0:\n",
    "            print(\"‚úÖ Commit successful\")\n",
    "\n",
    "            push = subprocess.run(\n",
    "                [\"git\", \"push\"],\n",
    "                cwd=root_dir,\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "\n",
    "            if push.returncode == 0:\n",
    "                print(\"‚úÖ Successfully pushed updates to GitHub\")\n",
    "                print(\"   Netlify will rebuild automatically\")\n",
    "            else:\n",
    "                print(\"‚ùå Git push failed\")\n",
    "                print(push.stderr)\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Commit failed\")\n",
    "            print(commit.stderr)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Git operation error: {e}\")\n",
    "    print(\"   Files exported locally but not pushed to git\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77a05f",
   "metadata": {},
   "source": [
    "### 4.9. Pipeline Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ac922",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ BATCH INFERENCE PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - Predictions generated: {len(predictions)}\")\n",
    "print(f\"   - Heatmap images created: {len(interpolation_df['date'].unique())}\")\n",
    "print(f\"   - Feature group: {predictions_fg.name} (v{predictions_fg.version})\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
