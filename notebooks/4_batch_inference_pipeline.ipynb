{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0807c7e3",
   "metadata": {},
   "source": [
    "# 4. Batch Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e540c",
   "metadata": {},
   "source": [
    "## 4.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62eb402",
   "metadata": {},
   "source": [
    "### 4.1.1. Import Libraries and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e036d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib3.exceptions import ProtocolError  \n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "from confluent_kafka import KafkaException\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings()\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a9d05",
   "metadata": {},
   "source": [
    "### 4.1.2. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bff40",
   "metadata": {},
   "source": [
    "### 4.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4450bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f1d0b",
   "metadata": {},
   "source": [
    "## 4.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe79872",
   "metadata": {},
   "source": [
    "## 4.4. Sensor Location Loading \n",
    "Load sensor location metadata from Hopsworks secrets for all sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_secrets = secrets.get_secrets()\n",
    "locations = {}\n",
    "for secret in all_secrets:\n",
    "    if secret.name.startswith(\"SENSOR_LOCATION_JSON_\"):\n",
    "        sensor_id = secret.name.replace(\"SENSOR_LOCATION_JSON_\", \"\")\n",
    "        location_str = secrets.get_secret(secret.name).value\n",
    "        if location_str:\n",
    "            locations[sensor_id] = json.loads(location_str)\n",
    "print(f\"Retrieved locations for {len(locations)} sensors from Hopsworks Secrets Manager.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4c59a",
   "metadata": {},
   "source": [
    "## 4.5. Load Data from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530feda4",
   "metadata": {},
   "source": [
    "### 4.5.1. Set Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32127435",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_date = today - timedelta(days=7)  # Get 7 days of historical data for feature engineering\n",
    "future_date = today + timedelta(days=7)  # Get 7 days of future weather forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66556865",
   "metadata": {},
   "source": [
    "### 4.5.2. Weather Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dced6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batch_weather = weather_fg.filter(\n",
    "        (weather_fg.date >= past_date) & (weather_fg.date <= future_date)\n",
    "    ).read()\n",
    "except Exception:\n",
    "    batch_weather = weather_fg.read()\n",
    "    batch_weather = batch_weather[\n",
    "        (batch_weather[\"date\"] >= past_date) & (batch_weather[\"date\"] <= future_date)\n",
    "    ]\n",
    "\n",
    "batch_weather[\"date\"] = pd.to_datetime(batch_weather[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_weather)} weather records from {past_date} to {future_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca24aa",
   "metadata": {},
   "source": [
    "### 4.5.3. Air Quality Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    batch_airquality = air_quality_fg.filter(\n",
    "        air_quality_fg.date >= past_date\n",
    "    ).read()\n",
    "except Exception:\n",
    "    # batch_airquality = pd.DataFrame()\n",
    "    batch_airquality = air_quality_fg.read()\n",
    "    batch_airquality = batch_airquality[\n",
    "        batch_airquality[\"date\"] >= past_date\n",
    "    ]\n",
    "\n",
    "batch_airquality[\"date\"] = pd.to_datetime(batch_airquality[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_airquality)} air quality records from {past_date} to {today}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f8fb",
   "metadata": {},
   "source": [
    "## 4.7. Model Retrieval\n",
    "Download trained XGBoost models from Hopsworks model registry for each sensor and extract feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c861de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "MODEL_NAME_TEMPLATE = \"air_quality_xgboost_model_{sensor_id}\"\n",
    "\n",
    "# model, model_dir, features\n",
    "retrieved_models = {}\n",
    "\n",
    "for sensor_id in locations.keys():\n",
    "    model_name = MODEL_NAME_TEMPLATE.format(sensor_id=sensor_id)\n",
    "    retrieved_model = None\n",
    "\n",
    "    available_models = mr.get_models(name=model_name)\n",
    "    if available_models:\n",
    "        retrieved_model = max(available_models, key=lambda model: model.version)\n",
    "\n",
    "    if retrieved_model is None:\n",
    "        print(f\"No model found for sensor {sensor_id}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    saved_model_dir = retrieved_model.download()  \n",
    "    \n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(saved_model_dir + \"/model.json\")\n",
    "    xgb_model = XGBRegressor()\n",
    "    xgb_model._Booster = booster\n",
    "\n",
    "    retrieved_models[sensor_id] = retrieved_model, xgb_model, booster.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieved {len(retrieved_models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88c6a6",
   "metadata": {},
   "source": [
    "## 4.8. Batch Prediction Loop\n",
    "Merge weather and air quality data, iteratively predict PM2.5 values for forecast days, update engineered features after each prediction, and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_CAP_MAX = 150.0  # Maximum reasonable PM2.5 value\n",
    "PREDICTION_CAP_MIN = 0.0    # Minimum reasonable PM2.5 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge historical data with weather data\n",
    "batch_data = pd.merge(batch_weather, batch_airquality, on=[\"date\", \"sensor_id\"], how=\"left\")\n",
    "batch_data = batch_data.sort_values([\"sensor_id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"pm25_rolling_3d\",\n",
    "    \"pm25_lag_1d\",\n",
    "    \"pm25_lag_2d\",\n",
    "    \"pm25_lag_3d\",\n",
    "    \"pm25_nearby_avg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for pedicted pm2.5 values, fill with NaN for now\n",
    "batch_data[\"predicted_pm25\"] = np.nan\n",
    "# Create a new column for days before forecast day, fill with NaN for now\n",
    "batch_data[\"days_before_forecast_day\"] = np.nan\n",
    "# For each feature name in feature_cols list, \n",
    "# create a new column for predicted feature values, fill with NaN for now\n",
    "for col in feature_cols:\n",
    "    batch_data[f\"predicted_{col}\"] = np.nan\n",
    "# Select all rows where pm25 is NaN and date is today or later\n",
    "# drop any NaN date values, sort the dates in ascending order, get unique dates\n",
    "# forecast days will be a list of dates for which pm2.5 predictions are needed\n",
    "forecast_days = (\n",
    "    batch_data.loc[batch_data[\"pm25\"].isna() & \n",
    "                   (batch_data[\"date\"] >= today.strftime(\"%Y-%m-%d\")), \"date\"]\n",
    "    .dropna()\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track sensors processed to prevent duplicates and count progress\n",
    "# sensors_processed = set()\n",
    "# warning_count = 0\n",
    "# MAX_WARNINGS = 3  # Reduced to minimize output noise\n",
    "# predictions_made = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(forecast_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38101be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_day in forecast_days:\n",
    "    # context with all sensors up to current day\n",
    "    window = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    day_rows = window[(window[\"date\"] == target_day) & window[\"pm25\"].isna()]\n",
    "\n",
    "    for _, row in day_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        try:\n",
    "            _, xgb_model, model_features = retrieved_models[sensor_id]\n",
    "        except KeyError:\n",
    "            print(f\"No model for sensor {sensor_id}, skipping prediction for {target_day}.\")\n",
    "            continue\n",
    "        features = (row.reindex(model_features).to_frame().T.apply(pd.to_numeric, errors=\"coerce\"))\n",
    "        y_hat = xgb_model.predict(features)[0]\n",
    "\n",
    "        idx = batch_data.index[(batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)][0]\n",
    "        batch_data.at[idx, \"pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"predicted_pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"days_before_forecast_day\"] = (target_day - pd.Timestamp(today)).days + 1\n",
    "\n",
    "    # Recompute features for after filling this day\n",
    "    temp_df = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    temp_df = feature_engineering.add_rolling_window_feature(\n",
    "        temp_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\"\n",
    "    )\n",
    "    temp_df = feature_engineering.add_lagged_features(temp_df, column=\"pm25\", lags=[1, 2, 3])\n",
    "    temp_df = feature_engineering.add_nearby_sensor_feature(\n",
    "        temp_df,\n",
    "        locations,\n",
    "        column=\"pm25_lag_1d\",\n",
    "        n_closest=3,\n",
    "        new_column=\"pm25_nearby_avg\",\n",
    "    )\n",
    "\n",
    "    current_rows = temp_df[temp_df[\"date\"] == target_day]\n",
    "    for _, row in current_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        mask = (batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)\n",
    "        if mask.any():\n",
    "            for col in feature_cols:\n",
    "                batch_data.loc[mask, f\"predicted_{col}\"] = row[col]\n",
    "\n",
    "predictions = batch_data.loc[\n",
    "    batch_data[\"predicted_pm25\"].notna(),\n",
    "    [\"date\", \"sensor_id\", \"predicted_pm25\", \"days_before_forecast_day\"]\n",
    "    + [f\"predicted_{col}\" for col in feature_cols],\n",
    "].reset_index(drop=True)\n",
    "batch_data.loc[batch_data[\"date\"] > pd.Timestamp(today), \"pm25\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add after model retrieval section\n",
    "# all_models = mr.get_models()\n",
    "# print(f\"Total models in registry: {len(all_models)}\")\n",
    "# for model in all_models[:5]:  # Show first 5\n",
    "#     print(f\"  - {model.name} (v{model.version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308fdb6",
   "metadata": {},
   "source": [
    "## 4.9. Save Predictions\n",
    "Export prediction results to CSV file in models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b576792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_data.to_csv(f\"{root_dir}/models/predictions.csv\", columns=batch_data.columns, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef56771",
   "metadata": {},
   "source": [
    "save predictions to feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ee961",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"air_quality_predictions\",\n",
    "    version=1,\n",
    "    primary_key=[\"sensor_id\", \"date\"],\n",
    "    description=\"Daily PM2.5 predictions per sensor\",\n",
    "    event_time=\"date\"\n",
    ")\n",
    "\n",
    "# Use the predictions DataFrame that was already created above\n",
    "print(f\"Inserting {len(predictions)} prediction rows into feature group...\")\n",
    "if len(predictions) > 0:\n",
    "    predictions_fg.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "else:\n",
    "    print(\"⚠️ No predictions to insert. Check if forecast_days is empty or prediction loop ran correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574a8ec",
   "metadata": {},
   "source": [
    "## 4.10. Generate Forecast Plots\n",
    "Create forecast visualization plots for each sensor and upload them to Hopsworks dataset storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_paths = []\n",
    "\n",
    "for sensor_id, location in locations.items():\n",
    "    sensor_forecast = predictions[predictions[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "    city, street = location[\"city\"], location[\"street\"]\n",
    "    forecast_path = f\"{root_dir}/models/{sensor_id}/images/forecast.png\"\n",
    "    Path(forecast_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig = visualization.plot_air_quality_forecast(\n",
    "        location[\"city\"],\n",
    "        location[\"street\"],\n",
    "        sensor_forecast,\n",
    "        forecast_path,\n",
    "        hindcast=False,\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    forecast_paths.append((sensor_id, forecast_path))\n",
    "\n",
    "dataset_api = project.get_dataset_api()\n",
    "today_short = today.strftime(\"%Y-%m-%d\")\n",
    "if not dataset_api.exists(\"Resources/airquality\"):\n",
    "    dataset_api.mkdir(\"Resources/airquality\")\n",
    "\n",
    "for sensor_id, forecast_path in forecast_paths:\n",
    "    dataset_api.upload(\n",
    "        forecast_path,\n",
    "        f\"Resources/airquality/{sensor_id}_{today_short}_forecast.png\",\n",
    "        overwrite=True,\n",
    "    )\n",
    "print(f\"Forecast plots available in Hopsworks under {project.get_url()}/settings/fb/path/Resources/airquality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce2d71",
   "metadata": {},
   "source": [
    "## 4.11. Insert Monitoring Data\n",
    "Save predictions to monitoring feature group in Hopsworks for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_fg = fs.get_or_create_feature_group(\n",
    "    name=\"aq_predictions\",\n",
    "    description=\"Air Quality prediction monitoring\",\n",
    "    version=1,\n",
    "    primary_key=[\"sensor_id\", \"date\", \"days_before_forecast_day\"],\n",
    "    event_time=\"date\",\n",
    ")\n",
    "\n",
    "monitor_fg.insert(predictions, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf631c3b",
   "metadata": {},
   "source": [
    "## 4.12. Hindcast Analysis\n",
    "Compare predicted with forecasted values (1-day prior forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16099436",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_df = monitor_fg.filter(monitor_fg.days_before_forecast_day == 1).read()\n",
    "monitoring_df[\"date\"] = pd.to_datetime(monitoring_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "air_quality_df = air_quality_fg.read()[[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "air_quality_df[\"date\"] = pd.to_datetime(air_quality_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "for sensor_id, location in locations.items():\n",
    "    try:\n",
    "        sensor_preds = monitoring_df[monitoring_df[\"sensor_id\"] == sensor_id][[\"date\", \"predicted_pm25\"]]\n",
    "        merged = sensor_preds.merge(\n",
    "            air_quality_df[air_quality_df[\"sensor_id\"] == sensor_id][[\"date\", \"pm25\"]],\n",
    "            on=\"date\",\n",
    "            how=\"inner\",\n",
    "        ).sort_values(\"date\")\n",
    "\n",
    "        city, street = location[\"city\"], location[\"street\"]\n",
    "        hindcast_path = f\"{root_dir}/models/{sensor_id}/images/hindcast_prediction.png\"\n",
    "        Path(hindcast_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        plt = visualization.plot_air_quality_forecast(\n",
    "            city,\n",
    "            street,\n",
    "            merged if not merged.empty else sensor_preds.assign(pm25=np.nan),\n",
    "            hindcast_path,\n",
    "            hindcast=True,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        dataset_api.upload(\n",
    "            hindcast_path,\n",
    "            f\"Resources/airquality/{sensor_id}_{today:%Y-%m-%d}_hindcast.png\",\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error processing hindcast for sensor {sensor_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a256c",
   "metadata": {},
   "source": [
    "## 4.13 IDW Heatmap\n",
    "IDW - Inverse Distance Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632256a",
   "metadata": {},
   "source": [
    "### 4.13.1 IDW interpolation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idw_interpolation(points, values, grid_points, lon_mesh, power=2):\n",
    "    # compute distances between grid points and known data points \n",
    "    distances = cdist(grid_points, points)\n",
    "    # replace 0 with a small value to avoid division by zero\n",
    "    distances = np.where(distances == 0, 1e-10, distances)\n",
    "    # compute weights based on inverse distance\n",
    "    weights = 1.0 / (distances ** power)\n",
    "    # sum of weights for normalization\n",
    "    weights_sum = np.sum(weights, axis=1)\n",
    "    # compute interpolated values - weighted average of known values for each grid point\n",
    "    interpolated = np.sum(weights * values, axis=1) / weights_sum\n",
    "    # reshape to the match grid shape\n",
    "    return interpolated.reshape(lon_mesh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da547c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_bounds = tuple(list(json.load(open(f\"{root_dir}/frontend/coordinates.json\")).values())[:4])\n",
    "# grid_bounds = map_bounds[1], map_bounds[0], map_bounds[3], map_bounds[2]  # lat_min, lat_max, lon_min, lon_max\n",
    "print(grid_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import hopsworks\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "def clone_repository(username: str) -> None:\n",
    "    repo_dir = Path(\"pm25-forecast-openmeteo-aqicn\")\n",
    "    if repo_dir.exists():\n",
    "        print(f\"Repository already exists at {repo_dir.absolute()}\")\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "    else:\n",
    "        print(\"Cloning repository...\")\n",
    "        !git clone https://github.com/{username}/pm25-forecast-openmeteo-aqicn.git\n",
    "        %cd pm25-forecast-openmeteo-aqicn\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "for folder in (\"src\", \"airquality\", \"notebooks\"):\n",
    "    if root_dir.parts[-1:] == (folder,):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "root_dir = str(root_dir)\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "from utils import config\n",
    "\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "clone_repository(GITHUB_USERNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation_dir = f\"{root_dir}/models/interpolation\"\n",
    "# if not os.path.exists(interpolation_dir):\n",
    "#     os.mkdir(interpolation_dir)\n",
    "\n",
    "# # Use predictions DataFrame which contains all forecast days with PM2.5 values\n",
    "# interpolation_df = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "\n",
    "# Always resolve relative to the backend project root\n",
    "interpolation_dir = os.path.join(os.path.dirname(__file__), \"models\", \"interpolation\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(interpolation_dir, exist_ok=True)\n",
    "\n",
    "# Use predictions DataFrame which contains all forecast days with PM2.5 values\n",
    "interpolation_df = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582de454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add any actual PM2.5 data from today if available\n",
    "today_actual = batch_data[batch_data[\"date\"] == today_short].copy()\n",
    "\n",
    "if not today_actual.empty:\n",
    "    # Ensure both columns exist for the plotting function\n",
    "    today_actual = today_actual[[col for col in [\"date\", \"sensor_id\", \"pm25\", \"predicted_pm25\"] if col in today_actual.columns]]\n",
    "    interpolation_df = pd.concat([today_actual, interpolation_df], ignore_index=True)\n",
    "\n",
    "for i, forecast_date in enumerate(sorted(interpolation_df[\"date\"].unique())):\n",
    "    forecast_date_short = forecast_date.strftime(\"%Y-%m-%d\")\n",
    "    output_png = f\"{interpolation_dir}/forecast_interpolation_{i}d.png\"\n",
    "    print(interpolation_df.info())\n",
    "\n",
    "    plot_pm25_idw_heatmap(\n",
    "        interpolation_df,\n",
    "        locations,\n",
    "        forecast_date,\n",
    "        output_png,\n",
    "    )\n",
    "\n",
    "    dataset_api.upload(\n",
    "        output_png,\n",
    "        f\"Resources/airquality/interpolation_{today_short}_{forecast_date_short}.png\",\n",
    "        overwrite=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
