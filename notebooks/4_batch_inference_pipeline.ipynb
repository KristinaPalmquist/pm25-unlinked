{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0807c7e3",
   "metadata": {},
   "source": [
    "# 4. Batch Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e540c",
   "metadata": {},
   "source": [
    "## 4.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62eb402",
   "metadata": {},
   "source": [
    "### 4.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e036d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "import traceback\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4a169",
   "metadata": {},
   "source": [
    "### 4.1.2. Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a9d05",
   "metadata": {},
   "source": [
    "### 4.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7551d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bff40",
   "metadata": {},
   "source": [
    "### 4.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4450bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fe744",
   "metadata": {},
   "source": [
    "### 4.1.5. Get Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f1d0b",
   "metadata": {},
   "source": [
    "## 4.2. Get Feature Groups and Sensor Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)\n",
    "\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4c59a",
   "metadata": {},
   "source": [
    "## 4.3. Load Data from Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530feda4",
   "metadata": {},
   "source": [
    "### 4.3.1. Set Inference Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32127435",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_date = today - timedelta(days=7)  # Get 7 days of historical data for feature engineering\n",
    "future_date = today + timedelta(days=7)  # Get 7 days of future weather forecasts\n",
    "today_short = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Inference period: {past_date} to {future_date}\")\n",
    "print(f\"Today: {today_short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66556865",
   "metadata": {},
   "source": [
    "### 4.3.2. Load Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dced6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        batch_weather = weather_fg.filter(\n",
    "            (weather_fg.date >= past_date) & (weather_fg.date <= future_date)\n",
    "        ).read()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"‚ö†Ô∏è Weather read attempt {attempt + 1} failed, retrying...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to read weather data after {max_retries} attempts\")\n",
    "            batch_weather = weather_fg.read()\n",
    "            batch_weather = batch_weather[\n",
    "                (batch_weather[\"date\"] >= past_date) & (batch_weather[\"date\"] <= future_date)\n",
    "            ]\n",
    "\n",
    "batch_weather[\"date\"] = pd.to_datetime(batch_weather[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_weather)} weather records from {past_date} to {future_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca24aa",
   "metadata": {},
   "source": [
    "### 4.3.3. Load Air Quality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        batch_airquality = air_quality_fg.filter(\n",
    "            air_quality_fg.date >= past_date\n",
    "        ).read()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"‚ö†Ô∏è Air quality read attempt {attempt + 1} failed, retrying...\")\n",
    "            time.sleep(2 ** attempt)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to read air quality data after {max_retries} attempts\")\n",
    "            batch_airquality = air_quality_fg.read()\n",
    "            batch_airquality = batch_airquality[\n",
    "                batch_airquality[\"date\"] >= past_date\n",
    "            ]\n",
    "\n",
    "batch_airquality[\"date\"] = pd.to_datetime(batch_airquality[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "print(f\"Retrieved {len(batch_airquality)} air quality records from {past_date} to {today}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f8fb",
   "metadata": {},
   "source": [
    "## 4.4. Model Retrieval\n",
    "Download trained XGBoost models from Hopsworks model registry for each sensor and extract feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c861de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_TEMPLATE = \"air_quality_xgboost_model_{sensor_id}\"\n",
    "\n",
    "retrieved_models = {}\n",
    "\n",
    "for sensor_id in sensor_locations.keys():\n",
    "    model_name = MODEL_NAME_TEMPLATE.format(sensor_id=sensor_id)\n",
    "    \n",
    "    try:\n",
    "        available_models = mr.get_models(name=model_name)\n",
    "        if not available_models:\n",
    "            print(f\"‚ö†Ô∏è No model found for sensor {sensor_id}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_model = max(available_models, key=lambda model: model.version)\n",
    "        saved_model_dir = retrieved_model.download()\n",
    "        \n",
    "        xgb_model = XGBRegressor()\n",
    "        xgb_model.load_model(saved_model_dir + \"/model.json\")\n",
    "        booster = xgb_model.get_booster()\n",
    "        \n",
    "        retrieved_models[sensor_id] = retrieved_model, xgb_model, booster.feature_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model for sensor {sensor_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(retrieved_models)} models from registry\")\n",
    "print(f\"   Total sensors in feature store: {len(sensor_locations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieved {len(retrieved_models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88c6a6",
   "metadata": {},
   "source": [
    "## 4.5. Batch Prediction\n",
    "Merge weather and air quality data, iteratively predict PM2.5 values for forecast days, update engineered features after each prediction, and store results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475e6f33",
   "metadata": {},
   "source": [
    "### 4.5.1. Batch Prediction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge historical data with weather data\n",
    "batch_data = pd.merge(batch_weather, batch_airquality, on=[\"date\", \"sensor_id\"], how=\"left\")\n",
    "batch_data = batch_data.sort_values([\"sensor_id\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"pm25_rolling_3d\",\n",
    "    \"pm25_lag_1d\",\n",
    "    \"pm25_lag_2d\",\n",
    "    \"pm25_lag_3d\",\n",
    "    \"pm25_nearby_avg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new columns, fill with NaN for now\n",
    "batch_data[\"predicted_pm25\"] = np.nan\n",
    "batch_data[\"days_before_forecast_day\"] = np.nan\n",
    "for col in feature_cols:\n",
    "    batch_data[f\"predicted_{col}\"] = np.nan\n",
    "    \n",
    "\n",
    "forecast_days = [pd.Timestamp(today) + pd.Timedelta(days=i) for i in range(7)]\n",
    "\n",
    "# # Select all rows where pm25 is NaN and date is today or later\n",
    "# # drop any NaN date values, sort the dates in ascending order, get unique dates\n",
    "# # forecast days will be a list of dates for which pm2.5 predictions are needed\n",
    "# forecast_days = (\n",
    "#     batch_data.loc[batch_data[\"pm25\"].isna() & \n",
    "#                    (batch_data[\"date\"] >= today.strftime(\"%Y-%m-%d\")), \"date\"]\n",
    "#     .dropna()\n",
    "#     .sort_values()\n",
    "#     .unique()\n",
    "# )\n",
    "\n",
    "# Ensure today is always included for UI display, even if we have some actual data\n",
    "if today.strftime(\"%Y-%m-%d\") not in forecast_days:\n",
    "    forecast_days = np.append([pd.Timestamp(today)], forecast_days)\n",
    "    print(f\"‚ÑπÔ∏è  Added today ({today}) to forecast_days for UI completeness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38101be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_day in forecast_days:\n",
    "    # context with all sensors up to current day\n",
    "    window = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    day_rows = window[window[\"date\"] == target_day]\n",
    "\n",
    "    for _, row in day_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        try:\n",
    "            _, xgb_model, model_features = retrieved_models[sensor_id]\n",
    "        except KeyError:\n",
    "            print(f\"No model for sensor {sensor_id}, skipping prediction for {target_day}.\")\n",
    "            continue\n",
    "        features = (row.reindex(model_features).to_frame().T.apply(pd.to_numeric, errors=\"coerce\"))\n",
    "        y_hat = xgb_model.predict(features)[0]\n",
    "\n",
    "        idx = batch_data.index[(batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)][0]\n",
    "       \n",
    "        if pd.isna(row[\"pm25\"]):\n",
    "            batch_data.at[idx, \"pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"predicted_pm25\"] = y_hat\n",
    "        batch_data.at[idx, \"days_before_forecast_day\"] = (target_day - pd.Timestamp(today)).days\n",
    "\n",
    "    # Recompute features for after filling this day\n",
    "    temp_df = batch_data.loc[batch_data[\"date\"] <= target_day].copy()\n",
    "    temp_df = feature_engineering.add_rolling_window_feature(\n",
    "        temp_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\"\n",
    "    )\n",
    "    temp_df = feature_engineering.add_lagged_features(temp_df, column=\"pm25\", lags=[1, 2, 3])\n",
    "    temp_df = feature_engineering.add_nearby_sensor_feature(\n",
    "        temp_df,\n",
    "        sensor_locations,\n",
    "        column=\"pm25_lag_1d\",\n",
    "        n_closest=3,\n",
    "        new_column=\"pm25_nearby_avg\",\n",
    "    )\n",
    "\n",
    "    current_rows = temp_df[temp_df[\"date\"] == target_day]\n",
    "    for _, row in current_rows.iterrows():\n",
    "        sensor_id = row[\"sensor_id\"]\n",
    "        mask = (batch_data[\"sensor_id\"] == sensor_id) & (batch_data[\"date\"] == target_day)\n",
    "        if mask.any():\n",
    "            for col in feature_cols:\n",
    "                batch_data.loc[mask, f\"predicted_{col}\"] = row[col]\n",
    "\n",
    "predictions = batch_data.loc[\n",
    "    batch_data[\"predicted_pm25\"].notna(),\n",
    "    [\"date\", \"sensor_id\", \"predicted_pm25\", \"days_before_forecast_day\"]\n",
    "    + [f\"predicted_{col}\" for col in feature_cols],\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6fa61",
   "metadata": {},
   "source": [
    "### 4.5.2. Assemble Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d621f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions.copy()\n",
    "\n",
    "print(f\"‚úÖ Generated {len(predictions_df)} prediction rows\")\n",
    "print(f\"   Date range: {predictions_df['date'].min()} to {predictions_df['date'].max()}\")\n",
    "print(f\"   Sensors: {predictions_df['sensor_id'].nunique()}\")\n",
    "print(f\"   Forecast days: {sorted(predictions_df['days_before_forecast_day'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd25d0",
   "metadata": {},
   "source": [
    "### 4.5.3. Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get predictions feature group (same as in training pipeline)\n",
    "predictions_fg = fs.get_or_create_feature_group(\n",
    "    name=\"aq_predictions\",\n",
    "    version=1,\n",
    "    primary_key=[\"sensor_id\", \"date\", \"days_before_forecast_day\"],\n",
    "    description=\"Air Quality prediction monitoring\",\n",
    "    event_time=\"date\"\n",
    ")\n",
    "\n",
    "# Insert predictions\n",
    "if len(predictions) > 0:\n",
    "\n",
    "    # Difference in types between feature stores\n",
    "    if env in (\"job\", \"jupyter\"):\n",
    "        predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int64\")\n",
    "    else:\n",
    "       predictions[\"sensor_id\"] = predictions[\"sensor_id\"].astype(\"int32\")\n",
    "\n",
    "    max_retries = 5\n",
    "    delay = 2  # seconds\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            predictions_fg.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "            print(f\"‚úÖ Insert successful on attempt {attempt}\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Insert failed on attempt {attempt}: {type(e).__name__}: {e}\")\n",
    "\n",
    "            if attempt == max_retries:\n",
    "                print(\"‚ùå Max retries reached. Insert failed permanently.\")\n",
    "                raise\n",
    "\n",
    "            sleep_time = delay * (2 ** (attempt - 1))\n",
    "            print(f\"‚è≥ Retrying in {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    print(f\"‚úÖ Inserted {len(predictions)} predictions to {predictions_fg.name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No predictions to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f7a97",
   "metadata": {},
   "source": [
    "## 4.6. Analysis & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519157fb",
   "metadata": {},
   "source": [
    "### 4.6.1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure visualization for production vs development\n",
    "SKIP_SENSOR_PLOTS = env == \"job\"  # Skip individual sensor plots when running as Hopsworks job\n",
    "\n",
    "if SKIP_SENSOR_PLOTS:\n",
    "    print(\"‚è≠Ô∏è Skipping individual sensor plots (running as Hopsworks job)\")\n",
    "    print(\"   Heatmap interpolations will still be generated for UI\")\n",
    "else:\n",
    "    print(\"üìä Full visualization enabled (running locally/Jupyter)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574a8ec",
   "metadata": {},
   "source": [
    "### 4.6.2. Generate Forecast Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sensor plots are skipped in jobs\n",
    "if SKIP_SENSOR_PLOTS:\n",
    "    print(\"‚è≠Ô∏è Skipping forecast plot generation (200+ files)\")\n",
    "else:\n",
    "    dataset_api = project.get_dataset_api()\n",
    "    forecast_paths = []\n",
    "\n",
    "    for sensor_id, location in sensor_locations.items():\n",
    "        sensor_forecast = predictions[predictions[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "        city, street = location[\"city\"], location[\"street\"]\n",
    "        forecast_path = f\"{root_dir}/models/{sensor_id}/images/forecast.png\"\n",
    "        Path(forecast_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fig = visualization.plot_air_quality_forecast(\n",
    "            location[\"city\"],\n",
    "            location[\"street\"],\n",
    "            sensor_forecast,\n",
    "            forecast_path,\n",
    "            hindcast=False,\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        forecast_paths.append((sensor_id, forecast_path))\n",
    "\n",
    "    if not dataset_api.exists(\"Resources/airquality\"):\n",
    "        dataset_api.mkdir(\"Resources/airquality\")\n",
    "\n",
    "    # Upload with retry logic and error handling\n",
    "    upload_success = 0\n",
    "    upload_failed = 0\n",
    "    \n",
    "    for i, (sensor_id, forecast_path) in enumerate(forecast_paths):\n",
    "        max_retries = 3\n",
    "        retry_delay = 2  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                dataset_api.upload(\n",
    "                    forecast_path,\n",
    "                    f\"Resources/airquality/{sensor_id}_{today_short}_forecast.png\",\n",
    "                    overwrite=True,\n",
    "                )\n",
    "                upload_success += 1\n",
    "                if (i + 1) % 20 == 0:  # Progress update every 20 uploads\n",
    "                    print(f\"   Uploaded {i + 1}/{len(forecast_paths)} plots...\")\n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except (ConnectionError, ProtocolError, Timeout, RequestException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"‚ö†Ô∏è Upload failed for sensor {sensor_id} (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to upload for sensor {sensor_id} after {max_retries} attempts: {e}\")\n",
    "                    upload_failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Unexpected error uploading for sensor {sensor_id}: {e}\")\n",
    "                upload_failed += 1\n",
    "                break\n",
    "        \n",
    "        # Small delay between uploads to avoid overwhelming the connection\n",
    "        if i < len(forecast_paths) - 1:\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    print(f\"‚úÖ Upload complete: {upload_success} successful, {upload_failed} failed\")\n",
    "    if upload_success > 0:\n",
    "        print(f\"   Forecast plots available in Hopsworks under {project.get_url()}/settings/fb/path/Resources/airquality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3ebe3",
   "metadata": {},
   "source": [
    "### 4.6.3. Hindcast Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e326719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sensor plots are skipped in jobs\n",
    "if SKIP_SENSOR_PLOTS:\n",
    "    print(\"‚è≠Ô∏è Skipping hindcast plot generation (200+ files)\")\n",
    "else:\n",
    "    # Use predictions_fg (same variable name as in training pipeline)\n",
    "\n",
    "    try:\n",
    "        monitoring_df = predictions_fg.filter(predictions_fg.days_before_forecast_day == 1).read()\n",
    "        monitoring_df[\"date\"] = pd.to_datetime(monitoring_df[\"date\"]).dt.tz_localize(None)\n",
    "        print(f\"‚úÖ Successfully read {len(monitoring_df)} hindcast predictions\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not read monitoring data: {e}\")\n",
    "        print(\"Skipping hindcast analysis...\")\n",
    "        monitoring_df = pd.DataFrame()  # Empty dataframe to prevent further errors\n",
    "\n",
    "    if not monitoring_df.empty:\n",
    "        air_quality_df = air_quality_fg.read()[[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        air_quality_df[\"date\"] = pd.to_datetime(air_quality_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "        for sensor_id, location in sensor_locations.items():\n",
    "            try:\n",
    "                sensor_preds = monitoring_df[monitoring_df[\"sensor_id\"] == sensor_id][[\"date\", \"predicted_pm25\"]]\n",
    "                \n",
    "                if sensor_preds.empty:\n",
    "                    continue\n",
    "                    \n",
    "                merged = sensor_preds.merge(\n",
    "                    air_quality_df[air_quality_df[\"sensor_id\"] == sensor_id][[\"date\", \"pm25\"]],\n",
    "                    on=\"date\",\n",
    "                    how=\"inner\",\n",
    "                ).sort_values(\"date\")\n",
    "\n",
    "                city, street = location[\"city\"], location[\"street\"]\n",
    "                hindcast_path = f\"{root_dir}/models/{sensor_id}/images/hindcast_prediction.png\"\n",
    "                Path(hindcast_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                fig = visualization.plot_air_quality_forecast(\n",
    "                    city,\n",
    "                    street,\n",
    "                    merged if not merged.empty else sensor_preds.assign(pm25=np.nan),\n",
    "                    hindcast_path,\n",
    "                    hindcast=True,\n",
    "                )\n",
    "                if fig is not None:\n",
    "                    plt.close(fig)\n",
    "\n",
    "                dataset_api.upload(\n",
    "                    hindcast_path,\n",
    "                    f\"Resources/airquality/{sensor_id}_{today:%Y-%m-%d}_hindcast.png\",\n",
    "                    overwrite=True,\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing hindcast for sensor {sensor_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abb161",
   "metadata": {},
   "source": [
    "### 4.6.4. IDW Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a225b",
   "metadata": {},
   "source": [
    "#### 4.6.4.2. Generate Heatmap Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüó∫Ô∏è Generating heatmap interpolation images (required for UI)\")\n",
    "\n",
    "try:\n",
    "    with open(f\"{root_dir}/frontend/coordinates.json\") as f:\n",
    "        coords_data = json.load(f)\n",
    "        # CRITICAL: Skip REGION_NAME and get only the numeric coordinate values\n",
    "        grid_bounds = tuple([\n",
    "            float(coords_data[\"MIN_LONGITUDE\"]),\n",
    "            float(coords_data[\"MIN_LATITUDE\"]),\n",
    "            float(coords_data[\"MAX_LONGITUDE\"]),\n",
    "            float(coords_data[\"MAX_LATITUDE\"])\n",
    "        ])\n",
    "    print(f\"   Grid bounds from coordinates.json: {grid_bounds}\")\n",
    "    print(f\"   (min_lon={grid_bounds[0]}, min_lat={grid_bounds[1]}, max_lon={grid_bounds[2]}, max_lat={grid_bounds[3]})\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load coordinates.json: {e}\")\n",
    "    print(\"   Calculating bounds from sensor locations...\")\n",
    "    \n",
    "    # Calculate bounds from sensor locations with padding\n",
    "    lons = [loc['longitude'] for loc in sensor_locations.values()]\n",
    "    lats = [loc['latitude'] for loc in sensor_locations.values()]\n",
    "    \n",
    "    min_lon, max_lon = min(lons), max(lons)\n",
    "    min_lat, max_lat = min(lats), max(lats)\n",
    "    \n",
    "    # Add 10% padding\n",
    "    lon_padding = (max_lon - min_lon) * 0.1\n",
    "    lat_padding = (max_lat - min_lat) * 0.1\n",
    "    \n",
    "    grid_bounds = (\n",
    "        float(min_lon - lon_padding),\n",
    "        float(min_lat - lat_padding),\n",
    "        float(max_lon + lon_padding),\n",
    "        float(max_lat + lat_padding)\n",
    "    )\n",
    "    print(f\"   Calculated grid bounds: {grid_bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_dir = f\"{root_dir}/models/interpolation\"\n",
    "os.makedirs(interpolation_dir, exist_ok=True)\n",
    "\n",
    "# Start with predictions and ensure numeric types immediately\n",
    "interpolation_df = predictions.copy()\n",
    "interpolation_df[\"predicted_pm25\"] = pd.to_numeric(interpolation_df[\"predicted_pm25\"], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582de454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For day 0 heatmap: Use actual sensor measurements if available, otherwise fall back to predictions\n",
    "# For days 1-6 heatmaps: Always use predictions\n",
    "interpolation_df = predictions.copy()\n",
    "interpolation_df[\"predicted_pm25\"] = pd.to_numeric(interpolation_df[\"predicted_pm25\"], errors='coerce')\n",
    "\n",
    "# Get today's actual measurements\n",
    "today_actual = batch_data[batch_data[\"date\"] == today_short].copy()\n",
    "\n",
    "if not today_actual.empty:\n",
    "    # Keep only actual measurements with valid pm25 values\n",
    "    today_actual = today_actual[[col for col in [\"date\", \"sensor_id\", \"pm25\"] if col in today_actual.columns]]\n",
    "    today_actual[\"pm25\"] = pd.to_numeric(today_actual[\"pm25\"], errors='coerce')\n",
    "    \n",
    "    # Remove any rows with NaN pm25 (we only want real measurements)\n",
    "    today_actual = today_actual[today_actual[\"pm25\"].notna()]\n",
    "    \n",
    "    if len(today_actual) > 0:\n",
    "        # IMPORTANT: Remove any prediction rows for today (we want ONLY actual measurements for day 0)\n",
    "        interpolation_df = interpolation_df[interpolation_df[\"date\"] != pd.Timestamp(today)]\n",
    "        \n",
    "        # Add actual measurements for today\n",
    "        interpolation_df = pd.concat([today_actual, interpolation_df], ignore_index=True)\n",
    "        print(f\"   Using {len(today_actual)} actual sensor measurements for day 0 heatmap\")\n",
    "    else:\n",
    "        # No valid actual readings - keep predictions for today\n",
    "        print(f\"   ‚ö†Ô∏è No valid actual sensor data for today - using predictions for day 0 heatmap\")\n",
    "        # Rename predicted_pm25 to pm25 for today's data only\n",
    "        today_preds = interpolation_df[interpolation_df[\"date\"] == pd.Timestamp(today)].copy()\n",
    "        if not today_preds.empty:\n",
    "            today_preds[\"pm25\"] = today_preds[\"predicted_pm25\"]\n",
    "            interpolation_df = interpolation_df[interpolation_df[\"date\"] != pd.Timestamp(today)]\n",
    "            interpolation_df = pd.concat([today_preds, interpolation_df], ignore_index=True)\n",
    "else:\n",
    "    # No data at all for today in batch_data - use predictions  \n",
    "    print(f\"   ‚ö†Ô∏è No batch data for today - using predictions for day 0 heatmap\")\n",
    "    today_preds = interpolation_df[interpolation_df[\"date\"] == pd.Timestamp(today)].copy()\n",
    "    if not today_preds.empty:\n",
    "        today_preds[\"pm25\"] = today_preds[\"predicted_pm25\"]\n",
    "        interpolation_df = interpolation_df[interpolation_df[\"date\"] != pd.Timestamp(today)]\n",
    "        interpolation_df = pd.concat([today_preds, interpolation_df], ignore_index=True)\n",
    "\n",
    "# DIAGNOSTIC: Print dtypes to debug type conversion issues\n",
    "print(f\"\\nüîç Interpolation DataFrame dtypes:\")\n",
    "print(f\"   predicted_pm25: {interpolation_df['predicted_pm25'].dtype if 'predicted_pm25' in interpolation_df.columns else 'N/A'}\")\n",
    "if \"pm25\" in interpolation_df.columns:\n",
    "    print(f\"   pm25: {interpolation_df['pm25'].dtype}\")\n",
    "    print(f\"   Sample pm25 values: {interpolation_df[interpolation_df['pm25'].notna()]['pm25'].head(3).tolist()}\")\n",
    "\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "unique_dates = sorted(interpolation_df[\"date\"].unique())\n",
    "print(f\"\\nüìÖ Generating {len(unique_dates)} heatmap images...\")\n",
    "print(f\"   Today (reference date): {today}\")\n",
    "print(f\"   Unique forecast dates: {[str(d)[:10] for d in unique_dates]}\")\n",
    "\n",
    "# Create frontend interpolation directory\n",
    "frontend_interpolation_dir = f\"{root_dir}/frontend/interpolation\"\n",
    "os.makedirs(frontend_interpolation_dir, exist_ok=True)\n",
    "\n",
    "successful_images = 0\n",
    "failed_images = 0\n",
    "\n",
    "for i, forecast_date in enumerate(unique_dates):\n",
    "    forecast_date_short = forecast_date.strftime(\"%Y-%m-%d\")\n",
    "    days_ahead = (forecast_date - pd.Timestamp(today)).days\n",
    "    output_png = f\"{interpolation_dir}/forecast_interpolation_{days_ahead}d.png\"\n",
    "    frontend_png = f\"{frontend_interpolation_dir}/forecast_interpolation_{days_ahead}d.png\"\n",
    "\n",
    "    # Diagnostic logging for day calculation\n",
    "    if days_ahead <= 1:\n",
    "        print(f\"\\n   üîç Day {days_ahead} diagnostics:\")\n",
    "        print(f\"      forecast_date: {forecast_date} (type: {type(forecast_date).__name__})\")\n",
    "        print(f\"      pd.Timestamp(today): {pd.Timestamp(today)}\")\n",
    "        print(f\"      days_ahead = ({forecast_date} - {pd.Timestamp(today)}).days = {days_ahead}\")\n",
    "        print(f\"      Filename: {os.path.basename(frontend_png)}\")\n",
    "\n",
    "    print(f\"   [{i+1}/{len(unique_dates)}] {forecast_date_short} (+{days_ahead}d)...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        visualization.plot_pm25_idw_heatmap(\n",
    "            interpolation_df,\n",
    "            sensor_locations,\n",
    "            forecast_date,\n",
    "            output_png,\n",
    "            grid_bounds=grid_bounds,\n",
    "            today=today,\n",
    "        )\n",
    "        \n",
    "        # Copy to frontend directory for Netlify deployment\n",
    "        shutil.copy2(output_png, frontend_png)\n",
    "        \n",
    "        if days_ahead == 0:\n",
    "                    from IPython.display import Image, display\n",
    "                    print(\"\\nüñºÔ∏è DEBUG: Showing Day 0 heatmap image\")\n",
    "                    display(Image(filename=output_png))\n",
    "\n",
    "        print(\"‚úÖ\")\n",
    "\n",
    "\n",
    "        # Verify the copy succeeded\n",
    "        if not os.path.exists(frontend_png):\n",
    "            raise FileNotFoundError(f\"Failed to copy to {frontend_png}\")\n",
    "        \n",
    "        print(\"‚úÖ\")\n",
    "        successful_images += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {type(e).__name__}: {str(e)[:100]}\")\n",
    "        failed_images += 1\n",
    "\n",
    "print(f\"\\nüìä Heatmap generation complete: {successful_images} successful, {failed_images} failed\")\n",
    "\n",
    "if failed_images > 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: {failed_images} images failed to generate\")\n",
    "if successful_images == 0:\n",
    "    raise Exception(\"‚ùå All heatmap images failed - pipeline cannot complete without UI data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3639158",
   "metadata": {},
   "source": [
    "## 4.7. Export Predictions for Frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01eea5",
   "metadata": {},
   "source": [
    "### 4.7.1. Prepare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì¶ Preparing predictions for frontend export...\")\n",
    "\n",
    "# Debug: show available forecast days and prediction count before merging with metadata\n",
    "print(f\"\\nüîç Debug - Available days_before_forecast_day values: {sorted(predictions['days_before_forecast_day'].unique())}\")\n",
    "print(f\"   Total predictions: {len(predictions)}\")\n",
    "\n",
    "frontend_predictions = predictions.copy()\n",
    "\n",
    "\n",
    "# Build metadata dataframe safely\n",
    "sensor_metadata_rows = []\n",
    "for sensor_id, location_data in sensor_locations.items():\n",
    "    sensor_metadata_rows.append({\n",
    "        'sensor_id': int(sensor_id),  # force int\n",
    "        'latitude': float(location_data['latitude']),\n",
    "        'longitude': float(location_data['longitude']),\n",
    "        'city': location_data.get('city', ''),\n",
    "        'street': location_data.get('street', '')\n",
    "    })\n",
    "\n",
    "sensor_metadata_df = pd.DataFrame(sensor_metadata_rows)\n",
    "\n",
    "# Ensure consistent dtypes\n",
    "frontend_predictions['sensor_id'] = frontend_predictions['sensor_id'].astype(int)\n",
    "sensor_metadata_df['sensor_id'] = sensor_metadata_df['sensor_id'].astype(int)\n",
    "\n",
    "print(f\"   Metadata sensors: {sensor_metadata_df['sensor_id'].nunique()}\")\n",
    "print(f\"   Prediction sensors: {frontend_predictions['sensor_id'].nunique()}\")\n",
    "\n",
    "# Merge predictions with metadata\n",
    "merged = frontend_predictions.merge(sensor_metadata_df, on='sensor_id', how='left')\n",
    "\n",
    "# Validate coordinates\n",
    "missing_coords = merged[merged['latitude'].isna() | merged['longitude'].isna()]\n",
    "if not missing_coords.empty:\n",
    "    print(\"\\n‚ùå ERROR: Missing coordinates for these sensor IDs:\")\n",
    "    print(missing_coords['sensor_id'].unique())\n",
    "    raise ValueError(\"Some prediction rows have no matching sensor metadata. Fix required.\")\n",
    "\n",
    "frontend_predictions = merged\n",
    "print(f\"   ‚úÖ Coordinates added for {len(frontend_predictions)} rows\")\n",
    "print(frontend_predictions.head())\n",
    "\n",
    "# Convert datetime to string for JSON\n",
    "frontend_predictions[\"date\"] = frontend_predictions[\"date\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf08af",
   "metadata": {},
   "source": [
    "### 4.7.2. Export Predictions JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15951942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions as JSON for frontend\n",
    "print(\"\\nüì¶ Exporting predictions for frontend...\")\n",
    "\n",
    "# Export to frontend directory\n",
    "predictions_json_path = f\"{root_dir}/frontend/predictions.json\"\n",
    "frontend_predictions.to_json(predictions_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"‚úÖ Exported {len(frontend_predictions)} predictions to frontend/predictions.json\")\n",
    "print(f\"   Sensors: {frontend_predictions['sensor_id'].nunique()}\")\n",
    "if len(frontend_predictions) > 0:\n",
    "    print(f\"   Date range: {frontend_predictions['date'].min()} to {frontend_predictions['date'].max()}\" if len(frontend_predictions) > 0 else \"   No predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f8f35",
   "metadata": {},
   "source": [
    "### 4.7.3. Commit Frontend Artifacts to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüì§ Committing predictions and interpolation images to git...\")\n",
    "\n",
    "try:\n",
    "    # List PNGs for debugging\n",
    "    frontend_pngs = glob.glob(f\"{root_dir}/frontend/interpolation/*.png\")\n",
    "    print(f\"   Found {len(frontend_pngs)} PNGs in frontend/interpolation/\")\n",
    "    if frontend_pngs:\n",
    "        print(f\"   Example: {os.path.basename(frontend_pngs[0])}\")\n",
    "\n",
    "    # Configure git identity\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"Hopsworks Bot\"], cwd=root_dir, check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"bot@hopsworks.ai\"], cwd=root_dir, check=True)\n",
    "\n",
    "    # Stage only the generated srtifacts\n",
    "    print(\"   Staging frontend artifacts...\")\n",
    "    subprocess.run([\"git\", \"add\", \"-f\", \"frontend/predictions.json\"], cwd=root_dir, check=True)\n",
    "    subprocess.run([\"git\", \"add\", \"-f\", \"frontend/interpolation/\"], cwd=root_dir, check=True)\n",
    "\n",
    "    # Check if anything changed\n",
    "    status = subprocess.run(\n",
    "        [\"git\", \"status\", \"--porcelain\"],\n",
    "        cwd=root_dir,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if not status.stdout.strip():\n",
    "        print(\"‚ÑπÔ∏è  No changes to commit ‚Äî predictions and heatmaps already up to date\")\n",
    "    else:\n",
    "        print(\"   Changes detected:\")\n",
    "        print(status.stdout)\n",
    "        commit = subprocess.run(\n",
    "            [\"git\", \"commit\", \"-m\", f\"Update predictions and heatmaps - {today_short}\"],\n",
    "            cwd=root_dir,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if commit.returncode == 0:\n",
    "            print(\"‚úÖ Commit successful\")\n",
    "\n",
    "            push = subprocess.run(\n",
    "                [\"git\", \"push\"],\n",
    "                cwd=root_dir,\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "\n",
    "            if push.returncode == 0:\n",
    "                print(\"‚úÖ Successfully pushed updates to GitHub\")\n",
    "                print(\"   Netlify will rebuild automatically\")\n",
    "\n",
    "            else:\n",
    "                print(\"‚ùå Git push failed\")\n",
    "                print(push.stderr)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Commit failed\")\n",
    "            print(commit.stderr)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Git operation error: {e}\")\n",
    "    print(\"   Files exported locally but not pushed to git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Commit and push predictions and interpolation images to git\n",
    "# print(\"\\nüì§ Committing predictions and interpolation images to git...\")\n",
    "# try:    \n",
    "#     # Verify files exist before committing\n",
    "#     frontend_pngs = glob.glob(f\"{root_dir}/frontend/interpolation/*.png\")\n",
    "#     print(f\"   Found {len(frontend_pngs)} PNG files in frontend/interpolation/\")\n",
    "#     if len(frontend_pngs) > 0:\n",
    "#         print(f\"   Example: {os.path.basename(frontend_pngs[0])}\")\n",
    "    \n",
    "#     # Configure git\n",
    "#     subprocess.run([\"git\", \"config\", \"user.name\", \"Hopsworks Bot\"], cwd=root_dir, check=True)\n",
    "#     subprocess.run([\"git\", \"config\", \"user.email\", \"bot@hopsworks.ai\"], cwd=root_dir, check=True)\n",
    "    \n",
    "#     # Add frontend files\n",
    "#     print(\"   Staging frontend files...\")\n",
    "#     subprocess.run([\"git\", \"add\", \"-f\", \"frontend/predictions.json\"], cwd=root_dir, check=True)\n",
    "#     subprocess.run([\"git\", \"add\", \"-f\", \"frontend/interpolation/\"], cwd=root_dir, check=True)\n",
    "#     print(f\"   Staged predictions.json and {len(frontend_pngs)} interpolation images\")\n",
    "    \n",
    "#     # Check if there are actually changes to commit\n",
    "#     status_result = subprocess.run(\n",
    "#         [\"git\", \"status\", \"--porcelain\"],\n",
    "#         cwd=root_dir,\n",
    "#         capture_output=True,\n",
    "#         text=True\n",
    "#     )\n",
    "    \n",
    "#     if not status_result.stdout.strip():\n",
    "#         print(\"‚ÑπÔ∏è  No changes detected - predictions and heatmaps are already up to date\")\n",
    "#     else:\n",
    "#         print(f\"   Changes detected:\\n{status_result.stdout}\")\n",
    "        \n",
    "#         commit_result = subprocess.run(\n",
    "#             [\"git\", \"commit\", \"-m\", f\"Update predictions and heatmaps - {today_short}\"],\n",
    "#             cwd=root_dir,\n",
    "#             capture_output=True,\n",
    "#             text=True\n",
    "#         )\n",
    "        \n",
    "#         if commit_result.returncode == 0:\n",
    "#             print(f\"‚úÖ Commit successful!\")\n",
    "            \n",
    "#             # Push with error handling\n",
    "#             push_result = subprocess.run(\n",
    "#                 [\"git\", \"push\"],\n",
    "#                 cwd=root_dir,\n",
    "#                 capture_output=True,\n",
    "#                 text=True\n",
    "#             )\n",
    "            \n",
    "#             if push_result.returncode == 0:\n",
    "#                 print(\"‚úÖ Pushed predictions + interpolation images to repository\")\n",
    "#                 print(f\"   This will trigger a Netlify rebuild with updated data\")\n",
    "#             else:\n",
    "#                 print(f\"‚ùå Git push failed (exit code {push_result.returncode})\")\n",
    "#                 print(f\"   stdout: {push_result.stdout}\")\n",
    "#                 print(f\"   stderr: {push_result.stderr}\")\n",
    "#                 print(\"   Files are committed locally but not pushed to GitHub\")\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è  Commit failed: {commit_result.stderr}\")\n",
    "            \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error in git operations: {e}\")\n",
    "#     print(traceback.format_exc())\n",
    "#     print(\"   Files exported locally but not pushed to repository\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47392f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"   Ensuring we're on main/master branch...\")\n",
    "\n",
    "# # Always fetch latest refs\n",
    "# subprocess.run([\"git\", \"fetch\", \"origin\"], cwd=root_dir, check=True)\n",
    "\n",
    "# # Detect the default branch name (main or master)\n",
    "# branch_result = subprocess.run(\n",
    "#     [\"git\", \"branch\", \"-r\"],\n",
    "#     cwd=root_dir,\n",
    "#     capture_output=True,\n",
    "#     text=True,\n",
    "#     check=True\n",
    "# )\n",
    "\n",
    "# # Determine which default branch exists\n",
    "# if \"origin/main\" in branch_result.stdout:\n",
    "#     default_branch = \"main\"\n",
    "# elif \"origin/master\" in branch_result.stdout:\n",
    "#     default_branch = \"master\"\n",
    "# else:\n",
    "#     raise RuntimeError(\"Could not find origin/main or origin/master branch\")\n",
    "\n",
    "# print(f\"   Detected default branch: {default_branch}\")\n",
    "\n",
    "# # Get current branch\n",
    "# current_branch_result = subprocess.run(\n",
    "#     [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n",
    "#     cwd=root_dir,\n",
    "#     capture_output=True,\n",
    "#     text=True,\n",
    "#     check=True\n",
    "# )\n",
    "# current_branch = current_branch_result.stdout.strip()\n",
    "\n",
    "# # If we're in detached HEAD or on wrong branch, checkout the default branch\n",
    "# if current_branch != default_branch:\n",
    "#     print(f\"   Switching from {current_branch} to {default_branch}\")\n",
    "#     subprocess.run([\"git\", \"checkout\", default_branch], cwd=root_dir, check=True)\n",
    "\n",
    "# # Reset to remote state (discarding local notebook changes)\n",
    "# print(f\"   Resetting to origin/{default_branch}\")\n",
    "# subprocess.run(\n",
    "#     [\"git\", \"reset\", \"--hard\", f\"origin/{default_branch}\"],\n",
    "#     cwd=root_dir,\n",
    "#     check=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35adda7",
   "metadata": {},
   "source": [
    "### 4.7.4. Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b89cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e77a05f",
   "metadata": {},
   "source": [
    "### 4.10. Pipeline Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ac922",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ BATCH INFERENCE PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - Predictions generated: {len(predictions)}\")\n",
    "print(f\"   - Heatmap images created: {len(interpolation_df['date'].unique())}\")\n",
    "print(f\"   - Feature group: {predictions_fg.name} (v{predictions_fg.version})\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
