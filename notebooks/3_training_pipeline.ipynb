{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e975c5",
   "metadata": {},
   "source": [
    "# 3. Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b6067",
   "metadata": {},
   "source": [
    "## 3.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73a5f2",
   "metadata": {},
   "source": [
    "### 3.1.1. Import Libraries and initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings()\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad062b7b",
   "metadata": {},
   "source": [
    "### 3.1.2. Repository Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e887f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63871bc",
   "metadata": {},
   "source": [
    "### 3.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secrets = hopsworks.get_secrets_api()\n",
    "# AQICN_API_KEY = secrets.get_secret(\"AQICN_API_KEY\").value\n",
    "\n",
    "# # Retrieve feature groups\n",
    "# air_quality_fg = fs.get_feature_group(\n",
    "#     name=\"air_quality\",\n",
    "#     version=1,\n",
    "# )\n",
    "\n",
    "# weather_fg = fs.get_feature_group(\n",
    "#     name=\"weather\",\n",
    "#     version=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8b26b",
   "metadata": {},
   "source": [
    "## 3.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec603c8",
   "metadata": {},
   "source": [
    "## 3.3. Load Sensor Locations from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Build sensor location dictionary: sensor_id -> (lat, lon, city, street, country, aqicn_url)\n",
    "sensor_locations = {}\n",
    "existing_aq_data = air_quality_fg.read()\n",
    "existing_sensors = set(existing_aq_data[\"sensor_id\"].unique())\n",
    "print(f\"üìã Found {len(existing_sensors)} sensors in feature store\")\n",
    "\n",
    "# Build location dict\n",
    "for _, row in existing_aq_data[[\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\"]].drop_duplicates(subset=[\"sensor_id\"]).iterrows():\n",
    "    sensor_locations[row[\"sensor_id\"]] = (\n",
    "        row[\"latitude\"], \n",
    "        row[\"longitude\"], \n",
    "        row[\"city\"], \n",
    "        row[\"street\"], \n",
    "        row[\"country\"]\n",
    "    )\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dec32e",
   "metadata": {},
   "source": [
    "## 3.4. Create Additional Feature Views\n",
    "Create multiple feature views with different feature combinations (baseline, rolling windows, lagged features, nearby sensors, complete) for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "baseline_features = air_quality_fg.select([\"pm25\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "\n",
    "baseline_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_baseline_fv\",\n",
    "    description=\"Weather features for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=baseline_features,\n",
    ")\n",
    "\n",
    "rolling_features = air_quality_fg.select([\"pm25\", \"pm25_rolling_3d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "rolling_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_rolling_fv\",\n",
    "    description=\"Weather features, PM2.5 rolling window (3d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=rolling_features,\n",
    ")\n",
    "\n",
    "nearby_features = air_quality_fg.select([\"pm25\", \"pm25_nearby_avg\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "nearby_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_nearby_fv\",\n",
    "    description=\"Weather features, PM2.5 nearby average (1d lag, 3 sensors) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=nearby_features,\n",
    ")\n",
    "\n",
    "lagged_1d_features = air_quality_fg.select([\"pm25\", \"pm25_lag_1d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "lagged_1d_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_lagged_1d_fv\",\n",
    "    description=\"Weather features, PM2.5 lags (1d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=lagged_1d_features,\n",
    ")\n",
    "\n",
    "lagged_2d_features = air_quality_fg.select([\"pm25\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "lagged_2d_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_lagged_2d_fv\",\n",
    "    description=\"Weather features, PM2.5 lags (1d, 2d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=lagged_2d_features,\n",
    ")\n",
    "\n",
    "lagged_3d_features = air_quality_fg.select([\"pm25\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "lagged_3d_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_lagged_3d_fv\",\n",
    "    description=\"Weather features, PM2.5 lags (1d, 2d, 3d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=lagged_3d_features,\n",
    ")\n",
    "\n",
    "complete_features = air_quality_fg.select([\"pm25\", \"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\", \"pm25_nearby_avg\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "complete_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_complete_fv\",\n",
    "    description=\"Weather features, PM2.5 rolling window (3d), and PM2.5 lags (1d, 2d, 3d), and PM2.5 nearby average (1d lag, 3 sensors) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=complete_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b046b",
   "metadata": {},
   "source": [
    "## 3.5. Model Training Setup\n",
    "Set up test data split date, initialize containers for models and predictions, and define feature view dictionary for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = defaultdict(dict)\n",
    "y_preds = defaultdict(dict)\n",
    "results = []\n",
    "\n",
    "feature_views = {\n",
    "    \"baseline\": baseline_feature_view,\n",
    "    \"rolling\": rolling_feature_view,\n",
    "    \"nearby\": nearby_feature_view,\n",
    "    \"lagged_1d\": lagged_1d_feature_view,\n",
    "    \"lagged_2d\": lagged_2d_feature_view,\n",
    "    \"lagged_3d\": lagged_3d_feature_view,\n",
    "    \"complete\": complete_feature_view,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9685e4",
   "metadata": {},
   "source": [
    "## 3.7. Model Training Loop\n",
    "Train XGBoost models for each feature combination and sensor, run 5 iterations per configuration, select best model based on R2 score, and store results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store candidates in Hopsworks as artifacts, not models (for reproducibility or auditing), \n",
    "# store models as artifacts in the Feature Store or in a Dataset, not the Model Registry.\n",
    "\n",
    "model_dir = f\"{root_dir}/models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "print(f\"Model directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean for weather features for each day and sensor_id and replace in fg\n",
    "weather_daily_mean = (\n",
    "    weather_fg.read()\n",
    "    .groupby([\"sensor_id\", \"date\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dfc4f",
   "metadata": {},
   "source": [
    "## 3.8. Debug data quality\n",
    "To store trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  DEBUG:\n",
    "# print(\"üîç DEBUGGING DATA QUALITY\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Check one sensor's data\n",
    "# first_feature_name = list(feature_views.keys())[0]\n",
    "# sample_data = feature_views[first_feature_name].query.read()\n",
    "# test_sensor_id = sample_data[\"sensor_id\"].iloc[0]\n",
    "\n",
    "# print(f\"\\nüìä Checking sensor {test_sensor_id}:\")\n",
    "\n",
    "# for feature_name, feature_view in feature_views.items():\n",
    "#     data = feature_view.query.read()\n",
    "#     sensor_data = data[data[\"sensor_id\"] == test_sensor_id].copy()\n",
    "    \n",
    "#     if len(sensor_data) == 0:\n",
    "#         print(f\"  {feature_name}: NO DATA\")\n",
    "#         continue\n",
    "    \n",
    "#     print(f\"\\n  {feature_name}: {len(sensor_data)} rows\")\n",
    "#     print(f\"    Date range: {sensor_data['date'].min()} to {sensor_data['date'].max()}\")\n",
    "    \n",
    "#     # Check each column for NaN\n",
    "#     for col in sensor_data.columns:\n",
    "#         nan_count = sensor_data[col].isna().sum()\n",
    "#         nan_pct = (nan_count / len(sensor_data)) * 100\n",
    "#         if nan_pct > 0:\n",
    "#             print(f\"    {col}: {nan_count}/{len(sensor_data)} NaN ({nan_pct:.1f}%)\")\n",
    "    \n",
    "#     # Show first few rows\n",
    "#     print(f\"\\n    Sample data:\")\n",
    "#     print(sensor_data.head())\n",
    "#     break  # Only check baseline for now\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dca006",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(feature_name, data.shape)\n",
    "# print(data[\"sensor_id\"].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ccc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=== FEATURE VIEW CHECK ===\")\n",
    "# for feature_name, feature_view in feature_views.items():\n",
    "#     data = feature_view.query.read()\n",
    "#     print(feature_name, data.shape)\n",
    "#     print(\"unique sensors:\", len(data[\"sensor_id\"].unique()))\n",
    "#     print(\"sample sensors:\", data[\"sensor_id\"].unique()[:5])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature_name, feature_view in feature_views.items():\n",
    "#     data = feature_view.query.read()\n",
    "#     data['date'] = pd.to_datetime(data['date']).dt.tz_localize(None)\n",
    "\n",
    "#     for sensor_id in data[\"sensor_id\"].unique():\n",
    "#         df = data[data[\"sensor_id\"] == sensor_id].copy()\n",
    "#         print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490129a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {name: {} for name in feature_views.keys()}\n",
    "y_preds = {name: {} for name in feature_views.keys()}\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c78c5",
   "metadata": {},
   "source": [
    "### Load Feature Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_cache = {}\n",
    "\n",
    "for feature_name, feature_view in feature_views.items():\n",
    "    print(\"Reading\", feature_name)\n",
    "    df = feature_view.query.read()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "    feature_data_cache[feature_name] = df\n",
    "\n",
    "joblib.dump(feature_data_cache, \"feature_data_cache.pkl\")\n",
    "print(\"Saved feature data cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9a53a",
   "metadata": {},
   "source": [
    "### Prepare a List of All Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c889c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "\n",
    "for feature_name, df in feature_data_cache.items():\n",
    "    for sensor_id in df[\"sensor_id\"].unique():\n",
    "        tasks.append((feature_name, sensor_id))\n",
    "\n",
    "joblib.dump(tasks, \"tasks.pkl\")\n",
    "print(\"Prepared task list:\", len(tasks), \"tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595b3fc",
   "metadata": {},
   "source": [
    "### Load Checkpoints is they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"models.pkl\"):\n",
    "    models = joblib.load(\"models.pkl\")\n",
    "else:\n",
    "    models = {}\n",
    "\n",
    "if os.path.exists(\"results.pkl\"):\n",
    "    results = joblib.load(\"results.pkl\")\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "if os.path.exists(\"preds.pkl\"):\n",
    "    y_preds = joblib.load(\"preds.pkl\")\n",
    "else:\n",
    "    y_preds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97c416",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name, sensor_id in tasks:\n",
    "    print(feature_name, sensor_id)\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if feature_name in models and sensor_id in models[feature_name]:\n",
    "        print(\"Skipping\", feature_name, sensor_id)\n",
    "        continue\n",
    "\n",
    "    df = feature_data_cache[feature_name]\n",
    "    df = df[df[\"sensor_id\"] == sensor_id].dropna(subset=[\"pm25\"])\n",
    "\n",
    "    if len(df) < 10:\n",
    "        continue\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in [\n",
    "        \"pm25\",\"date\",\"sensor_id\",\"city\",\"street\",\"country\",\n",
    "        \"latitude\",\"longitude\",\"aqicn_url\"\n",
    "    ]]\n",
    "\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    if len(test_df) < 2:\n",
    "        continue\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"pm25\"]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[\"pm25\"]\n",
    "\n",
    "    best_model = None\n",
    "    best_r2 = -1e9\n",
    "    best_mse = 1e9\n",
    "\n",
    "    for i in range(5):\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            random_state=165439*i\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_mse = mse\n",
    "            best_model = model\n",
    "\n",
    "    # Save results in memory\n",
    "    models.setdefault(feature_name, {})[sensor_id] = best_model\n",
    "    y_preds.setdefault(feature_name, {})[sensor_id] = best_model.predict(X_test)\n",
    "    results.append({\n",
    "        \"feature_name\": feature_name,\n",
    "        \"sensor_id\": sensor_id,\n",
    "        \"R2\": best_r2,\n",
    "        \"MSE\": best_mse,\n",
    "        \"train_size\": len(train_df),\n",
    "        \"test_size\": len(test_df),\n",
    "    })\n",
    "\n",
    "    # Save checkpoint after each task\n",
    "    joblib.dump(models, \"models.pkl\")\n",
    "    joblib.dump(results, \"results.pkl\")\n",
    "    joblib.dump(y_preds, \"preds.pkl\")\n",
    "\n",
    "    print(\"Saved\", feature_name, sensor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature_name, feature_view in feature_views.items():\n",
    "#     data = feature_view.query.read()\n",
    "#     data['date'] = pd.to_datetime(data['date']).dt.tz_localize(None)\n",
    "\n",
    "#     for sensor_id in data[\"sensor_id\"].unique():\n",
    "#         df = data[data[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "#         # Drop rows where pm25 is NaN\n",
    "#         df = df.dropna(subset=[\"pm25\"])\n",
    "        \n",
    "#         if len(df) < 10:\n",
    "#             print(f\"‚ö†Ô∏è  Skipping sensor {sensor_id} ({feature_name}): insufficient data ({len(df)} rows)\")\n",
    "#             continue\n",
    "\n",
    "#         # Identify which features this feature view actually uses\n",
    "#         # Drop non-feature columns first\n",
    "#         feature_cols = [c for c in df.columns if c not in [\"pm25\", \"date\", \"sensor_id\", \"city\", \"street\", \"country\", \"latitude\", \"longitude\", \"aqicn_url\"]]\n",
    "        \n",
    "#         # Keep rows with NaN in features - XGBoost handles NaN natively\n",
    "#         df_clean = df.copy()\n",
    "        \n",
    "#         if len(df_clean) < 10:\n",
    "#             print(f\"‚ö†Ô∏è  Skipping sensor {sensor_id} ({feature_name}): insufficient clean data ({len(df_clean)} rows, needed features: {feature_cols})\")\n",
    "#             continue\n",
    "\n",
    "#         # Split\n",
    "#         train_size = int(0.8 * len(df_clean))\n",
    "#         train_df = df_clean.iloc[:train_size]\n",
    "#         test_df = df_clean.iloc[train_size:]\n",
    "\n",
    "#         if len(test_df) < 2:\n",
    "#             print(f\"‚ö†Ô∏è  Skipping sensor {sensor_id} ({feature_name}): test set too small ({len(test_df)} rows)\")\n",
    "#             continue\n",
    "\n",
    "#         # Prepare features and target\n",
    "#         X_train = train_df[feature_cols]\n",
    "#         y_train = train_df[\"pm25\"]\n",
    "#         X_test = test_df[feature_cols]\n",
    "#         y_test = test_df[\"pm25\"]\n",
    "\n",
    "#         best_r2 = -float('inf')\n",
    "#         best_mse = float('inf')\n",
    "#         best_model = None\n",
    "\n",
    "#         mse_list = []\n",
    "#         r2_list = []\n",
    "        \n",
    "#         for i in range(5):\n",
    "#             model = XGBRegressor(n_estimators=100, learning_rate=0.05, random_state=165439*i)\n",
    "#             model.fit(X_train, y_train)\n",
    "#             y_pred = model.predict(X_test)\n",
    "#             r2 = r2_score(y_test, y_pred)\n",
    "#             mse = mean_squared_error(y_test, y_pred)\n",
    "#             r2_list.append(r2)\n",
    "#             mse_list.append(mse)\n",
    "            \n",
    "#             if r2 > best_r2:\n",
    "#                 best_r2 = r2\n",
    "#                 best_mse = mse\n",
    "#                 best_model = model\n",
    "\n",
    "#         models[feature_name][sensor_id] = best_model\n",
    "\n",
    "#         if best_model is not None:\n",
    "#             y_preds[feature_name][sensor_id] = best_model.predict(X_test)\n",
    "#             results.append({\n",
    "#                 \"feature_name\": feature_name,\n",
    "#                 \"sensor_id\": sensor_id,\n",
    "#                 \"MSE\": sum(mse_list) / len(mse_list),\n",
    "#                 \"R2\": sum(r2_list) / len(r2_list),\n",
    "#                 \"train_size\": len(X_train),\n",
    "#                 \"test_size\": len(X_test),\n",
    "#             })\n",
    "#             print(f\"‚úÖ {feature_name} - Sensor {sensor_id}: R2={best_r2:.3f}, MSE={best_mse:.2f}\")\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è  No valid model for {feature_name} - {sensor_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7235fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature_name, feature_view in feature_views.items():\n",
    "#     data = feature_view.query.read()\n",
    "#     data['date'] = pd.to_datetime(data['date']).dt.tz_localize(None)\n",
    "\n",
    "#     for sensor_id in metadata_df.index:\n",
    "#         df = data[data[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "#         # Clean the data before splitting\n",
    "#         df = df.dropna(subset=[\"pm25\"])\n",
    "#         # features_for_cleaning = df.drop(columns=[\"pm25\", \"date\", \"city\", \"sensor_id\"])\n",
    "#         features_for_cleaning = df.drop(columns=[\"pm25\", \"date\", \"sensor_id\"])\n",
    "\n",
    "#         target_for_cleaning = df[\"pm25\"]\n",
    "#         clean_mask = ~(features_for_cleaning.isna().any(axis=1) | target_for_cleaning.isna())\n",
    "#         df_clean = df[clean_mask].copy()\n",
    "#         if len(df_clean) < 10:\n",
    "#             print(f\"‚ö†Ô∏è  Skipping sensor {sensor_id}: insufficient data after cleaning ({len(df_clean)} rows)\")\n",
    "#             continue\n",
    "\n",
    "#         train_size = int(0.8 * len(df_clean))\n",
    "#         train_df = df_clean.iloc[:train_size]\n",
    "#         test_df = df_clean.iloc[train_size:]\n",
    "\n",
    "#         if len(test_df) < 2:\n",
    "#             print(f\"‚ö†Ô∏è  Skipping sensor {sensor_id}: test set too small after split ({len(test_df)} rows)\")\n",
    "#             continue\n",
    "\n",
    "#         # Drop non-feature columns (pm25 is target, others are metadata)\n",
    "#         X_train = train_df.drop(columns=[\"pm25\", \"date\", \"city\", \"sensor_id\"])\n",
    "#         y_train = train_df[\"pm25\"]\n",
    "#         X_test = test_df.drop(columns=[\"pm25\", \"date\", \"city\", \"sensor_id\"])\n",
    "#         y_test = test_df[\"pm25\"]\n",
    "\n",
    "#         best_r2 = -float('inf')\n",
    "#         best_mse = float('inf')\n",
    "#         best_model = None\n",
    "\n",
    "#         mse_list = []\n",
    "#         r2_list = []\n",
    "#         for i in range(5):\n",
    "#             model = XGBRegressor(n_estimators=100, learning_rate=0.05, random_state=165439*i)\n",
    "#             model.fit(X_train, y_train)\n",
    "#             y_pred = model.predict(X_test)\n",
    "#             r2 = r2_score(y_test, y_pred)\n",
    "#             mse = mean_squared_error(y_test, y_pred)\n",
    "#             r2_list.append(r2)\n",
    "#             mse_list.append(mse)\n",
    "#             if r2 > best_r2:\n",
    "#                 best_r2 = r2\n",
    "#                 best_mse = mse\n",
    "#                 best_model = model\n",
    "\n",
    "#         models[feature_name][sensor_id] = best_model\n",
    "\n",
    "#         if best_model is not None:\n",
    "#             y_preds[feature_name][sensor_id] = best_model.predict(X_test)\n",
    "#             results.append({\n",
    "#                 \"feature_name\": feature_name,\n",
    "#                 \"sensor_id\": sensor_id,\n",
    "#                 \"MSE\": sum(mse_list) / len(mse_list),\n",
    "#                 \"R2\": sum(r2_list) / len(r2_list),\n",
    "#                 \"train_size\": len(X_train),\n",
    "#                 \"test_size\": len(X_test),\n",
    "#             })\n",
    "#         else:\n",
    "#             print(f\"‚ö†Ô∏è  No valid model trained for {feature_name} - {sensor_id}, R2 scores: {r2_list}, Best R2: {best_r2}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f965ae3",
   "metadata": {},
   "source": [
    "## 3.9. Model Selection and Saving\n",
    "Identify best performing model for each sensor, save models and feature importance plots, and prepare test data with predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36cd914",
   "metadata": {},
   "source": [
    "### 3.9.1. Find Best Model per Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4410cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "best_models = results_df.loc[results_df.groupby(\"sensor_id\")[\"R2\"].idxmax()]\n",
    "best_models = best_models.reset_index(drop=False)\n",
    "\n",
    "print(\"Best models per sensor:\")\n",
    "print(best_models[[\"sensor_id\", \"feature_name\", \"R2\", \"MSE\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef473b",
   "metadata": {},
   "source": [
    "### 3.9.2. Cache all Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_feature_data = {\n",
    "    name: fv.query.read()\n",
    "    for name, fv in feature_views.items()\n",
    "}\n",
    "\n",
    "# Normalize dates in cached data\n",
    "for name, df_cached in cached_feature_data.items():\n",
    "    df_cached[\"date\"] = pd.to_datetime(df_cached[\"date\"]).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204ea1b",
   "metadata": {},
   "source": [
    "### 3.9.3. Loop Through Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01838162",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "\n",
    "# --- Loop through best models ---\n",
    "for _, row in best_models.iterrows():\n",
    "    sensor_id = row['sensor_id']\n",
    "    print(sensor_id)\n",
    "\n",
    "    best_feature = row[\"feature_name\"]\n",
    "\n",
    "    # --- Save model + feature importance ---\n",
    "    sensor_dir = f\"{model_dir}/{sensor_id}\"\n",
    "    images_dir = f\"{sensor_dir}/images\"\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    best_model = models[best_feature][sensor_id]\n",
    "    best_model.save_model(f\"{sensor_dir}/model.json\")\n",
    "\n",
    "    plot_importance(best_model)\n",
    "    plt.savefig(f\"{images_dir}/feature_importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- Load cached feature view data ---\n",
    "    df = cached_feature_data[best_feature]\n",
    "    df = df[df[\"sensor_id\"] == sensor_id].copy()\n",
    "\n",
    "    # --- EXACT same cleaning as training ---\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    df = df.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in [\"pm25\", \"date\", \"sensor_id\", \"city\", \"street\", \"country\", \"latitude\", \"longitude\", \"aqicn_url\"]\n",
    "    ]\n",
    "\n",
    "    # Keep rows with NaN in features - same as training (XGBoost handles NaN)\n",
    "    df_clean = df.copy()\n",
    "    if len(df_clean) < 10:\n",
    "        continue\n",
    "\n",
    "    # --- Generate predictions for FULL dataset ---\n",
    "    X_full = df_clean[feature_cols]\n",
    "    predictions_full = best_model.predict(X_full)\n",
    "    df_clean[\"predicted_pm25\"] = predictions_full\n",
    "    df_clean[\"best_model\"] = best_feature\n",
    "    \n",
    "    # --- Filter to last 18 months for hindcast visualization ---\n",
    "    cutoff_date = pd.Timestamp.now() - pd.DateOffset(months=18)\n",
    "    df_hindcast = df_clean[df_clean[\"date\"] >= cutoff_date].copy()\n",
    "    \n",
    "    if len(df_hindcast) == 0:\n",
    "        print(f\"‚ö†Ô∏è Skipping sensor {sensor_id}: no data in last 18 months\")\n",
    "        continue\n",
    "\n",
    "   # Get lat/lon AND city/street from metadata\n",
    "    if sensor_id in aq_data.index:\n",
    "        df_clean[\"latitude\"] = aq_data.at[sensor_id, \"latitude\"]\n",
    "        df_clean[\"longitude\"] = aq_data.at[sensor_id, \"longitude\"]\n",
    "        df_clean[\"city\"] = aq_data.at[sensor_id, \"city\"]\n",
    "        df_clean[\"street\"] = aq_data.at[sensor_id, \"street\"]\n",
    "        df_clean[\"sensor_id\"] = sensor_id\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping sensor {sensor_id}: metadata not found\")\n",
    "        continue\n",
    "\n",
    "    all_test_data.append(\n",
    "        df_clean[[\"date\", \"sensor_id\", \"pm25\", \"predicted_pm25\", \"latitude\", \"longitude\", \"city\", \"street\", \"best_model\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd65c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIAGNOSTIC: Check data coverage by year ---\n",
    "print(\"\\nüìä DATA COVERAGE DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature_name, df_cached in cached_feature_data.items():\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    \n",
    "    # Check one sensor as example\n",
    "    sample_sensor = df_cached[\"sensor_id\"].iloc[0]\n",
    "    sensor_data = df_cached[df_cached[\"sensor_id\"] == sample_sensor].copy()\n",
    "    sensor_data[\"year\"] = pd.to_datetime(sensor_data[\"date\"]).dt.year\n",
    "    \n",
    "    print(f\"  Sensor {sample_sensor}:\")\n",
    "    print(f\"  Total rows: {len(sensor_data)}\")\n",
    "    print(f\"  Date range: {sensor_data['date'].min()} to {sensor_data['date'].max()}\")\n",
    "    \n",
    "    # Check pm25 by year\n",
    "    yearly_stats = sensor_data.groupby(\"year\").agg({\n",
    "        \"pm25\": [\"count\", lambda x: x.isna().sum()]\n",
    "    })\n",
    "    yearly_stats.columns = [\"Total Rows\", \"NaN Count\"]\n",
    "    yearly_stats[\"Non-NaN\"] = yearly_stats[\"Total Rows\"] - yearly_stats[\"NaN Count\"]\n",
    "    print(\"\\n  PM2.5 availability by year:\")\n",
    "    print(yearly_stats)\n",
    "    \n",
    "    # After dropna\n",
    "    sensor_data_clean = sensor_data.dropna(subset=[\"pm25\"])\n",
    "    if len(sensor_data_clean) > 0:\n",
    "        print(f\"\\n  After dropna(subset=['pm25']):\")\n",
    "        print(f\"    Remaining rows: {len(sensor_data_clean)}\")\n",
    "        print(f\"    Date range: {sensor_data_clean['date'].min()} to {sensor_data_clean['date'].max()}\")\n",
    "        print(f\"    Years present: {sorted(sensor_data_clean['year'].unique())}\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è NO DATA LEFT after dropna!\")\n",
    "    \n",
    "    break  # Only check first feature view for now\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee87500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find best model (highest R2) for each sensor\n",
    "# results_df = pd.DataFrame(results)\n",
    "# best_models = results_df.loc[results_df.groupby('sensor_id')['R2'].idxmax()]\n",
    "# best_models = results_df.loc[results_df.groupby('sensor_id')['R2'].idxmax()]\n",
    "\n",
    "# print(\"Best models per sensor:\")\n",
    "# print(best_models[['sensor_id', 'feature_name', 'R2', 'MSE']])\n",
    "\n",
    "# all_data = baseline_features.read()\n",
    "# all_data['date'] = pd.to_datetime(all_data['date']).dt.tz_localize(None)\n",
    "\n",
    "# cached_feature_data = {\n",
    "#     name: fv.query.read()\n",
    "#     for name, fv in feature_views.items()\n",
    "# }\n",
    "\n",
    "# all_test_data = []\n",
    "\n",
    "# for _, row in best_models.iterrows():\n",
    "#     sensor_id = row['sensor_id']\n",
    "#     best_feature = row['feature_name']\n",
    "    \n",
    "#     sensor_dir = f\"{model_dir}/{sensor_id}\"\n",
    "#     if not os.path.exists(sensor_dir):\n",
    "#         os.mkdir(sensor_dir)\n",
    "#     images_dir = f\"{model_dir}/{sensor_id}/images\"\n",
    "#     if not os.path.exists(images_dir):\n",
    "#         os.mkdir(images_dir)\n",
    "\n",
    "#     best_model = models[best_feature][sensor_id]\n",
    "#     model_path = f\"{sensor_dir}/model.json\"\n",
    "#     plot_importance(best_model)\n",
    "#     importance_path = f\"{images_dir}/feature_importance.png\"\n",
    "#     plt.savefig(importance_path)\n",
    "#     plt.close()\n",
    "    \n",
    "#     best_model.save_model(model_path)\n",
    "\n",
    "#     # Use the same feature view and data processing logic that was used for training\n",
    "#     best_feature_view = feature_views[best_feature]\n",
    "#     sensor_data = cached_feature_data[best_feature]\n",
    "#     sensor_data['date'] = pd.to_datetime(sensor_data['date']).dt.tz_localize(None)\n",
    "    \n",
    "#     df = sensor_data[sensor_data['sensor_id'] == sensor_id].copy()\n",
    "    \n",
    "#     df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "#     # Apply EXACT same cleaning logic as in training loop\n",
    "#     df = df.dropna(subset=['pm25'])\n",
    "    \n",
    "#     # Recompute feature columns exactly as in training\n",
    "#     feature_cols = [c for c in df.columns if c not in [\"pm25\", \"date\", \"sensor_id\", \"location_id\"]]\n",
    "\n",
    "#     # Drop rows with NaN in any feature\n",
    "#     df_clean = df.dropna(subset=feature_cols).copy()\n",
    "\n",
    "#     if len(df_clean) < 10:\n",
    "#         continue\n",
    "\n",
    "#     # Same split as training\n",
    "#     train_size = int(0.8 * len(df_clean))\n",
    "#     test_df = df_clean.iloc[train_size:].copy()\n",
    "\n",
    "#     # # Create feature matrix for comprehensive NaN cleaning (same as training)\n",
    "#     # features_for_cleaning = df.drop(columns=[\"pm25\", \"date\", \"sensor_id\"])\n",
    "#     # target_for_cleaning = df[\"pm25\"]\n",
    "    \n",
    "#     # # Remove rows with NaN values in any feature or target (same as training)\n",
    "#     # clean_mask = ~(features_for_cleaning.isna().any(axis=1) | target_for_cleaning.isna())\n",
    "#     # df_clean = df[clean_mask].copy()\n",
    "    \n",
    "#     # if len(df_clean) < 10:\n",
    "#     #     continue\n",
    "        \n",
    "#     # # Split the cleaned data (same as training)\n",
    "#     # train_size = int(0.8 * len(df_clean))\n",
    "#     # test_df = df_clean.iloc[train_size:].copy()\n",
    "    \n",
    "#     if len(test_df) == 0:\n",
    "#         continue\n",
    "    \n",
    "#     # Test data is already clean from the comprehensive cleaning above\n",
    "#     clean_test_df = test_df.copy()\n",
    "#     predictions = y_preds[best_feature][sensor_id]\n",
    "    \n",
    "#     if len(clean_test_df) == len(predictions):\n",
    "#         clean_test_df['predicted_pm25'] = predictions\n",
    "#         clean_test_df['best_model'] = best_feature\n",
    "#         all_test_data.append(clean_test_df[['date', 'pm25', 'predicted_pm25', 'latitude', 'longitude', 'best_model']])\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è  Skipping sensor {sensor_id}: prediction length mismatch ({len(predictions)} vs {len(clean_test_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e31b22",
   "metadata": {},
   "source": [
    "## 3.10. Model Registration & Visualization\n",
    "Create prediction plots for each sensor, register model in Hopsworks model registry with metrics and save models with their configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31340430",
   "metadata": {},
   "source": [
    "### 3.10.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in df:\", df.columns.tolist())\n",
    "print(\"df shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "df = pd.concat(all_test_data, ignore_index=True) if all_test_data else pd.DataFrame()\n",
    "df = df.sort_values(by=[\"date\"])\n",
    "df_by_sensor = {sid: g.copy() for sid, g in df.groupby(\"sensor_id\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f72f5",
   "metadata": {},
   "source": [
    "### 3.10.2. Precompute Best Model per Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25201600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_per_sensor = (\n",
    "#     results_df.sort_values(\"R2\", ascending=False)\n",
    "#               .groupby(\"sensor_id\")\n",
    "#               .first()[[\"feature_name\", \"R2\", \"MSE\"]]\n",
    "#               .rename(columns={\"feature_name\": \"best_model\"})\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631c0ab",
   "metadata": {},
   "source": [
    "### 3.10.3. Precompute Training Dataset Versions per Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_versions = {}\n",
    "\n",
    "for model_name, fv in feature_views.items():\n",
    "    try:\n",
    "        # Check if a training dataset already exists\n",
    "        existing = fv.get_training_data_versions()\n",
    "        if existing:\n",
    "            version = existing[-1]   # latest version\n",
    "            print(f\"‚ÑπÔ∏è Using existing training dataset for {model_name}: version {version}\")\n",
    "            training_versions[model_name] = version\n",
    "            continue\n",
    "\n",
    "        # Otherwise create a new one\n",
    "        td_version, td_job = fv.create_training_data(\n",
    "            description=f\"Training data for model type {model_name}\",\n",
    "            data_format=\"csv\",\n",
    "            write_options={\"wait_for_job\": True}\n",
    "        )\n",
    "\n",
    "        version = td_version.version if hasattr(td_version, \"version\") else td_version\n",
    "        training_versions[model_name] = version\n",
    "        print(f\"‚úÖ Created training dataset for {model_name}: version {version}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not create training dataset for model type {model_name}: {e}\")\n",
    "        training_versions[model_name] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_versions = {}\n",
    "\n",
    "# for model_name, fv in feature_views.items():\n",
    "#     try:\n",
    "#         td_version, td_job = fv.create_training_data(\n",
    "#             description=f\"Training data for model type {model_name}\",\n",
    "#             data_format=\"csv\",\n",
    "#             write_options={\"wait_for_job\": True}\n",
    "#         )\n",
    "\n",
    "#         # Handle both possible return types\n",
    "#         if hasattr(td_version, \"version\"):\n",
    "#             training_versions[model_name] = td_version.version\n",
    "#         else:\n",
    "#             # td_version is already an int\n",
    "#             training_versions[model_name] = td_version\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Could not create training dataset for model type {model_name}: {e}\")\n",
    "#         training_versions[model_name] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d034d",
   "metadata": {},
   "source": [
    "### 3.10.4. Loop over Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id in df_by_sensor.keys():  # Loop through sensors that have predictions\n",
    "    df_subset = df_by_sensor[sensor_id].copy()\n",
    "    \n",
    "    # Get city/street directly from the data\n",
    "    city = df_subset['city'].iloc[0]\n",
    "    street = df_subset['street'].iloc[0]\n",
    "\n",
    "    # Get best model info\n",
    "    if sensor_id not in best_models.index:\n",
    "        continue\n",
    "\n",
    "    best_model_name = best_models.loc[sensor_id, \"best_model\"]\n",
    "    best_model_r2 = best_models.loc[sensor_id, \"R2\"]\n",
    "    best_model_mse = best_models.loc[sensor_id, \"MSE\"]\n",
    "\n",
    "    # Get feature view for this model type\n",
    "    best_model_feature_view = feature_views[best_model_name]\n",
    "\n",
    "    # Get precomputed training dataset version\n",
    "    training_dataset_version = training_versions.get(best_model_name, None)\n",
    "\n",
    "    # Drop unnecessary columns for visualization\n",
    "    df_subset = df_subset.drop(columns=[\"latitude\", \"longitude\", \"best_model\", \"sensor_id\", \"city\", \"street\"], errors=\"ignore\")\n",
    "\n",
    "    images_dir = f\"{model_dir}/{sensor_id}/images\"\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    image_path = f\"{images_dir}/hindcast_training.png\"\n",
    "\n",
    "    fig = visualization.plot_air_quality_forecast(\n",
    "        city, street, df_subset, image_path, hindcast=True\n",
    "    )\n",
    "    if fig is not None:\n",
    "        fig.suptitle(f\"{city} {street} (Best Model: {best_model_name})\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Register model\n",
    "    model_kwargs = {\n",
    "        \"name\": f\"air_quality_xgboost_model_{sensor_id}\",\n",
    "        \"metrics\": {\n",
    "            \"R2\": best_model_r2,\n",
    "            \"MSE\": best_model_mse,\n",
    "        },\n",
    "        \"feature_view\": best_model_feature_view,\n",
    "        \"description\": (\n",
    "            f\"Air Quality (PM2.5) predictor for {city} {street} \"\n",
    "            f\"using {best_model_name} configuration\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if training_dataset_version is not None:\n",
    "        model_kwargs[\"training_dataset_version\"] = training_dataset_version\n",
    "\n",
    "    aq_model = mr.python.create_model(**model_kwargs)\n",
    "    aq_model.save(f\"{model_dir}/{sensor_id}\")\n",
    "\n",
    "print(\"‚úÖ All models registered and visualizations generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mr = project.get_model_registry()\n",
    "# df = pd.concat(all_test_data, ignore_index=True) if all_test_data else pd.DataFrame()\n",
    "# df = df.sort_values(by=[\"date\"])\n",
    "\n",
    "# # Plot the best model for each sensor\n",
    "# for sensor_id, meta in metadata_df.iterrows():\n",
    "#     city = meta[\"city\"]\n",
    "#     street = meta[\"street\"]\n",
    "#     latitude = meta[\"latitude\"]\n",
    "#     longitude = meta[\"longitude\"]\n",
    "    \n",
    "#     df_subset = df[(df[\"latitude\"] == latitude) & (df[\"longitude\"] == longitude)].copy()\n",
    "#     if len(df_subset) == 0:\n",
    "#         continue\n",
    "    \n",
    "#     # Get the best model name for display\n",
    "#     best_model_name = df_subset['best_model'].iloc[0] if 'best_model' in df_subset.columns else 'unknown'\n",
    "#     best_model_r2 = df_subset['R2'].iloc[0] if 'R2' in df_subset.columns else 0\n",
    "#     best_model_mse = df_subset['MSE'].iloc[0] if 'MSE' in df_subset.columns else 0\n",
    "#     best_model_feature_view = feature_views[best_model_name]\n",
    "    \n",
    "#     df_subset = df_subset.sort_values(by=[\"date\"])\n",
    "#     df_subset = df_subset.drop(columns=[\"latitude\", \"longitude\", \"best_model\"])\n",
    "    \n",
    "#     images_dir = f\"{model_dir}/{sensor_id}/images\"\n",
    "#     image_path = f\"{images_dir}/hindcast_training.png\"\n",
    "    \n",
    "#     fig = visualization.plot_air_quality_forecast(\n",
    "#         city, street, df_subset, image_path, hindcast=True\n",
    "#     )\n",
    "#     if fig is not None:\n",
    "#         fig.suptitle(f\"{city} {street} (Best Model: {best_model_name})\")\n",
    "#         plt.close(fig)  # Clean up after saving\n",
    "\n",
    "#     aq_model = mr.python.create_model(\n",
    "#         name=f\"air_quality_xgboost_model_{sensor_id}\",\n",
    "#         metrics={\n",
    "#             \"R2\": best_model_r2,\n",
    "#             \"MSE\": best_model_mse,\n",
    "#         },\n",
    "#         feature_view=best_model_feature_view,\n",
    "#         description=f\"Air Quality (PM2.5) predictor for {city} {street} using {best_model_name} configuration\",\n",
    "#     )\n",
    "\n",
    "#     aq_model.save(f\"{model_dir}/{sensor_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce5863",
   "metadata": {},
   "source": [
    "## 3.11. Upload visuals to HopsFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4401252",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "base_dir = \"Resources/plots\"\n",
    "try:\n",
    "    dataset_api.mkdir(base_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "uploaded_images = 0\n",
    "\n",
    "for sensor_id in sensor_locations.keys():\n",
    "    sensor_dir = f\"{base_dir}/{sensor_id}\"\n",
    "    try:\n",
    "        dataset_api.mkdir(sensor_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    local_path = f\"{model_dir}/{sensor_id}/images/hindcast_training.png\"\n",
    "    remote_path = f\"{sensor_dir}/hindcast_training.png\"\n",
    "\n",
    "    ok = hopsworks_admin.safe_upload(dataset_api, local_path, remote_path)\n",
    "    if ok:\n",
    "        uploaded_images += 1\n",
    "        print(f\"Uploaded image for sensor {sensor_id} ({uploaded_images}/{len(sensor_locations)})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Giving up on sensor {sensor_id} after repeated failures\")\n",
    "\n",
    "    # dataset_api.upload( \n",
    "    #     local_path, remote_path, overwrite=True\n",
    "    # )\n",
    "    # uploaded_images = uploaded_images + 1\n",
    "    # print(f\"Uploaded image for sensor {sensor_id}, number {uploaded_images} / {len(sensor_locations)}\")\n",
    "\n",
    "print(f\"Done uploading {uploaded_images} images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
