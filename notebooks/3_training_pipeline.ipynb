{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e975c5",
   "metadata": {},
   "source": [
    "# 3. Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b6067",
   "metadata": {},
   "source": [
    "## 3.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73a5f2",
   "metadata": {},
   "source": [
    "### 3.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9090d59",
   "metadata": {},
   "source": [
    "### 3.1.2. Load Settings and Initialize Hopsworks Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76421c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad062b7b",
   "metadata": {},
   "source": [
    "### 3.1.3. Repository Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e887f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63871bc",
   "metadata": {},
   "source": [
    "### 3.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cfb86",
   "metadata": {},
   "source": [
    "### 3.1.5. Get Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8b26b",
   "metadata": {},
   "source": [
    "## 3.2. Load Feature Groups & Sensor Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)\n",
    "\n",
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dec32e",
   "metadata": {},
   "source": [
    "## 3.3. Create Additional Feature Views\n",
    "Create multiple feature views with different feature combinations (baseline, rolling windows, lagged features, nearby sensors, complete) for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "baseline_features = air_quality_fg.select([\"pm25\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "\n",
    "baseline_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_baseline_fv\",\n",
    "    description=\"Weather features for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=baseline_features,\n",
    ")\n",
    "\n",
    "rolling_features = air_quality_fg.select([\"pm25\", \"pm25_rolling_3d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "rolling_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_rolling_fv\",\n",
    "    description=\"Weather features, PM2.5 rolling window (3d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=rolling_features,\n",
    ")\n",
    "\n",
    "nearby_features = air_quality_fg.select([\"pm25\", \"pm25_nearby_avg\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "nearby_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_nearby_fv\",\n",
    "    description=\"Weather features, PM2.5 nearby average (1d lag, 3 sensors) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=nearby_features,\n",
    ")\n",
    "\n",
    "lagged_1d_features = air_quality_fg.select([\"pm25\", \"pm25_lag_1d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "lagged_1d_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_lagged_1d_fv\",\n",
    "    description=\"Weather features, PM2.5 lags (1d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=lagged_1d_features,\n",
    ")\n",
    "\n",
    "lagged_2d_features = air_quality_fg.select([\"pm25\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "lagged_2d_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_lagged_2d_fv\",\n",
    "    description=\"Weather features, PM2.5 lags (1d, 2d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=lagged_2d_features,\n",
    ")\n",
    "\n",
    "lagged_3d_features = air_quality_fg.select([\"pm25\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "lagged_3d_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_lagged_3d_fv\",\n",
    "    description=\"Weather features, PM2.5 lags (1d, 2d, 3d) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=lagged_3d_features,\n",
    ")\n",
    "\n",
    "complete_features = air_quality_fg.select([\"pm25\", \"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\", \"pm25_nearby_avg\", \"date\", \"sensor_id\"]).join(\n",
    "    weather_fg.select_features(), on=[\"sensor_id\", \"date\"])\n",
    "complete_feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"air_quality_complete_fv\",\n",
    "    description=\"Weather features, PM2.5 rolling window (3d), and PM2.5 lags (1d, 2d, 3d), and PM2.5 nearby average (1d lag, 3 sensors) for PM2.5 prediction\",\n",
    "    version=1,\n",
    "    labels=[\"pm25\"],\n",
    "    query=complete_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b046b",
   "metadata": {},
   "source": [
    "## 3.4. Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafed42d",
   "metadata": {},
   "source": [
    "### 3.4.1. Define Training Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"{root_dir}/models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "print(f\"Model directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2c15d",
   "metadata": {},
   "source": [
    "### 3.4.2. Define Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92116b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"pm25\"\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "MIN_ROWS = 10\n",
    "MIN_TEST_ROWS = 2\n",
    "\n",
    "EXCLUDE_COLS = [\n",
    "    \"pm25\",\"date\",\"sensor_id\",\"city\",\"street\",\"country\",\n",
    "    \"latitude\",\"longitude\",\"aqicn_url\"\n",
    "]\n",
    "\n",
    "N_RESTARTS = 5\n",
    "BASE_SEED = 165439\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acaf657",
   "metadata": {},
   "source": [
    "### 3.4.3. Initialize Containers for Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare containers to store models, predictions, and results\n",
    "models = defaultdict(dict)\n",
    "y_preds = defaultdict(dict)\n",
    "results = []\n",
    "\n",
    "# Define feature view dictionary for iteration\n",
    "feature_views = {\n",
    "    \"baseline\": baseline_feature_view,\n",
    "    \"rolling\": rolling_feature_view,\n",
    "    \"nearby\": nearby_feature_view,\n",
    "    \"lagged_1d\": lagged_1d_feature_view,\n",
    "    \"lagged_2d\": lagged_2d_feature_view,\n",
    "    \"lagged_3d\": lagged_3d_feature_view,\n",
    "    \"complete\": complete_feature_view,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9685e4",
   "metadata": {},
   "source": [
    "## 3.5. Training Loop\n",
    "Train XGBoost models for each feature combination and sensor, run 5 iterations per configuration, select best model based on R2 score, and store results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c78c5",
   "metadata": {},
   "source": [
    "### 3.5.1. Load Feature View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_cache = {}\n",
    "total_views = len(feature_views)\n",
    "\n",
    "for i, (feature_name, feature_view) in enumerate(feature_views.items(), start=1):\n",
    "    print(f\"Reading ({i}/{total_views}): {feature_name}\")\n",
    "\n",
    "    df = feature_view.query.read()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "    feature_data_cache[feature_name] = df\n",
    "\n",
    "    print(f\"    ‚úî Loaded {len(df):,} rows\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded all {total_views} feature views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9a53a",
   "metadata": {},
   "source": [
    "### 3.5.2. Build Task List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c889c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "total_views = len(feature_data_cache)\n",
    "\n",
    "print(f\"Building task list from {total_views} feature views:\")\n",
    "\n",
    "for i, (feature_name, df) in enumerate(feature_data_cache.items(), start=1):\n",
    "    sensor_ids = df[\"sensor_id\"].unique()\n",
    "    count = len(sensor_ids)\n",
    "\n",
    "    print(f\"[{i}/{total_views}] {feature_name}: {count} sensors\")\n",
    "\n",
    "    for sensor_id in sensor_ids:\n",
    "        tasks.append((feature_name, sensor_id))\n",
    "\n",
    "print(f\"\\n‚úÖ {len(tasks):,} total training tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97c416",
   "metadata": {},
   "source": [
    "### 3.5.3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(tasks)\n",
    "\n",
    "for idx, (feature_name, sensor_id) in enumerate(tasks, start=1):\n",
    "    df = feature_data_cache[feature_name]\n",
    "    df = df[df[\"sensor_id\"] == sensor_id].dropna(subset=[TARGET])\n",
    "\n",
    "    if len(df) < MIN_ROWS:\n",
    "        continue\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "    train_size = int(TRAIN_RATIO * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    if len(test_df) < MIN_TEST_ROWS:\n",
    "        continue\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[TARGET]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[TARGET]\n",
    "\n",
    "    best_model = None\n",
    "    best_r2 = -1e9\n",
    "    best_mse = 1e9\n",
    "\n",
    "    for i in range(N_RESTARTS):\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=xgb_params[\"n_estimators\"],\n",
    "            learning_rate=xgb_params[\"learning_rate\"],\n",
    "            random_state=BASE_SEED*i\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_mse = mse\n",
    "            best_model = model\n",
    "\n",
    "    # Store results\n",
    "    models.setdefault(feature_name, {})[sensor_id] = best_model\n",
    "    y_preds.setdefault(feature_name, {})[sensor_id] = best_model.predict(X_test)\n",
    "    results.append({\n",
    "        \"feature_name\": feature_name,\n",
    "        \"sensor_id\": sensor_id,\n",
    "        \"R2\": best_r2,\n",
    "        \"MSE\": best_mse,\n",
    "        \"train_size\": len(train_df),\n",
    "        \"test_size\": len(test_df),\n",
    "    })\n",
    "\n",
    "    if idx % 10 == 0 or idx == total:\n",
    "        print(f\"[{idx}/{total}] Trained {feature_name} / sensor {sensor_id}: R¬≤={best_r2:.3f}, MSE={best_mse:.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete: {len(results)} models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f965ae3",
   "metadata": {},
   "source": [
    "## 3.6. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcfbdd",
   "metadata": {},
   "source": [
    "### 3.6.1. Identify Best Model per Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4410cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_models = results_df.loc[results_df.groupby(\"sensor_id\")[\"R2\"].idxmax()]\n",
    "best_models = best_models.set_index(\"sensor_id\")\n",
    "\n",
    "print(f\"Identified best models for {len(best_models)} sensors:\\n\")\n",
    "\n",
    "summary = best_models[[\"feature_name\", \"R2\", \"MSE\"]].sort_index()\n",
    "\n",
    "print(summary.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e98a0",
   "metadata": {},
   "source": [
    "### 3.6.2. Load Feature View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_views = len(feature_views)\n",
    "print(f\"Loading {total_views} feature views...\\n\")\n",
    "\n",
    "cached_feature_data = {}\n",
    "\n",
    "for i, (name, fv) in enumerate(feature_views.items(), start=1):\n",
    "    print(f\"[{i}/{total_views}] Reading feature view: {name}...\")\n",
    "    df_cached = fv.query.read()\n",
    "    cached_feature_data[name] = df_cached\n",
    "    print(f\"    ‚úî Loaded {len(df_cached):,} rows\")\n",
    "\n",
    "print(\"\\nNormalizing date columns...\\n\")\n",
    "\n",
    "for name, df_cached in cached_feature_data.items():\n",
    "    df_cached[\"date\"] = pd.to_datetime(df_cached[\"date\"]).dt.tz_localize(None)\n",
    "    print(f\"    ‚úî Normalized dates for '{name}'\")\n",
    "\n",
    "print(\"\\nAll feature views loaded and normalized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204ea1b",
   "metadata": {},
   "source": [
    "### 3.9.3. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "total_sensors = len(best_models)\n",
    "\n",
    "print(f\"Processing {total_sensors} sensors...\\n\")\n",
    "\n",
    "for idx, (_, row) in enumerate(best_models.iterrows(), start=1):\n",
    "    sensor_id = row.name\n",
    "    best_feature = row[\"feature_name\"]\n",
    "\n",
    "    status = []  # collect short status flags\n",
    "\n",
    "    # --- Save model + feature importance ---\n",
    "    sensor_dir = f\"{model_dir}/{sensor_id}\"\n",
    "    images_dir = f\"{sensor_dir}/images\"\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    best_model = models[best_feature][sensor_id]\n",
    "    best_model.save_model(f\"{sensor_dir}/model.json\")\n",
    "    plot_importance(best_model)\n",
    "    plt.savefig(f\"{images_dir}/feature_importance.png\")\n",
    "    plt.close()\n",
    "    status.append(\"model+plot\")\n",
    "\n",
    "    # --- Load cached feature view data ---\n",
    "    df = cached_feature_data[best_feature]\n",
    "    df = df[df[\"sensor_id\"] == sensor_id].copy()\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    df = df.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    feature_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in [\"pm25\", \"date\", \"sensor_id\", \"city\", \"street\",\n",
    "                     \"country\", \"latitude\", \"longitude\", \"aqicn_url\"]\n",
    "    ]\n",
    "\n",
    "    df_clean = df.copy()\n",
    "    if len(df_clean) < 10:\n",
    "        print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: skipped (too few rows)\")\n",
    "        continue\n",
    "\n",
    "    # --- Full predictions ---\n",
    "    X_full = df_clean[feature_cols]\n",
    "    df_clean[\"predicted_pm25\"] = best_model.predict(X_full)\n",
    "    df_clean[\"best_model\"] = best_feature\n",
    "    status.append(\"predictions\")\n",
    "\n",
    "    # --- Hindcast window ---\n",
    "    cutoff_date = pd.Timestamp.now() - pd.DateOffset(months=18)\n",
    "    df_hindcast = df_clean[df_clean[\"date\"] >= cutoff_date].copy()\n",
    "\n",
    "    if len(df_hindcast) == 0:\n",
    "        print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: skipped (no recent data)\")\n",
    "        continue\n",
    "    status.append(\"hindcast\")\n",
    "  \n",
    "    # --- Metadata ---\n",
    "    if sensor_id in sensor_locations:\n",
    "        lat, lon, city, street, country = sensor_locations[sensor_id]\n",
    "        df_clean[\"latitude\"] = lat\n",
    "        df_clean[\"longitude\"] = lon\n",
    "        df_clean[\"city\"] = city\n",
    "        df_clean[\"street\"] = street\n",
    "        df_clean[\"sensor_id\"] = sensor_id\n",
    "        status.append(\"metadata\")\n",
    "    else:\n",
    "        print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: skipped (no metadata)\")\n",
    "        continue\n",
    "\n",
    "    # --- Append final data ---\n",
    "    all_test_data.append(\n",
    "        df_clean[[\n",
    "            \"date\", \"sensor_id\", \"pm25\", \"predicted_pm25\",\n",
    "            \"latitude\", \"longitude\", \"city\", \"street\", \"best_model\"\n",
    "        ]]\n",
    "    )\n",
    "    status.append(\"saved\")\n",
    "\n",
    "    # --- One-line summary ---\n",
    "    print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: \" + \", \".join(status))\n",
    "\n",
    "print(f\"\\nDone. Successfully processed {len(all_test_data)} sensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd65c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DIAGNOSTIC: Check data coverage by year ---\n",
    "print(\"\\nüìä DATA COVERAGE DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature_name, df_cached in cached_feature_data.items():\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    \n",
    "    # Check one sensor as example\n",
    "    sample_sensor = df_cached[\"sensor_id\"].iloc[0]\n",
    "    sensor_data = df_cached[df_cached[\"sensor_id\"] == sample_sensor].copy()\n",
    "    sensor_data[\"year\"] = pd.to_datetime(sensor_data[\"date\"]).dt.year\n",
    "    \n",
    "    print(f\"  Sensor {sample_sensor}:\")\n",
    "    print(f\"  Total rows: {len(sensor_data)}\")\n",
    "    print(f\"  Date range: {sensor_data['date'].min()} to {sensor_data['date'].max()}\")\n",
    "    \n",
    "    # Check pm25 by year\n",
    "    yearly_stats = sensor_data.groupby(\"year\").agg({\n",
    "        \"pm25\": [\"count\", lambda x: x.isna().sum()]\n",
    "    })\n",
    "    yearly_stats.columns = [\"Total Rows\", \"NaN Count\"]\n",
    "    yearly_stats[\"Non-NaN\"] = yearly_stats[\"Total Rows\"] - yearly_stats[\"NaN Count\"]\n",
    "    print(\"\\n  PM2.5 availability by year:\")\n",
    "    print(yearly_stats)\n",
    "    \n",
    "    # After dropna\n",
    "    sensor_data_clean = sensor_data.dropna(subset=[\"pm25\"])\n",
    "    if len(sensor_data_clean) > 0:\n",
    "        print(f\"\\n  After dropna(subset=['pm25']):\")\n",
    "        print(f\"    Remaining rows: {len(sensor_data_clean)}\")\n",
    "        print(f\"    Date range: {sensor_data_clean['date'].min()} to {sensor_data_clean['date'].max()}\")\n",
    "        print(f\"    Years present: {sorted(sensor_data_clean['year'].unique())}\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è NO DATA LEFT after dropna!\")\n",
    "    \n",
    "    break  # Only check first feature view for now\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada9766",
   "metadata": {},
   "source": [
    "### 3.6.3. Preparation for Visualization and Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13676cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(all_test_data, ignore_index=True) if all_test_data else pd.DataFrame()\n",
    "df = df.sort_values(by=[\"date\"])\n",
    "df_by_sensor = {sid: g.copy() for sid, g in df.groupby(\"sensor_id\")}\n",
    "\n",
    "HINDCAST_MONTHS = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3a6f0",
   "metadata": {},
   "source": [
    "## 3.7. Visualization\n",
    "Generate Plots and Hindcats Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7cc4f",
   "metadata": {},
   "source": [
    "### 3.7.1. Insert Predictions to Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get monitoring feature group\n",
    "monitor_fg = fs.get_or_create_feature_group(\n",
    "    name=\"aq_predictions\",\n",
    "    description=\"Air Quality prediction monitoring from training\",\n",
    "    version=1,\n",
    "    primary_key=[\"sensor_id\", \"date\", \"days_before_forecast_day\"],\n",
    "    event_time=\"date\",\n",
    ")\n",
    "\n",
    "monitoring_predictions = []\n",
    "\n",
    "print(f\"\\nPreparing predictions with features for {len(all_test_data)} sensors...\\n\")\n",
    "\n",
    "for sensor_df in all_test_data:\n",
    "    if sensor_df.empty:\n",
    "        continue\n",
    "    \n",
    "    sensor_id = sensor_df[\"sensor_id\"].iloc[0]\n",
    "    best_feature = sensor_df[\"best_model\"].iloc[0]\n",
    "    \n",
    "    # Get full feature data for this sensor\n",
    "    full_df = cached_feature_data[best_feature]\n",
    "    full_df = full_df[full_df[\"sensor_id\"] == sensor_id].copy()\n",
    "    full_df = full_df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    # Get hindcast window\n",
    "    cutoff_date = pd.Timestamp.now() - pd.DateOffset(months=HINDCAST_MONTHS)\n",
    "    hindcast_df = full_df[full_df[\"date\"] >= cutoff_date].copy()\n",
    "    \n",
    "    if hindcast_df.empty:\n",
    "        continue\n",
    "    \n",
    "    # Get predictions from sensor_df\n",
    "    predictions_merged = hindcast_df.merge(\n",
    "        sensor_df[[\"date\", \"predicted_pm25\"]], \n",
    "        on=\"date\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Prepare monitoring record with engineered features\n",
    "    # Use .get() method which returns the column if it exists, otherwise creates a Series of NaN\n",
    "    pred_df = predictions_merged[[\"date\", \"predicted_pm25\"]].copy()\n",
    "    pred_df[\"sensor_id\"] = sensor_id\n",
    "    pred_df[\"days_before_forecast_day\"] = 0.0  # 0 for training/hindcast\n",
    "    \n",
    "    # Add engineered features if they exist in the source data\n",
    "    pred_df[\"predicted_pm25_rolling_3d\"] = predictions_merged.get(\"pm25_rolling_3d\", pd.Series([np.nan] * len(predictions_merged)))\n",
    "    pred_df[\"predicted_pm25_lag_1d\"] = predictions_merged.get(\"pm25_lag_1d\", pd.Series([np.nan] * len(predictions_merged)))\n",
    "    pred_df[\"predicted_pm25_lag_2d\"] = predictions_merged.get(\"pm25_lag_2d\", pd.Series([np.nan] * len(predictions_merged)))\n",
    "    pred_df[\"predicted_pm25_lag_3d\"] = predictions_merged.get(\"pm25_lag_3d\", pd.Series([np.nan] * len(predictions_merged)))\n",
    "    pred_df[\"predicted_pm25_nearby_avg\"] = predictions_merged.get(\"pm25_nearby_avg\", pd.Series([np.nan] * len(predictions_merged)))\n",
    "    \n",
    "    monitoring_predictions.append(pred_df)\n",
    "    # print(f\"  Prepared {len(pred_df)} predictions for sensor {sensor_id}\")\n",
    "print(f\"Prepared {len(monitoring_predictions)} sensors' predictions.\")\n",
    "\n",
    "# Combine all predictions\n",
    "if monitoring_predictions:\n",
    "    all_monitoring_df = pd.concat(monitoring_predictions, ignore_index=True)\n",
    "    \n",
    "    # Ensure correct data types to match feature group schema\n",
    "    all_monitoring_df[\"date\"] = pd.to_datetime(all_monitoring_df[\"date\"])\n",
    "    all_monitoring_df[\"sensor_id\"] = all_monitoring_df[\"sensor_id\"].astype(int)\n",
    "    all_monitoring_df[\"predicted_pm25\"] = all_monitoring_df[\"predicted_pm25\"].astype(float)\n",
    "    all_monitoring_df[\"days_before_forecast_day\"] = all_monitoring_df[\"days_before_forecast_day\"].astype(float)\n",
    "    \n",
    "    # Also convert the predicted feature columns to float\n",
    "    for col in [\"predicted_pm25_rolling_3d\", \"predicted_pm25_lag_1d\", \n",
    "                \"predicted_pm25_lag_2d\", \"predicted_pm25_lag_3d\", \"predicted_pm25_nearby_avg\"]:\n",
    "        if col in all_monitoring_df.columns:\n",
    "            all_monitoring_df[col] = all_monitoring_df[col].astype(float)\n",
    "    \n",
    "    print(f\"\\nüìä Total predictions to insert: {len(all_monitoring_df)}\")\n",
    "    # print(f\"\\nSample data:\")\n",
    "    # print(all_monitoring_df.head())\n",
    "    # print(f\"\\nData types:\")\n",
    "    # print(all_monitoring_df.dtypes)\n",
    "    \n",
    "    # Insert into feature group with retry logic\n",
    "    print(f\"\\nInserting {len(all_monitoring_df)} predictions into aq_predictions feature group...\")\n",
    "\n",
    "    BATCH_SIZE = 2000\n",
    "    MAX_RETRIES = 5\n",
    "    INITIAL_BACKOFF = 2  # seconds\n",
    "\n",
    "    num_batches = (len(all_monitoring_df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    successful_batches = 0\n",
    "    failed_batches = []\n",
    "\n",
    "    for batch_idx, start in enumerate(range(0, len(all_monitoring_df), BATCH_SIZE), start=1):\n",
    "        end = min(start + BATCH_SIZE, len(all_monitoring_df))\n",
    "        batch = all_monitoring_df.iloc[start:end]\n",
    "        \n",
    "        success = False\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                monitor_fg.insert(batch, write_options={\"wait_for_job\": False})\n",
    "                success = True\n",
    "                successful_batches += 1\n",
    "                print(f\"‚úÖ Batch {batch_idx}/{num_batches} [{start}:{end}] inserted successfully\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, Timeout, RequestException) as e:\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    backoff_time = INITIAL_BACKOFF * (2 ** (attempt - 1))  # Exponential backoff\n",
    "                    print(f\"‚ö†Ô∏è Batch {batch_idx}/{num_batches} attempt {attempt}/{MAX_RETRIES} failed: {type(e).__name__}\")\n",
    "                    print(f\"   Retrying in {backoff_time}s...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Batch {batch_idx}/{num_batches} FAILED after {MAX_RETRIES} attempts\")\n",
    "                    failed_batches.append((batch_idx, start, end))\n",
    "        \n",
    "        # Small delay between batches to avoid overwhelming the server\n",
    "        if success and batch_idx < num_batches:\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ Successfully inserted {successful_batches}/{num_batches} batches\")\n",
    "    \n",
    "    if failed_batches:\n",
    "        print(f\"‚ùå Failed batches: {len(failed_batches)}\")\n",
    "        for batch_idx, start, end in failed_batches:\n",
    "            print(f\"   Batch {batch_idx}: rows {start}-{end}\")\n",
    "    \n",
    "    # Verify insertion\n",
    "    print(\"\\nüîç Verifying insertion...\")\n",
    "    time.sleep(5)  # Wait longer for materialization\n",
    "    \n",
    "    verification_df = monitor_fg.read()\n",
    "    print(f\"‚úÖ Feature group now contains {len(verification_df)} total rows\")\n",
    "    print(f\"   Training predictions prepared: {len(all_monitoring_df)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No predictions to insert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "total_sensors = len(best_models)\n",
    "\n",
    "print(f\"Generating visualizations for {total_sensors} sensors...\\n\")\n",
    "\n",
    "for idx, (_, row) in enumerate(best_models.iterrows(), start=1):\n",
    "    sensor_id = row.name\n",
    "    best_feature = row[\"feature_name\"]\n",
    "\n",
    "    status = []\n",
    "\n",
    "    # Load cached feature view data\n",
    "    df = cached_feature_data[best_feature]\n",
    "    df = df[df[\"sensor_id\"] == sensor_id].copy()\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    df = df.dropna(subset=[TARGET])\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "    if len(df) < MIN_ROWS:\n",
    "        print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: skipped (too few rows)\")\n",
    "        continue\n",
    "\n",
    "    # Generate predictions for full dataset\n",
    "    model_obj = models[best_feature][sensor_id]\n",
    "    df[\"predicted_pm25\"] = model_obj.predict(df[feature_cols])\n",
    "    df[\"best_model\"] = best_feature\n",
    "    status.append(\"predictions\")\n",
    "\n",
    "    # Hindcast window (last N months)\n",
    "    cutoff_date = pd.Timestamp.now() - pd.DateOffset(months=HINDCAST_MONTHS)\n",
    "    df_hindcast = df[df[\"date\"] >= cutoff_date].copy()\n",
    "\n",
    "    if df_hindcast.empty:\n",
    "        print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: skipped (no recent data)\")\n",
    "        continue\n",
    "    status.append(\"hindcast\")\n",
    "\n",
    "    # Attach metadata\n",
    "    if sensor_id in sensor_locations:\n",
    "        lat, lon, city, street, country = sensor_locations[sensor_id]\n",
    "        df[\"latitude\"] = lat\n",
    "        df[\"longitude\"] = lon\n",
    "        df[\"city\"] = city\n",
    "        df[\"street\"] = street\n",
    "        status.append(\"metadata\")\n",
    "    else:\n",
    "        print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: skipped (no metadata)\")\n",
    "        continue\n",
    "\n",
    "    # Save plots\n",
    "    sensor_dir = f\"{model_dir}/{sensor_id}\"\n",
    "    images_dir = f\"{sensor_dir}/images\"\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    # Feature importance plot\n",
    "    plot_importance(model_obj)\n",
    "    plt.savefig(f\"{images_dir}/feature_importance.png\")\n",
    "    plt.close()\n",
    "    status.append(\"feature_importance\")\n",
    "\n",
    "    # Hindcast plot\n",
    "    fig = visualization.plot_air_quality_forecast(\n",
    "        df[\"city\"].iloc[0],\n",
    "        df[\"street\"].iloc[0],\n",
    "        df_hindcast,\n",
    "        f\"{images_dir}/hindcast_training.png\",\n",
    "        hindcast=True\n",
    "    )\n",
    "    if fig is not None:\n",
    "        fig.suptitle(f\"{df['city'].iloc[0]} {df['street'].iloc[0]} (Best Model: {best_feature})\")\n",
    "        plt.close(fig)\n",
    "    status.append(\"hindcast_plot\")\n",
    "\n",
    "    # Append data\n",
    "    all_test_data.append(\n",
    "        df[[\n",
    "            \"date\", \"sensor_id\", \"pm25\", \"predicted_pm25\",\n",
    "            \"latitude\", \"longitude\", \"city\", \"street\", \"best_model\"\n",
    "        ]]\n",
    "    )\n",
    "    status.append(\"saved\")\n",
    "\n",
    "    print(f\"[{idx}/{total_sensors}] Sensor {sensor_id}: \" + \", \".join(status))\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization complete: {len(all_test_data)} sensors processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e31b22",
   "metadata": {},
   "source": [
    "## 3.8. Model Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807c69d",
   "metadata": {},
   "source": [
    "### 3.8.1. Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3098bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training datasets for each feature view\n",
    "training_datasets = {}\n",
    "\n",
    "MAX_RETRIES = 10  # Increased for quota issues\n",
    "INITIAL_BACKOFF = 3\n",
    "QUOTA_WAIT = 60  # Wait 60 seconds when hitting quota limits\n",
    "\n",
    "print(f\"Creating training datasets for {len(feature_views)} feature views...\\n\")\n",
    "\n",
    "for idx, (feature_name, fv) in enumerate(feature_views.items(), start=1):\n",
    "    success = False\n",
    "    \n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            # Create a training dataset version\n",
    "            version, job = fv.create_training_data(\n",
    "                description=f\"Training data for {feature_name} features\",\n",
    "                data_format=\"csv\",\n",
    "                write_options={\"wait_for_job\": True}\n",
    "            )\n",
    "            training_datasets[feature_name] = version\n",
    "            print(f\"‚úÖ [{idx}/{len(feature_views)}] {feature_name}: created training dataset v{version}\")\n",
    "            success = True\n",
    "            break\n",
    "            \n",
    "        except RestAPIError as e:\n",
    "            # Handle quota limits and other API errors\n",
    "            error_msg = str(e).lower()\n",
    "            if \"quota\" in error_msg or \"parallel executions\" in error_msg:\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    print(f\"‚ö†Ô∏è {feature_name}: attempt {attempt}/{MAX_RETRIES} - quota limit reached\")\n",
    "                    print(f\"   Waiting {QUOTA_WAIT}s for jobs to complete...\")\n",
    "                    time.sleep(QUOTA_WAIT)\n",
    "                else:\n",
    "                    print(f\"‚ùå {feature_name}: FAILED after {MAX_RETRIES} attempts (quota)\")\n",
    "                    raise\n",
    "            else:\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    backoff_time = INITIAL_BACKOFF * (2 ** (attempt - 1))\n",
    "                    print(f\"‚ö†Ô∏è {feature_name}: attempt {attempt}/{MAX_RETRIES} failed (RestAPIError)\")\n",
    "                    print(f\"   Error: {str(e)[:150]}\")\n",
    "                    print(f\"   Retrying in {backoff_time}s...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå {feature_name}: FAILED after {MAX_RETRIES} attempts\")\n",
    "                    raise\n",
    "            \n",
    "        except (ProtocolError, ConnectionError, Timeout, RequestException, OSError) as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                backoff_time = INITIAL_BACKOFF * (2 ** (attempt - 1))\n",
    "                print(f\"‚ö†Ô∏è {feature_name}: attempt {attempt}/{MAX_RETRIES} failed ({type(e).__name__}: {str(e)[:100]})\")\n",
    "                print(f\"   Retrying in {backoff_time}s...\")\n",
    "                time.sleep(backoff_time)\n",
    "            else:\n",
    "                print(f\"‚ùå {feature_name}: FAILED after {MAX_RETRIES} attempts\")\n",
    "                raise\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Catch-all for unexpected errors\n",
    "            if attempt < MAX_RETRIES:\n",
    "                backoff_time = INITIAL_BACKOFF * (2 ** (attempt - 1))\n",
    "                print(f\"‚ö†Ô∏è {feature_name}: attempt {attempt}/{MAX_RETRIES} failed with unexpected error\")\n",
    "                print(f\"   Error: {type(e).__name__}: {str(e)[:100]}\")\n",
    "                print(f\"   Retrying in {backoff_time}s...\")\n",
    "                time.sleep(backoff_time)\n",
    "            else:\n",
    "                print(f\"‚ùå {feature_name}: FAILED after {MAX_RETRIES} attempts with {type(e).__name__}\")\n",
    "                raise\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"‚ùå Could not create training dataset for {feature_name}\")\n",
    "        raise RuntimeError(f\"Failed to create training dataset for {feature_name}\")\n",
    "    \n",
    "    # Small delay between feature views\n",
    "    if idx < len(feature_views):\n",
    "        time.sleep(2)\n",
    "\n",
    "print(f\"\\n‚úÖ All {len(training_datasets)} training datasets created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5eef00",
   "metadata": {},
   "source": [
    "### 3.8.2. Register Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70aaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = 0\n",
    "total_sensors = len(best_models)\n",
    "\n",
    "print(f\"Registering models for {total_sensors} sensors...\\n\")\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "INITIAL_BACKOFF = 2\n",
    "\n",
    "for sensor_id, row in best_models.iterrows():\n",
    "\n",
    "    best_feature = row[\"feature_name\"]\n",
    "    best_r2 = row[\"R2\"]\n",
    "    best_mse = row[\"MSE\"]\n",
    "\n",
    "    # Get trained model\n",
    "    model_obj = models[best_feature][sensor_id]\n",
    "\n",
    "    # Save model locally\n",
    "    sensor_model_dir = f\"{model_dir}/{sensor_id}\"\n",
    "    os.makedirs(sensor_model_dir, exist_ok=True)\n",
    "    model_obj.save_model(f\"{sensor_model_dir}/model.json\")\n",
    "\n",
    "    # Register model\n",
    "    model = mr.python.create_model(\n",
    "        name=f\"air_quality_xgboost_model_{sensor_id}\",\n",
    "        metrics={\"R2\": best_r2, \"MSE\": best_mse},\n",
    "        feature_view=feature_views[best_feature],\n",
    "        training_dataset_version=training_datasets[best_feature],\n",
    "        description=f\"PM2.5 predictor for sensor {sensor_id} using {best_feature} features\",\n",
    "    )\n",
    "\n",
    "    success = False\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            model.save(sensor_model_dir)\n",
    "            success = True\n",
    "            break\n",
    "        except (ProtocolError, ConnectionError, Timeout, RequestException) as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                backoff_time = INITIAL_BACKOFF * (2 ** (attempt - 1))\n",
    "                print(f\"  ‚ö†Ô∏è Sensor {sensor_id}: attempt {attempt}/{MAX_RETRIES} failed ({type(e).__name__})\")\n",
    "                print(f\"     Retrying in {backoff_time}s...\")\n",
    "                time.sleep(backoff_time)\n",
    "            else:\n",
    "                print(f\"  ‚ùå Sensor {sensor_id}: FAILED after {MAX_RETRIES} attempts\")\n",
    "\n",
    "    if success:\n",
    "        uploaded += 1\n",
    "        print(f\"[{uploaded}/{total_sensors}] Sensor {sensor_id}: registered ({best_feature})\")\n",
    "    else:\n",
    "        print(f\"[--/--] Sensor {sensor_id}: FAILED to register\")\n",
    "\n",
    "print(f\"\\n‚úÖ Done. {uploaded}/{total_sensors} models successfully registered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a7054",
   "metadata": {},
   "source": [
    "## 3.9. Upload Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4401252",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_api = project.get_dataset_api()\n",
    "base_dir = \"Resources/plots\"\n",
    "try:\n",
    "    dataset_api.mkdir(base_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "uploaded_images = 0\n",
    "total_sensors = len(sensor_locations)\n",
    "\n",
    "print(f\"Uploading plots for {total_sensors} sensors...\\n\")\n",
    "\n",
    "for sensor_id in sensor_locations.keys():\n",
    "    sensor_dir = f\"{base_dir}/{sensor_id}\"\n",
    "    try:\n",
    "        dataset_api.mkdir(sensor_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    local_path = f\"{model_dir}/{sensor_id}/images/hindcast_training.png\"\n",
    "    remote_path = f\"{sensor_dir}/hindcast_training.png\"\n",
    "\n",
    "    ok = hopsworks_admin.safe_upload(dataset_api, local_path, remote_path)\n",
    "\n",
    "    if ok:\n",
    "        uploaded_images += 1\n",
    "        print(f\"Uploaded image for sensor {sensor_id} ({uploaded_images}/{total_sensors})\")\n",
    "    else:\n",
    "        print(f\"‚ùå [fail] Sensor {sensor_id}: upload failed after retries\")\n",
    "\n",
    "print(f\"‚úÖ Done uploading {uploaded_images} images.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
