{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6c5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Import Libraries and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\n",
      "HopsworksSettings initialized!\n",
      "2026-01-19 11:01:13,378 INFO: Initializing external client\n",
      "2026-01-19 11:01:13,378 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-19 11:01:15,408 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "2026-01-19 11:01:17,175 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-19 11:01:17,181 INFO: Initializing external client\n",
      "2026-01-19 11:01:17,181 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-19 11:01:18,896 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError  \n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "from confluent_kafka import KafkaException\n",
    "import numpy as np\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings()\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "### 2.1.2. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists at c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa04a",
   "metadata": {},
   "source": [
    "### 2.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010e645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.3. Load Metadata from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (13.04s) \n",
      "üìç Loaded metadata for 103 sensors\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Extract unique sensor metadata\n",
    "metadata_df = aq_data[[\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\", \"aqicn_url\"]].drop_duplicates(subset=[\"sensor_id\"])\n",
    "print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "metadata_df = metadata_df.set_index(\"sensor_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.4. Data Collection\n",
    "Loop through all sensors to fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8e84",
   "metadata": {},
   "source": [
    "### 2.4.1. Initialize Processing Counters and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616e383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 103 sensor locations.\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from feature group for nearby sensor calculations\n",
    "metadata_indexed = metadata_df.copy()\n",
    "metadata_indexed.index = metadata_indexed.index.astype(int)\n",
    "\n",
    "# aq_successful = 0\n",
    "# aq_failed = 0\n",
    "# aq_skipped = 0\n",
    "\n",
    "# weather_successful = 0\n",
    "# weather_failed = 0\n",
    "# weather_skipped = 0\n",
    "\n",
    "print(f\"üîç Processing {len(metadata_indexed)} sensor locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "### 2.4.2. Load Historical Air Quality Data (Last 4 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (8.78s) \n"
     ]
    }
   ],
   "source": [
    "historical_start = today - timedelta(days=4)\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        # Include TODAY in historical data (we'll filter it out later per sensor)\n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt)  # Changed < to <=\n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(metadata_indexed.index)]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "### 2.4.3. Identify Missing Dates for Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (6.60s) \n",
      "üìÖ Missing dates to backfill: 2026-01-15, 2026-01-16, 2026-01-17, 2026-01-18, 2026-01-19\n"
     ]
    }
   ],
   "source": [
    "existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()\n",
    "\n",
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # Check last 7 days for missing data\n",
    "\n",
    "expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "missing_dates = [d for d in expected_dates if d not in existing_dates]\n",
    "\n",
    "# print(f\"üìÖ Missing dates to backfill: {missing_dates}\")\n",
    "formatted = \", \".join(d.isoformat() for d in missing_dates)\n",
    "print(f\"üìÖ Missing dates to backfill: {formatted}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f41259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize containers for results\n",
    "# aq_list = []\n",
    "# weather_dict = {}  # sensor_id -> weather_df\n",
    "\n",
    "# # Determine missing dates\n",
    "# existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()\n",
    "\n",
    "# today = datetime.today().date()\n",
    "# start_date = today - timedelta(days=7)  # or however far back you want to check\n",
    "\n",
    "# expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "# missing_dates = [d for d in expected_dates if d not in existing_dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "### 2.4.4. Prepare Historical Data Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (12.32s) \n"
     ]
    }
   ],
   "source": [
    "historical_cutoff = pd.to_datetime(min(missing_dates)) - pd.Timedelta(days=3)\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "historical = historical [historical[\"date\"] >= historical_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807bdec",
   "metadata": {},
   "source": [
    "### 2.4.5. Track Existing Sensor-Date Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c2227",
   "metadata": {},
   "source": [
    "### 2.4.6. Initialize Data Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa013b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aq_rows = [historical]\n",
    "all_weather_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "### 2.4.7. Fetch Missing Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching air quality for sensor 60853, 1/103\n",
      "Fetching air quality for sensor 59497, 2/103\n",
      "Fetching air quality for sensor 59650, 3/103\n",
      "Fetching air quality for sensor 112672, 4/103\n",
      "Fetching air quality for sensor 60889, 5/103\n",
      "Fetching air quality for sensor 60076, 6/103\n",
      "Fetching air quality for sensor 58921, 7/103\n",
      "Fetching air quality for sensor 84085, 8/103\n",
      "Fetching air quality for sensor 89584, 9/103\n",
      "Fetching air quality for sensor 198559, 10/103\n",
      "Fetching air quality for sensor 149242, 11/103\n",
      "Fetching air quality for sensor 105325, 12/103\n",
      "Fetching air quality for sensor 78529, 13/103\n",
      "Fetching air quality for sensor 88876, 14/103\n",
      "Fetching air quality for sensor 65272, 15/103\n",
      "Fetching air quality for sensor 77488, 16/103\n",
      "Fetching air quality for sensor 351115, 17/103\n",
      "Fetching air quality for sensor 122302, 18/103\n",
      "Fetching air quality for sensor 196735, 19/103\n",
      "Fetching air quality for sensor 69724, 20/103\n",
      "Fetching air quality for sensor 60859, 21/103\n",
      "Fetching air quality for sensor 65146, 22/103\n",
      "Fetching air quality for sensor 57421, 23/103\n",
      "Fetching air quality for sensor 194215, 24/103\n",
      "Fetching air quality for sensor 82384, 25/103\n",
      "Fetching air quality for sensor 180187, 26/103\n",
      "Fetching air quality for sensor 68167, 27/103\n",
      "Fetching air quality for sensor 129124, 28/103\n",
      "Fetching air quality for sensor 79999, 29/103\n",
      "Fetching air quality for sensor 59593, 30/103\n",
      "Fetching air quality for sensor 462457, 31/103\n",
      "Fetching air quality for sensor 417595, 32/103\n",
      "Fetching air quality for sensor 59410, 33/103\n",
      "Fetching air quality for sensor 249862, 34/103\n",
      "Fetching air quality for sensor 345007, 35/103\n",
      "Fetching air quality for sensor 128095, 36/103\n",
      "Fetching air quality for sensor 70564, 37/103\n",
      "Fetching air quality for sensor 63637, 38/103\n",
      "Fetching air quality for sensor 65104, 39/103\n",
      "Fetching air quality for sensor 65290, 40/103\n",
      "Fetching air quality for sensor 252352, 41/103\n",
      "Fetching air quality for sensor 60535, 42/103\n",
      "Fetching air quality for sensor 79750, 43/103\n",
      "Fetching air quality for sensor 58912, 44/103\n",
      "Fetching air quality for sensor 415030, 45/103\n",
      "Fetching air quality for sensor 65284, 46/103\n",
      "Fetching air quality for sensor 107110, 47/103\n",
      "Fetching air quality for sensor 90676, 48/103\n",
      "Fetching air quality for sensor 163156, 49/103\n",
      "Fetching air quality for sensor 59893, 50/103\n",
      "Fetching air quality for sensor 121810, 51/103\n",
      "Fetching air quality for sensor 60541, 52/103\n",
      "Fetching air quality for sensor 60886, 53/103\n",
      "Fetching air quality for sensor 77446, 54/103\n",
      "Fetching air quality for sensor 59095, 55/103\n",
      "Fetching air quality for sensor 88372, 56/103\n",
      "Fetching air quality for sensor 62566, 57/103\n",
      "Fetching air quality for sensor 494275, 58/103\n",
      "Fetching air quality for sensor 61867, 59/103\n",
      "Fetching air quality for sensor 376954, 60/103\n",
      "Fetching air quality for sensor 191047, 61/103\n",
      "Fetching air quality for sensor 59656, 62/103\n",
      "Fetching air quality for sensor 62848, 63/103\n",
      "Fetching air quality for sensor 407335, 64/103\n",
      "Fetching air quality for sensor 87319, 65/103\n",
      "Fetching air quality for sensor 420664, 66/103\n",
      "Fetching air quality for sensor 409513, 67/103\n",
      "Fetching air quality for sensor 78532, 68/103\n",
      "Fetching air quality for sensor 80773, 69/103\n",
      "Fetching air quality for sensor 250030, 70/103\n",
      "Fetching air quality for sensor 76915, 71/103\n",
      "Fetching air quality for sensor 61714, 72/103\n",
      "Fetching air quality for sensor 69628, 73/103\n",
      "Fetching air quality for sensor 476353, 74/103\n",
      "Fetching air quality for sensor 92683, 75/103\n",
      "Fetching air quality for sensor 112993, 76/103\n",
      "Fetching air quality for sensor 82942, 77/103\n",
      "Fetching air quality for sensor 58909, 78/103\n",
      "Fetching air quality for sensor 60838, 79/103\n",
      "Fetching air quality for sensor 192520, 80/103\n",
      "Fetching air quality for sensor 81505, 81/103\n",
      "Fetching air quality for sensor 65707, 82/103\n",
      "Fetching air quality for sensor 59887, 83/103\n",
      "Fetching air quality for sensor 63646, 84/103\n",
      "Fetching air quality for sensor 59356, 85/103\n",
      "Fetching air quality for sensor 60073, 86/103\n",
      "Fetching air quality for sensor 61045, 87/103\n",
      "Fetching air quality for sensor 61861, 88/103\n",
      "Fetching air quality for sensor 154549, 89/103\n",
      "Fetching air quality for sensor 61420, 90/103\n",
      "Fetching air quality for sensor 404209, 91/103\n",
      "Fetching air quality for sensor 59899, 92/103\n",
      "Fetching air quality for sensor 533086, 93/103\n",
      "Fetching air quality for sensor 113542, 94/103\n",
      "Fetching air quality for sensor 208483, 95/103\n",
      "Fetching air quality for sensor 62968, 96/103\n",
      "Fetching air quality for sensor 474841, 97/103\n",
      "Fetching air quality for sensor 113539, 98/103\n",
      "Fetching air quality for sensor 497266, 99/103\n",
      "Fetching air quality for sensor 58666, 100/103\n",
      "Fetching air quality for sensor 401314, 101/103\n",
      "Fetching air quality for sensor 562600, 102/103\n",
      "Fetching air quality for sensor 556792, 103/103\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for sensor_id, meta in metadata_df.iterrows():\n",
    "    print(f\"Fetching air quality for sensor {sensor_id}, {count}/{len(metadata_df)}\")\n",
    "    count += 1\n",
    "    for day in missing_dates:\n",
    "        # Skip any sensor date combination that already exists\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "            \n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "            continue\n",
    "\n",
    "print(f\"üìä Collected {len(all_aq_rows)} air quality dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf99689",
   "metadata": {},
   "source": [
    "### 2.4.8. Fetch Missing Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "649d753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for sensor 60853, 1/103\n",
      "Fetching weather for sensor 59497, 2/103\n",
      "Fetching weather for sensor 59650, 3/103\n",
      "Fetching weather for sensor 112672, 4/103\n",
      "2026-01-19 11:25:19,738 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=57.70716&longitude=11.96679&start_date=2026-01-16&end_date=2026-01-22&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 60889, 5/103\n",
      "Fetching weather for sensor 60076, 6/103\n",
      "Fetching weather for sensor 58921, 7/103\n",
      "Fetching weather for sensor 84085, 8/103\n",
      "Fetching weather for sensor 89584, 9/103\n",
      "Fetching weather for sensor 198559, 10/103\n",
      "Fetching weather for sensor 149242, 11/103\n",
      "Fetching weather for sensor 105325, 12/103\n",
      "Fetching weather for sensor 78529, 13/103\n",
      "Fetching weather for sensor 88876, 14/103\n",
      "Fetching weather for sensor 65272, 15/103\n",
      "Fetching weather for sensor 77488, 16/103\n",
      "Fetching weather for sensor 351115, 17/103\n",
      "2026-01-19 11:27:10,271 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=62.39129&longitude=17.3063&start_date=2026-01-19&end_date=2026-01-25&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 122302, 18/103\n",
      "Fetching weather for sensor 196735, 19/103\n",
      "2026-01-19 11:27:29,060 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.31053&longitude=18.16372&start_date=2026-01-16&end_date=2026-01-22&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 69724, 20/103\n",
      "Fetching weather for sensor 60859, 21/103\n",
      "Fetching weather for sensor 65146, 22/103\n",
      "Fetching weather for sensor 57421, 23/103\n",
      "Fetching weather for sensor 194215, 24/103\n",
      "Fetching weather for sensor 82384, 25/103\n",
      "Fetching weather for sensor 180187, 26/103\n",
      "Fetching weather for sensor 68167, 27/103\n",
      "Fetching weather for sensor 129124, 28/103\n",
      "Fetching weather for sensor 79999, 29/103\n",
      "Fetching weather for sensor 59593, 30/103\n",
      "Fetching weather for sensor 462457, 31/103\n",
      "Fetching weather for sensor 417595, 32/103\n",
      "Fetching weather for sensor 59410, 33/103\n",
      "Fetching weather for sensor 249862, 34/103\n",
      "Fetching weather for sensor 345007, 35/103\n",
      "Fetching weather for sensor 128095, 36/103\n",
      "Fetching weather for sensor 70564, 37/103\n",
      "Fetching weather for sensor 63637, 38/103\n",
      "Fetching weather for sensor 65104, 39/103\n",
      "Fetching weather for sensor 65290, 40/103\n",
      "Fetching weather for sensor 252352, 41/103\n",
      "Fetching weather for sensor 60535, 42/103\n",
      "Fetching weather for sensor 79750, 43/103\n",
      "Fetching weather for sensor 58912, 44/103\n",
      "Fetching weather for sensor 415030, 45/103\n",
      "Fetching weather for sensor 65284, 46/103\n",
      "Fetching weather for sensor 107110, 47/103\n",
      "Fetching weather for sensor 90676, 48/103\n",
      "Fetching weather for sensor 163156, 49/103\n",
      "Fetching weather for sensor 59893, 50/103\n",
      "Fetching weather for sensor 121810, 51/103\n",
      "Fetching weather for sensor 60541, 52/103\n",
      "Fetching weather for sensor 60886, 53/103\n",
      "Fetching weather for sensor 77446, 54/103\n",
      "Fetching weather for sensor 59095, 55/103\n",
      "Fetching weather for sensor 88372, 56/103\n",
      "Fetching weather for sensor 62566, 57/103\n",
      "2026-01-19 11:32:26,512 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=55.88333&longitude=12.93333&start_date=2026-01-15&end_date=2026-01-21&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 494275, 58/103\n",
      "Fetching weather for sensor 61867, 59/103\n",
      "Fetching weather for sensor 376954, 60/103\n",
      "2026-01-19 11:32:57,327 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=57.33273&longitude=18.71197&start_date=2026-01-15&end_date=2026-01-21&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 191047, 61/103\n",
      "Fetching weather for sensor 59656, 62/103\n",
      "Fetching weather for sensor 62848, 63/103\n",
      "Fetching weather for sensor 407335, 64/103\n",
      "Fetching weather for sensor 87319, 65/103\n",
      "Fetching weather for sensor 420664, 66/103\n",
      "2026-01-19 11:33:55,125 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=55.74006&longitude=13.27629&start_date=2026-01-18&end_date=2026-01-24&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 409513, 67/103\n",
      "Fetching weather for sensor 78532, 68/103\n",
      "Fetching weather for sensor 80773, 69/103\n",
      "Fetching weather for sensor 250030, 70/103\n",
      "2026-01-19 11:34:31,924 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.51839&longitude=17.91128&start_date=2026-01-17&end_date=2026-01-23&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 76915, 71/103\n",
      "Fetching weather for sensor 61714, 72/103\n",
      "Fetching weather for sensor 69628, 73/103\n",
      "Fetching weather for sensor 476353, 74/103\n",
      "Fetching weather for sensor 92683, 75/103\n",
      "Fetching weather for sensor 112993, 76/103\n",
      "Fetching weather for sensor 82942, 77/103\n",
      "Fetching weather for sensor 58909, 78/103\n",
      "Fetching weather for sensor 60838, 79/103\n",
      "Fetching weather for sensor 192520, 80/103\n",
      "Fetching weather for sensor 81505, 81/103\n",
      "Fetching weather for sensor 65707, 82/103\n",
      "Fetching weather for sensor 59887, 83/103\n",
      "Fetching weather for sensor 63646, 84/103\n",
      "Fetching weather for sensor 59356, 85/103\n",
      "Fetching weather for sensor 60073, 86/103\n",
      "Fetching weather for sensor 61045, 87/103\n",
      "Fetching weather for sensor 61861, 88/103\n",
      "Fetching weather for sensor 154549, 89/103\n",
      "Fetching weather for sensor 61420, 90/103\n",
      "Fetching weather for sensor 404209, 91/103\n",
      "Fetching weather for sensor 59899, 92/103\n",
      "Fetching weather for sensor 533086, 93/103\n",
      "Fetching weather for sensor 113542, 94/103\n",
      "Fetching weather for sensor 208483, 95/103\n",
      "Fetching weather for sensor 62968, 96/103\n",
      "2026-01-19 11:37:54,145 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=60.14959&longitude=15.18776&start_date=2026-01-16&end_date=2026-01-22&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 474841, 97/103\n",
      "Fetching weather for sensor 113539, 98/103\n",
      "Fetching weather for sensor 497266, 99/103\n",
      "Fetching weather for sensor 58666, 100/103\n",
      "2026-01-19 11:38:32,462 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.98333&longitude=17.73333&start_date=2026-01-16&end_date=2026-01-22&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 401314, 101/103\n",
      "Fetching weather for sensor 562600, 102/103\n",
      "Fetching weather for sensor 556792, 103/103\n",
      "üìä Collected 515 weather dataframes\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for sensor_id, meta in metadata_df.iterrows():\n",
    "    print(f\"Fetching weather for sensor {sensor_id}, {count}/{len(metadata_df)}\")\n",
    "    count += 1\n",
    "    \n",
    "    for day in missing_dates:\n",
    "        try:\n",
    "            # Fetch 7-day weather forecast starting from the missing date\n",
    "            weather_df = fetchers.get_weather_forecast(\n",
    "                sensor_id=sensor_id,\n",
    "                latitude=meta[\"latitude\"],\n",
    "                longitude=meta[\"longitude\"],\n",
    "                start_date=day,\n",
    "                end_date=day + timedelta(days=6)\n",
    "            )\n",
    "            \n",
    "            if weather_df.empty:\n",
    "                continue\n",
    "            \n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "            \n",
    "            all_weather_rows.append(weather_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weather for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "            continue\n",
    "\n",
    "print(f\"üìä Collected {len(all_weather_rows)} weather dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca1fa",
   "metadata": {},
   "source": [
    "### 2.4.9. Clean and Align Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Column names match: True\n",
      "üìã All dtypes match historical data.\n"
     ]
    }
   ],
   "source": [
    "cleaned_aq_rows = []\n",
    "expected_cols = historical.columns.tolist()\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(expected_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align columns\n",
    "    aligned = df.reindex(columns=expected_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(expected_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical\n",
    "    for col in expected_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "# Verify that column names and dtypes match\n",
    "print(\"üìã Column names match:\", all(df.columns.equals(historical.columns) for df in cleaned_aq_rows))\n",
    "\n",
    "no_mismatch = True\n",
    "for i, df in enumerate(cleaned_aq_rows):\n",
    "    mismatched = [(col, df[col].dtype, historical[col].dtype)\n",
    "                  for col in df.columns if col in historical.columns and df[col].dtype != historical[col].dtype]\n",
    "    if mismatched:\n",
    "        print(\"üìã Dtype mismatch:\")\n",
    "        print(f\"  df[{i}] mismatches: {mismatched}\")\n",
    "        no_mismatch = False\n",
    "if no_mismatch:\n",
    "    print(\"üìã All dtypes match historical data.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some dtypes do not match historical data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235625a",
   "metadata": {},
   "source": [
    "### 2.4.10. Combine and Clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e893b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è Total weather records: 1133\n",
      "üìÖ Weather date range: 2026-01-15 00:00:00 to 2026-01-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if all_weather_rows:\n",
    "    all_weather = pd.concat(all_weather_rows, ignore_index=True)\n",
    "    all_weather = all_weather.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "    all_weather[\"date\"] = pd.to_datetime(all_weather[\"date\"]).dt.tz_localize(None)\n",
    "    \n",
    "    # Remove duplicates (same sensor, same forecast date)\n",
    "    all_weather = all_weather.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather records: {len(all_weather)}\")\n",
    "    print(f\"üìÖ Weather date range: {all_weather['date'].min()} to {all_weather['date'].max()}\")\n",
    "else:\n",
    "    all_weather = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No weather data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565af2a",
   "metadata": {},
   "source": [
    "## 2.5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa784d",
   "metadata": {},
   "source": [
    "### 2.5.1. Combine Data and Add Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea612675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "all_aq = pd.concat([historical, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "# Add engineered features\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "metadata_indexed = metadata_indexed.reset_index()\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, metadata_indexed, n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780ad9",
   "metadata": {},
   "source": [
    "## 2.6. Insert Data to Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e42e56",
   "metadata": {},
   "source": [
    "### 2.6.1. Batch Insert Air Quality Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No valid rows for 2026-01-15\n",
      "2026-01-19 11:41:52,771 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 11:42:02,274 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 1 rows for 2026-01-16\n",
      "2026-01-19 11:42:02,608 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 11:42:11,574 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 1 rows for 2026-01-17\n",
      "2026-01-19 11:42:11,948 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 11:42:20,944 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 1 rows for 2026-01-18\n",
      "2026-01-19 11:42:21,225 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 11:42:29,514 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 1 rows for 2026-01-19\n"
     ]
    }
   ],
   "source": [
    "for day in missing_dates:\n",
    "    day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "    day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "    day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "    if not day_rows.empty:\n",
    "        # Convert types to match feature group schema\n",
    "        day_rows = day_rows.astype({\n",
    "            \"sensor_id\": \"int32\",\n",
    "            \"pm25\": \"float64\",\n",
    "            \"pm25_lag_1d\": \"float64\",\n",
    "            \"pm25_lag_2d\": \"float64\",\n",
    "            \"pm25_lag_3d\": \"float64\",\n",
    "            \"pm25_rolling_3d\": \"float64\",\n",
    "            \"pm25_nearby_avg\": \"float64\",\n",
    "            \"city\": \"string\",\n",
    "            \"street\": \"string\",\n",
    "            \"country\": \"string\",\n",
    "            \"aqicn_url\": \"string\",\n",
    "            \"latitude\": \"float64\",\n",
    "            \"longitude\": \"float64\",\n",
    "        })\n",
    "        \n",
    "        # Ensure correct column order\n",
    "        fg_columns = [f.name for f in air_quality_fg.features]\n",
    "        day_rows = day_rows[fg_columns]\n",
    "        \n",
    "        air_quality_fg.insert(day_rows)\n",
    "        print(f\"‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid rows for {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78444036",
   "metadata": {},
   "source": [
    "### 2.6.2. Verify Air Quality Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c2a0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sensor_id       date  pm25  pm25_lag_1d  pm25_rolling_3d  pm25_nearby_avg\n",
      "10       57421 2026-01-19  34.0         34.0             34.0        45.333333\n",
      "15       58666 2026-01-19   7.0          7.0              7.0              NaN\n",
      "20       58909 2026-01-19  52.0         52.0             52.0              NaN\n",
      "25       58912 2026-01-19  33.0         33.0             33.0              NaN\n",
      "30       58921 2026-01-19  14.0         14.0             14.0              NaN\n",
      "..         ...        ...   ...          ...              ...              ...\n",
      "500     494275 2026-01-19   3.0          3.0              3.0              NaN\n",
      "505     497266 2026-01-19   3.0          3.0              3.0              NaN\n",
      "510     533086 2026-01-19  52.0         52.0             52.0              NaN\n",
      "515     556792 2026-01-19  56.0         56.0             56.0              NaN\n",
      "520     562600 2026-01-19  52.0         52.0             52.0              NaN\n",
      "\n",
      "[103 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_rolling_3d\", \"pm25_nearby_avg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea2437cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sensor_id       date  pm25  pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  \\\n",
      "9        57421 2026-01-18  34.0         34.0         34.0         34.0   \n",
      "14       58666 2026-01-18   7.0          7.0          7.0          7.0   \n",
      "19       58909 2026-01-18  52.0         52.0         52.0         52.0   \n",
      "24       58912 2026-01-18  33.0         33.0         33.0         33.0   \n",
      "29       58921 2026-01-18  14.0         14.0         14.0         14.0   \n",
      "..         ...        ...   ...          ...          ...          ...   \n",
      "499     494275 2026-01-18   3.0          3.0          3.0          3.0   \n",
      "504     497266 2026-01-18   3.0          3.0          3.0          3.0   \n",
      "509     533086 2026-01-18  52.0         52.0         52.0         52.0   \n",
      "514     556792 2026-01-18  56.0         56.0         56.0         56.0   \n",
      "519     562600 2026-01-18  52.0         52.0         52.0         52.0   \n",
      "\n",
      "     pm25_rolling_3d  pm25_nearby_avg                        city  \\\n",
      "9               34.0        45.333333                 Johannehill   \n",
      "14               7.0              NaN                      √Ñngeby   \n",
      "19              52.0              NaN                       Slaka   \n",
      "24              33.0              NaN                    H√§gern√§s   \n",
      "29              14.0              NaN  Skarpn√§cks stadsdelsomr√•de   \n",
      "..               ...              ...                         ...   \n",
      "499              3.0              NaN                      Stavre   \n",
      "504              3.0              NaN                  Skellefte√•   \n",
      "509             52.0              NaN                        Berg   \n",
      "514             56.0              NaN                  Norrk√∂ping   \n",
      "519             52.0              NaN                       Solna   \n",
      "\n",
      "                 street country                            aqicn_url  \\\n",
      "9                  Ubby  Sweden   https://api.waqi.info/feed/A57421/   \n",
      "14        Jupitersv√§gen  Sweden   https://api.waqi.info/feed/A58666/   \n",
      "19        Tr√∂skaregatan  Sweden   https://api.waqi.info/feed/A58909/   \n",
      "24           Radarv√§gen  Sweden   https://api.waqi.info/feed/A58912/   \n",
      "29   Karin Larssons v√§g  Sweden   https://api.waqi.info/feed/A58921/   \n",
      "..                  ...     ...                                  ...   \n",
      "499               Z 565  Sweden  https://api.waqi.info/feed/A494275/   \n",
      "504        Mobackav√§gen  Sweden  https://api.waqi.info/feed/A497266/   \n",
      "509        Bj√∂rnsbacken  Sweden  https://api.waqi.info/feed/A533086/   \n",
      "514        Enebymov√§gen  Sweden  https://api.waqi.info/feed/A556792/   \n",
      "519      Enk√∂pingsv√§gen  Sweden  https://api.waqi.info/feed/A562600/   \n",
      "\n",
      "     latitude  longitude  \n",
      "9    62.00000   15.00000  \n",
      "14   59.98333   17.73333  \n",
      "19   58.36667   15.55000  \n",
      "24   59.75000   15.43333  \n",
      "29   62.00000   15.00000  \n",
      "..        ...        ...  \n",
      "499  63.41667   14.13333  \n",
      "504  64.75067   20.95279  \n",
      "509  62.00000   15.00000  \n",
      "514  58.59419   16.18260  \n",
      "519  59.36004   18.00086  \n",
      "\n",
      "[103 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today - timedelta(days=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "### 2.6.3. Batch Insert Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a667f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 11:42:29,757 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "‚úÖ Weather batch 1: 100 records (total: 100/1133)\n",
      "2026-01-19 11:42:44,317 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-19 11:42:53,058 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/weather_1_offline_fg_materialization/config_1768459788862) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Weather batch 2: 100 records (total: 200/1133)\n",
      "2026-01-19 11:42:53,234 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 3: 100 records (total: 300/1133)\n",
      "2026-01-19 11:43:01,789 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 4: 100 records (total: 400/1133)\n",
      "2026-01-19 11:43:09,363 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 5: 100 records (total: 500/1133)\n",
      "2026-01-19 11:43:18,150 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 6: 100 records (total: 600/1133)\n",
      "2026-01-19 11:43:25,779 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 7: 100 records (total: 700/1133)\n",
      "2026-01-19 11:43:33,169 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 8: 100 records (total: 800/1133)\n",
      "2026-01-19 11:43:40,719 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 9: 100 records (total: 900/1133)\n",
      "2026-01-19 11:43:48,220 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 10: 100 records (total: 1000/1133)\n",
      "2026-01-19 11:43:56,839 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 11: 100 records (total: 1100/1133)\n",
      "2026-01-19 11:44:05,858 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 33/33 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 12: 33 records (total: 1133/1133)\n",
      "üå§Ô∏è Total weather inserted: 1133/1133 records\n"
     ]
    }
   ],
   "source": [
    "if not all_weather.empty:\n",
    "    # Convert types to match feature group schema\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    weather_fg_columns = [f.name for f in weather_fg.features]\n",
    "    all_weather = all_weather[weather_fg_columns]\n",
    "    \n",
    "    # Insert in smaller batches to avoid connection issues\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"‚ö†Ô∏è Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather inserted: {total_inserted}/{len(all_weather)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No weather data to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845de6",
   "metadata": {},
   "source": [
    "### 2.6.4. Print Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9844",
   "metadata": {},
   "source": [
    "## 2.7. Inspect Inserted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1986667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Air quality records inserted: 521\n",
      "\n",
      "üìã Sample air quality data:\n",
      "   sensor_id       date  pm25  pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  \\\n",
      "0      57421 2026-01-12   4.0          NaN          NaN          NaN   \n",
      "1      57421 2026-01-12   4.0          4.0          NaN          NaN   \n",
      "2      57421 2026-01-13   4.0          4.0          4.0          NaN   \n",
      "3      57421 2026-01-13   4.0          4.0          4.0          4.0   \n",
      "4      57421 2026-01-14   4.0          4.0          4.0          4.0   \n",
      "\n",
      "   pm25_rolling_3d  pm25_nearby_avg         city street country  \\\n",
      "0              NaN              NaN  Johannehill   Ubby  Sweden   \n",
      "1              4.0              NaN  Johannehill   Ubby  Sweden   \n",
      "2              4.0              NaN  Johannehill   Ubby  Sweden   \n",
      "3              4.0              NaN  Johannehill   Ubby  Sweden   \n",
      "4              4.0              NaN  Johannehill   Ubby  Sweden   \n",
      "\n",
      "                            aqicn_url  latitude  longitude  \n",
      "0  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "1  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "2  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "3  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "4  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "\n",
      "üîß Air quality data types:\n",
      "sensor_id                   int32\n",
      "date               datetime64[us]\n",
      "pm25                      float64\n",
      "pm25_lag_1d               float64\n",
      "pm25_lag_2d               float64\n",
      "pm25_lag_3d               float64\n",
      "pm25_rolling_3d           float64\n",
      "pm25_nearby_avg           float64\n",
      "city                       object\n",
      "street                     object\n",
      "country                    object\n",
      "aqicn_url                  object\n",
      "latitude                  float64\n",
      "longitude                 float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Date range:\n",
      "From 2026-01-12 00:00:00 to 2026-01-19 00:00:00\n",
      "\n",
      "üå§Ô∏è Weather records inserted: 1133\n",
      "\n",
      "üìã Sample weather data:\n",
      "         date  sensor_id  temperature_2m_mean  precipitation_sum  \\\n",
      "0  2026-01-15      57421            -4.253917                0.2   \n",
      "1  2026-01-16      57421            -0.239333                3.0   \n",
      "3  2026-01-17      57421            -1.124750                0.0   \n",
      "6  2026-01-18      57421            -4.706000                0.0   \n",
      "10 2026-01-19      57421            -4.170583                0.0   \n",
      "\n",
      "    wind_speed_10m_max  wind_direction_10m_dominant  \n",
      "0            11.879999                   191.309143  \n",
      "1            16.199999                   166.159912  \n",
      "3            13.679999                   241.598740  \n",
      "6             6.480000                   207.441162  \n",
      "10            6.840000                   184.211838  \n",
      "\n",
      "üîß Weather data types:\n",
      "date                           datetime64[ns]\n",
      "sensor_id                               int32\n",
      "temperature_2m_mean                   float64\n",
      "precipitation_sum                     float64\n",
      "wind_speed_10m_max                    float64\n",
      "wind_direction_10m_dominant           float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Unique weather dates:\n",
      "<DatetimeArray>\n",
      "['2026-01-15 00:00:00', '2026-01-16 00:00:00', '2026-01-17 00:00:00',\n",
      " '2026-01-18 00:00:00', '2026-01-19 00:00:00', '2026-01-20 00:00:00',\n",
      " '2026-01-21 00:00:00', '2026-01-22 00:00:00', '2026-01-23 00:00:00',\n",
      " '2026-01-24 00:00:00', '2026-01-25 00:00:00']\n",
      "Length: 11, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "if 'all_aq' in locals() and not all_aq.empty:\n",
    "    print(f\"‚úÖ Air quality records inserted: {len(all_aq)}\")\n",
    "    print(\"\\nüìã Sample air quality data:\")\n",
    "    print(all_aq.head())\n",
    "    print(\"\\nüîß Air quality data types:\")\n",
    "    print(all_aq.dtypes)\n",
    "    print(\"\\nüìÖ Date range:\")\n",
    "    print(f\"From {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "if 'all_weather' in locals() and not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è Weather records inserted: {len(all_weather)}\")\n",
    "    print(\"\\nüìã Sample weather data:\")\n",
    "    print(all_weather.head())\n",
    "    print(\"\\nüîß Weather data types:\")\n",
    "    print(all_weather.dtypes)\n",
    "    print(\"\\nüìÖ Unique weather dates:\")\n",
    "    print(all_weather['date'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
