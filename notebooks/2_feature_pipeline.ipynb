{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6c5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f5bad",
   "metadata": {},
   "source": [
    "### 2.1.2. Load Settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d171fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "HopsworksSettings initialized!\n",
      "2026-01-28 15:10:02,655 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-28 15:10:02,660 INFO: Initializing external client\n",
      "2026-01-28 15:10:02,662 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-28 15:10:04,152 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "Environment initialized and Hopsworks connected!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "### 2.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Already in git repository at c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa04a",
   "metadata": {},
   "source": [
    "### 2.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.3. Load Sensor Locations from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (11.14s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (9.66s) \n",
      "üìç Loaded locations for 103 existing sensors\n"
     ]
    }
   ],
   "source": [
    "# Reload metadata module to pick up new functions\n",
    "import importlib\n",
    "importlib.reload(metadata)\n",
    "\n",
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.4. Data Collection\n",
    "Fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "### 2.4.1. Load Historical Air Quality Data (Last 4 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 103 sensor locations.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (10.28s) \n"
     ]
    }
   ],
   "source": [
    "print(f\"üîç Processing {len(sensor_locations)} sensor locations.\")\n",
    "historical_start = today - timedelta(days=4)\n",
    "\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt) \n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(sensor_locations.keys())]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "### 2.4.2. Identify Missing Dates for Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e64d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (9.83s) \n",
      "üìÖ Dates to fetch: 2026-01-18, 2026-01-19, 2026-01-20, 2026-01-21, 2026-01-22, 2026-01-23, 2026-01-24, 2026-01-25, 2026-01-26, 2026-01-27, 2026-01-28\n",
      "üìÖ Dates to insert: 2026-01-21, 2026-01-22, 2026-01-23, 2026-01-24, 2026-01-25, 2026-01-26, 2026-01-27, 2026-01-28\n"
     ]
    }
   ],
   "source": [
    "existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()\n",
    "\n",
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # Check last 7 days for missing data\n",
    "\n",
    "expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "original_missing_dates = [d for d in expected_dates if d not in existing_dates]\n",
    "\n",
    "# Separate: dates to fetch vs dates to insert\n",
    "dates_to_insert = original_missing_dates.copy()  # Only insert the actual missing dates\n",
    "dates_to_fetch = original_missing_dates.copy()   # Fetch missing dates + buffer\n",
    "\n",
    "# Add 3 buffer days before first missing date to ensure we can calculate lag features\n",
    "if original_missing_dates:\n",
    "    earliest_missing = min(original_missing_dates)\n",
    "    buffer_dates = [earliest_missing - timedelta(days=i) for i in range(1, 4)]\n",
    "    # Only add buffer dates that aren't already in existing_dates\n",
    "    buffer_dates = [d for d in buffer_dates if d not in existing_dates]\n",
    "    dates_to_fetch = sorted(buffer_dates + dates_to_fetch)\n",
    "\n",
    "formatted = \", \".join(d.isoformat() for d in dates_to_fetch)\n",
    "print(f\"üìÖ Dates to fetch: {formatted}\")\n",
    "print(f\"üìÖ Dates to insert: {', '.join(d.isoformat() for d in dates_to_insert)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "### 2.4.3. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (9.62s) \n"
     ]
    }
   ],
   "source": [
    "# Prepare historical data window\n",
    "historical_cutoff = pd.to_datetime(min(dates_to_fetch)) - pd.Timedelta(days=3)\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "historical = historical [historical[\"date\"] >= historical_cutoff]\n",
    "\n",
    "# Track existing sensor-date pairs\n",
    "existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))\n",
    "\n",
    "# Initialize data containers\n",
    "all_aq_rows = [historical]\n",
    "all_weather_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "### 2.4.4. Fetch Missing Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc65830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching air quality for sensor 60853, 1/103\n",
      "Fetching air quality for sensor 59497, 2/103\n",
      "Fetching air quality for sensor 59650, 3/103\n",
      "Fetching air quality for sensor 112672, 4/103\n",
      "Fetching air quality for sensor 60889, 5/103\n",
      "Fetching air quality for sensor 60076, 6/103\n",
      "Fetching air quality for sensor 58921, 7/103\n",
      "Fetching air quality for sensor 84085, 8/103\n",
      "Fetching air quality for sensor 89584, 9/103\n",
      "Fetching air quality for sensor 198559, 10/103\n",
      "Fetching air quality for sensor 149242, 11/103\n",
      "Fetching air quality for sensor 105325, 12/103\n",
      "Fetching air quality for sensor 78529, 13/103\n",
      "Fetching air quality for sensor 88876, 14/103\n",
      "Fetching air quality for sensor 65272, 15/103\n",
      "Fetching air quality for sensor 77488, 16/103\n",
      "Fetching air quality for sensor 351115, 17/103\n",
      "Fetching air quality for sensor 122302, 18/103\n",
      "Fetching air quality for sensor 196735, 19/103\n",
      "Fetching air quality for sensor 69724, 20/103\n",
      "Fetching air quality for sensor 60859, 21/103\n",
      "Fetching air quality for sensor 65146, 22/103\n",
      "Fetching air quality for sensor 57421, 23/103\n",
      "Fetching air quality for sensor 194215, 24/103\n",
      "Fetching air quality for sensor 82384, 25/103\n",
      "Fetching air quality for sensor 180187, 26/103\n",
      "Fetching air quality for sensor 68167, 27/103\n",
      "Fetching air quality for sensor 129124, 28/103\n",
      "Fetching air quality for sensor 79999, 29/103\n",
      "Fetching air quality for sensor 59593, 30/103\n",
      "Fetching air quality for sensor 462457, 31/103\n",
      "Fetching air quality for sensor 417595, 32/103\n",
      "Fetching air quality for sensor 59410, 33/103\n",
      "Fetching air quality for sensor 249862, 34/103\n",
      "Fetching air quality for sensor 345007, 35/103\n",
      "Fetching air quality for sensor 128095, 36/103\n",
      "Fetching air quality for sensor 70564, 37/103\n",
      "Fetching air quality for sensor 63637, 38/103\n",
      "Fetching air quality for sensor 65104, 39/103\n",
      "Fetching air quality for sensor 65290, 40/103\n",
      "Fetching air quality for sensor 252352, 41/103\n",
      "Fetching air quality for sensor 60535, 42/103\n",
      "Fetching air quality for sensor 79750, 43/103\n",
      "Fetching air quality for sensor 58912, 44/103\n",
      "Fetching air quality for sensor 415030, 45/103\n",
      "Fetching air quality for sensor 65284, 46/103\n",
      "Fetching air quality for sensor 107110, 47/103\n",
      "Fetching air quality for sensor 90676, 48/103\n",
      "Fetching air quality for sensor 163156, 49/103\n",
      "Fetching air quality for sensor 59893, 50/103\n",
      "Fetching air quality for sensor 121810, 51/103\n",
      "Fetching air quality for sensor 60541, 52/103\n",
      "Fetching air quality for sensor 60886, 53/103\n",
      "Fetching air quality for sensor 77446, 54/103\n",
      "Fetching air quality for sensor 59095, 55/103\n",
      "Fetching air quality for sensor 88372, 56/103\n",
      "Fetching air quality for sensor 62566, 57/103\n",
      "Fetching air quality for sensor 494275, 58/103\n",
      "Fetching air quality for sensor 61867, 59/103\n",
      "Fetching air quality for sensor 376954, 60/103\n",
      "Fetching air quality for sensor 191047, 61/103\n",
      "Fetching air quality for sensor 59656, 62/103\n",
      "Fetching air quality for sensor 62848, 63/103\n",
      "Fetching air quality for sensor 407335, 64/103\n",
      "Fetching air quality for sensor 87319, 65/103\n",
      "Fetching air quality for sensor 420664, 66/103\n",
      "Fetching air quality for sensor 409513, 67/103\n",
      "Fetching air quality for sensor 78532, 68/103\n",
      "Fetching air quality for sensor 80773, 69/103\n",
      "Fetching air quality for sensor 250030, 70/103\n",
      "Fetching air quality for sensor 76915, 71/103\n",
      "Fetching air quality for sensor 61714, 72/103\n",
      "Fetching air quality for sensor 69628, 73/103\n",
      "Fetching air quality for sensor 476353, 74/103\n",
      "Fetching air quality for sensor 92683, 75/103\n",
      "Fetching air quality for sensor 112993, 76/103\n",
      "Fetching air quality for sensor 82942, 77/103\n",
      "Fetching air quality for sensor 58909, 78/103\n",
      "Fetching air quality for sensor 60838, 79/103\n",
      "Fetching air quality for sensor 192520, 80/103\n",
      "Fetching air quality for sensor 81505, 81/103\n",
      "Fetching air quality for sensor 65707, 82/103\n",
      "Fetching air quality for sensor 59887, 83/103\n",
      "Fetching air quality for sensor 63646, 84/103\n",
      "Fetching air quality for sensor 59356, 85/103\n",
      "Fetching air quality for sensor 60073, 86/103\n",
      "Fetching air quality for sensor 61045, 87/103\n",
      "Fetching air quality for sensor 61861, 88/103\n",
      "Fetching air quality for sensor 154549, 89/103\n",
      "Fetching air quality for sensor 61420, 90/103\n",
      "Fetching air quality for sensor 404209, 91/103\n",
      "Fetching air quality for sensor 59899, 92/103\n",
      "Fetching air quality for sensor 533086, 93/103\n",
      "Fetching air quality for sensor 113542, 94/103\n",
      "Fetching air quality for sensor 208483, 95/103\n",
      "Fetching air quality for sensor 62968, 96/103\n",
      "Fetching air quality for sensor 474841, 97/103\n",
      "Fetching air quality for sensor 113539, 98/103\n",
      "Fetching air quality for sensor 497266, 99/103\n",
      "Fetching air quality for sensor 58666, 100/103\n",
      "Fetching air quality for sensor 401314, 101/103\n",
      "Fetching air quality for sensor 562600, 102/103\n",
      "Fetching air quality for sensor 556792, 103/103\n",
      "üìä Collected 1134 air quality dataframes\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching air quality for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in dates_to_fetch:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            # aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            # Add metadata\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "\n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Air quality for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_aq_rows)} air quality dataframes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf99689",
   "metadata": {},
   "source": [
    "### 2.4.5. Fetch Missing Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for sensor 60853, 1/103\n",
      "Fetching weather for sensor 59497, 2/103\n",
      "Fetching weather for sensor 59650, 3/103\n",
      "Fetching weather for sensor 112672, 4/103\n",
      "Fetching weather for sensor 60889, 5/103\n",
      "Fetching weather for sensor 60076, 6/103\n",
      "Fetching weather for sensor 58921, 7/103\n",
      "Fetching weather for sensor 84085, 8/103\n",
      "Fetching weather for sensor 89584, 9/103\n",
      "Fetching weather for sensor 198559, 10/103\n",
      "Fetching weather for sensor 149242, 11/103\n",
      "Fetching weather for sensor 105325, 12/103\n",
      "Fetching weather for sensor 78529, 13/103\n",
      "Fetching weather for sensor 88876, 14/103\n",
      "Fetching weather for sensor 65272, 15/103\n",
      "Fetching weather for sensor 77488, 16/103\n",
      "Fetching weather for sensor 351115, 17/103\n",
      "Fetching weather for sensor 122302, 18/103\n",
      "Fetching weather for sensor 196735, 19/103\n",
      "Fetching weather for sensor 69724, 20/103\n",
      "Fetching weather for sensor 60859, 21/103\n",
      "Fetching weather for sensor 65146, 22/103\n",
      "Fetching weather for sensor 57421, 23/103\n",
      "Fetching weather for sensor 194215, 24/103\n",
      "Fetching weather for sensor 82384, 25/103\n",
      "Fetching weather for sensor 180187, 26/103\n",
      "Fetching weather for sensor 68167, 27/103\n",
      "Fetching weather for sensor 129124, 28/103\n",
      "Fetching weather for sensor 79999, 29/103\n",
      "Fetching weather for sensor 59593, 30/103\n",
      "Fetching weather for sensor 462457, 31/103\n",
      "Fetching weather for sensor 417595, 32/103\n",
      "Fetching weather for sensor 59410, 33/103\n",
      "Fetching weather for sensor 249862, 34/103\n",
      "Fetching weather for sensor 345007, 35/103\n",
      "Fetching weather for sensor 128095, 36/103\n",
      "Fetching weather for sensor 70564, 37/103\n",
      "Fetching weather for sensor 63637, 38/103\n",
      "Fetching weather for sensor 65104, 39/103\n",
      "Fetching weather for sensor 65290, 40/103\n",
      "Fetching weather for sensor 252352, 41/103\n",
      "Fetching weather for sensor 60535, 42/103\n",
      "Fetching weather for sensor 79750, 43/103\n",
      "Fetching weather for sensor 58912, 44/103\n",
      "Fetching weather for sensor 415030, 45/103\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching weather for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in dates_to_fetch:\n",
    "        try:\n",
    "            weather_df = fetchers.get_weather_forecast(\n",
    "                sensor_id=sensor_id,\n",
    "                latitude=meta[\"latitude\"],\n",
    "                longitude=meta[\"longitude\"],\n",
    "                start_date=day,\n",
    "                end_date=day + timedelta(days=6)\n",
    "            )\n",
    "\n",
    "            if weather_df.empty:\n",
    "                continue\n",
    "\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.normalize()\n",
    "            # weather_df[\"date\"] = (\n",
    "            #     pd.to_datetime(weather_df[\"date\"])\n",
    "            #     .dt.normalize()\n",
    "            #     .dt.tz_localize(None)\n",
    "            # )\n",
    "\n",
    "            all_weather_rows.append(weather_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weather for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_weather_rows)} weather dataframes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca1fa",
   "metadata": {},
   "source": [
    "### 2.4.9. Clean and Align Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping empty or invalid df[0]\n",
      "üìã Cleaned 1133 air quality dataframes\n",
      "üìã Using base columns (excluding engineered features): 9 columns\n"
     ]
    }
   ],
   "source": [
    "cleaned_aq_rows = []\n",
    "\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "base_cols = [c for c in historical.columns if c not in engineered_cols]\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(base_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align to base columns only (no engineered features yet)\n",
    "    aligned = df.reindex(columns=base_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(base_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical (for base columns only)\n",
    "    for col in base_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "print(f\"üìã Cleaned {len(cleaned_aq_rows)} air quality dataframes\")\n",
    "print(f\"üìã Using base columns (excluding engineered features): {len(base_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235625a",
   "metadata": {},
   "source": [
    "### 2.4.10. Combine and Clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e893b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è Total weather records: 1751\n",
      "üìÖ Weather date range: 2026-01-18 00:00:00 to 2026-02-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if all_weather_rows:\n",
    "    all_weather = pd.concat(all_weather_rows, ignore_index=True)\n",
    "    all_weather = all_weather.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "    all_weather[\"date\"] = pd.to_datetime(all_weather[\"date\"]).dt.tz_localize(None)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_weather = all_weather.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather records: {len(all_weather)}\")\n",
    "    print(f\"üìÖ Weather date range: {all_weather['date'].min()} to {all_weather['date'].max()}\")\n",
    "else:\n",
    "    all_weather = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No weather data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa784d",
   "metadata": {},
   "source": [
    "## 2.5. Combine Data and Add Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd37f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import feature_engineering\n",
    "\n",
    "# locations = feature_engineering.build_sensor_location_map(df, sensor_locations)\n",
    "# print(\"DEBUG LOCATIONS:\", locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea612675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total records after deduplication: 1133\n",
      "üìä Unique sensors: 103\n",
      "üìä Date range: 2026-01-18 00:00:00 to 2026-01-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Drop engineered columns from historical data before combining\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "historical_base = historical.drop(columns=engineered_cols, errors=\"ignore\")\n",
    "\n",
    "# Combine data\n",
    "all_aq = pd.concat([historical_base, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "# Remove duplicates: keep the first occurrence of each sensor_id + date combination\n",
    "all_aq = all_aq.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Total records after deduplication: {len(all_aq)}\")\n",
    "print(f\"üìä Unique sensors: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"üìä Date range: {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "# Add engineered features\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "\n",
    "# Pass sensor_locations dict to nearby sensor feature\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780ad9",
   "metadata": {},
   "source": [
    "## 2.6. Insert Data to Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e42e56",
   "metadata": {},
   "source": [
    "### 2.6.1. Batch Insert Air Quality Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Date 2026-01-18: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   ‚ö†Ô∏è pm25_rolling_3d: 103/103 NaN values\n",
      "   ‚ö†Ô∏è pm25_lag_1d: 103/103 NaN values\n",
      "   ‚ö†Ô∏è pm25_lag_2d: 103/103 NaN values\n",
      "   ‚ö†Ô∏è pm25_lag_3d: 103/103 NaN values\n",
      "   ‚ö†Ô∏è pm25_nearby_avg: 103/103 NaN values\n",
      "   After engineered features filter: 0 rows\n",
      "‚ö†Ô∏è No valid rows for 2026-01-18\n",
      "\n",
      "üîç Date 2026-01-19: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   ‚ö†Ô∏è pm25_lag_2d: 103/103 NaN values\n",
      "   ‚ö†Ô∏è pm25_lag_3d: 103/103 NaN values\n",
      "   After engineered features filter: 0 rows\n",
      "‚ö†Ô∏è No valid rows for 2026-01-19\n",
      "\n",
      "üîç Date 2026-01-20: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   ‚ö†Ô∏è pm25_lag_3d: 103/103 NaN values\n",
      "   After engineered features filter: 0 rows\n",
      "‚ö†Ô∏è No valid rows for 2026-01-20\n",
      "\n",
      "üîç Date 2026-01-21: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:02:53,779 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:03,199 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-21\n",
      "\n",
      "üîç Date 2026-01-22: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:03,436 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:12,179 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-22\n",
      "\n",
      "üîç Date 2026-01-23: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:12,458 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:21,708 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-23\n",
      "\n",
      "üîç Date 2026-01-24: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:21,940 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:30,540 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-24\n",
      "\n",
      "üîç Date 2026-01-25: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:30,774 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:39,675 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-25\n",
      "\n",
      "üîç Date 2026-01-26: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:39,916 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:48,552 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-26\n",
      "\n",
      "üîç Date 2026-01-27: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:48,787 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:03:57,307 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-27\n",
      "\n",
      "üîç Date 2026-01-28: 103 total rows before filtering\n",
      "   After pm25 filter: 103 rows\n",
      "   After engineered features filter: 103 rows\n",
      "2026-01-28 15:03:57,538 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:04:06,131 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-28\n"
     ]
    }
   ],
   "source": [
    "for day in dates_to_insert:\n",
    "    day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "    \n",
    "    # Debug: Show what we have before filtering\n",
    "    print(f\"\\nüîç Date {day}: {len(day_rows)} total rows before filtering\", flush=True)\n",
    "    \n",
    "    day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "    print(f\"   After pm25 filter: {len(day_rows)} rows\", flush=True)\n",
    "\n",
    "    engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "    \n",
    "    # Check which engineered features have NaN\n",
    "    if not day_rows.empty:\n",
    "        for col in engineered_cols:\n",
    "            nan_count = day_rows[col].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                print(f\"   ‚ö†Ô∏è {col}: {nan_count}/{len(day_rows)} NaN values\", flush=True)\n",
    "    \n",
    "    day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "    print(f\"   After engineered features filter: {len(day_rows)} rows\", flush=True)\n",
    "\n",
    "    if not day_rows.empty:\n",
    "        # Convert types to match feature group schema\n",
    "        day_rows = day_rows.astype({\n",
    "            \"sensor_id\": \"int32\",\n",
    "            \"pm25\": \"float64\",\n",
    "            \"pm25_lag_1d\": \"float64\",\n",
    "            \"pm25_lag_2d\": \"float64\",\n",
    "            \"pm25_lag_3d\": \"float64\",\n",
    "            \"pm25_rolling_3d\": \"float64\",\n",
    "            \"pm25_nearby_avg\": \"float64\",\n",
    "            \"city\": \"string\",\n",
    "            \"street\": \"string\",\n",
    "            \"country\": \"string\",\n",
    "            \"aqicn_url\": \"string\",\n",
    "            \"latitude\": \"float64\",\n",
    "            \"longitude\": \"float64\",\n",
    "        })\n",
    "        \n",
    "        # Ensure correct column order\n",
    "        fg_columns = [f.name for f in air_quality_fg.features]\n",
    "        day_rows = day_rows[fg_columns]\n",
    "        \n",
    "        air_quality_fg.insert(day_rows)\n",
    "        print(f\"‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid rows for {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78444036",
   "metadata": {},
   "source": [
    "### 2.6.2. Verify Air Quality Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sensor_id       date  pm25  pm25_lag_1d  pm25_rolling_3d  \\\n",
      "10        57421 2026-01-28   5.0          5.0              5.0   \n",
      "21        58666 2026-01-28   7.0          7.0              7.0   \n",
      "32        58909 2026-01-28   7.0          7.0              7.0   \n",
      "43        58912 2026-01-28   4.0          4.0              4.0   \n",
      "54        58921 2026-01-28  15.0         15.0             15.0   \n",
      "...         ...        ...   ...          ...              ...   \n",
      "1088     494275 2026-01-28   0.0          0.0              0.0   \n",
      "1099     497266 2026-01-28  13.0         13.0             13.0   \n",
      "1110     533086 2026-01-28   5.0          5.0              5.0   \n",
      "1121     556792 2026-01-28   6.0          6.0              6.0   \n",
      "1132     562600 2026-01-28   9.0          9.0              9.0   \n",
      "\n",
      "      pm25_nearby_avg  \n",
      "10           5.333333  \n",
      "21           8.666667  \n",
      "32           8.000000  \n",
      "43           5.000000  \n",
      "54           5.333333  \n",
      "...               ...  \n",
      "1088         2.000000  \n",
      "1099         5.333333  \n",
      "1110         5.333333  \n",
      "1121         6.000000  \n",
      "1132         3.333333  \n",
      "\n",
      "[103 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_rolling_3d\", \"pm25_nearby_avg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2437cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sensor_id       date  pm25                        city  \\\n",
      "9         57421 2026-01-27   5.0                 Johannehill   \n",
      "20        58666 2026-01-27   7.0                      √Ñngeby   \n",
      "31        58909 2026-01-27   7.0                       Slaka   \n",
      "42        58912 2026-01-27   4.0                    H√§gern√§s   \n",
      "53        58921 2026-01-27  15.0  Skarpn√§cks stadsdelsomr√•de   \n",
      "...         ...        ...   ...                         ...   \n",
      "1087     494275 2026-01-27   0.0                      Stavre   \n",
      "1098     497266 2026-01-27  13.0                  Skellefte√•   \n",
      "1109     533086 2026-01-27   5.0                        Berg   \n",
      "1120     556792 2026-01-27   6.0                  Norrk√∂ping   \n",
      "1131     562600 2026-01-27   9.0                       Solna   \n",
      "\n",
      "                  street country                            aqicn_url  \\\n",
      "9                   Ubby  Sweden   https://api.waqi.info/feed/A57421/   \n",
      "20         Jupitersv√§gen  Sweden   https://api.waqi.info/feed/A58666/   \n",
      "31         Tr√∂skaregatan  Sweden   https://api.waqi.info/feed/A58909/   \n",
      "42            Radarv√§gen  Sweden   https://api.waqi.info/feed/A58912/   \n",
      "53    Karin Larssons v√§g  Sweden   https://api.waqi.info/feed/A58921/   \n",
      "...                  ...     ...                                  ...   \n",
      "1087               Z 565  Sweden  https://api.waqi.info/feed/A494275/   \n",
      "1098        Mobackav√§gen  Sweden  https://api.waqi.info/feed/A497266/   \n",
      "1109        Bj√∂rnsbacken  Sweden  https://api.waqi.info/feed/A533086/   \n",
      "1120        Enebymov√§gen  Sweden  https://api.waqi.info/feed/A556792/   \n",
      "1131      Enk√∂pingsv√§gen  Sweden  https://api.waqi.info/feed/A562600/   \n",
      "\n",
      "      latitude  longitude  pm25_rolling_3d  pm25_lag_1d  pm25_lag_2d  \\\n",
      "9     62.00000   15.00000              5.0          5.0          5.0   \n",
      "20    59.98333   17.73333              7.0          7.0          7.0   \n",
      "31    58.36667   15.55000              7.0          7.0          7.0   \n",
      "42    59.75000   15.43333              4.0          4.0          4.0   \n",
      "53    62.00000   15.00000             15.0         15.0         15.0   \n",
      "...        ...        ...              ...          ...          ...   \n",
      "1087  63.41667   14.13333              0.0          0.0          0.0   \n",
      "1098  64.75067   20.95279             13.0         13.0         13.0   \n",
      "1109  62.00000   15.00000              5.0          5.0          5.0   \n",
      "1120  58.59419   16.18260              6.0          6.0          6.0   \n",
      "1131  59.36004   18.00086              9.0          9.0          9.0   \n",
      "\n",
      "      pm25_lag_3d  pm25_nearby_avg  \n",
      "9             5.0         5.333333  \n",
      "20            7.0         8.666667  \n",
      "31            7.0         8.000000  \n",
      "42            4.0         5.000000  \n",
      "53           15.0         5.333333  \n",
      "...           ...              ...  \n",
      "1087          0.0         2.000000  \n",
      "1098         13.0         5.333333  \n",
      "1109          5.0         5.333333  \n",
      "1120          6.0         6.000000  \n",
      "1131          9.0         3.333333  \n",
      "\n",
      "[103 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today - timedelta(days=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab35d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging nearby sensor feature:\n",
      "Total sensors in all_aq: 103\n",
      "Total sensors in sensor_locations: 103\n",
      "\n",
      "pm25_lag_1d stats for today:\n",
      "count    103.000000\n",
      "mean       7.970874\n",
      "std       18.102354\n",
      "min        0.000000\n",
      "25%        3.000000\n",
      "50%        4.000000\n",
      "75%        8.000000\n",
      "max      151.000000\n",
      "Name: pm25_lag_1d, dtype: float64\n",
      "\n",
      "üîç Checking sensor 58666:\n",
      "    sensor_id       date  pm25  pm25_lag_1d  pm25_nearby_avg\n",
      "17      58666 2026-01-24   7.0          7.0         8.666667\n",
      "18      58666 2026-01-25   7.0          7.0         8.666667\n",
      "19      58666 2026-01-26   7.0          7.0         8.666667\n",
      "20      58666 2026-01-27   7.0          7.0         8.666667\n",
      "21      58666 2026-01-28   7.0          7.0         8.666667\n",
      "\n",
      "Sensor 58666 is in sensor_locations\n",
      "Lat/Lon: 59.98333, 17.73333\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check nearby sensor calculation\n",
    "print(\"üîç Debugging nearby sensor feature:\")\n",
    "print(f\"Total sensors in all_aq: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"Total sensors in sensor_locations: {len(sensor_locations)}\")\n",
    "\n",
    "# Check if pm25_lag_1d has values\n",
    "lag_stats = all_aq[all_aq['date'].dt.date == today]['pm25_lag_1d'].describe()\n",
    "print(f\"\\npm25_lag_1d stats for today:\")\n",
    "print(lag_stats)\n",
    "\n",
    "# Check one sensor specifically\n",
    "test_sensor = 58666\n",
    "print(f\"\\nüîç Checking sensor {test_sensor}:\")\n",
    "sensor_data = all_aq[all_aq['sensor_id'] == test_sensor].tail(5)\n",
    "print(sensor_data[['sensor_id', 'date', 'pm25', 'pm25_lag_1d', 'pm25_nearby_avg']])\n",
    "\n",
    "# Check if this sensor has neighbors in the locations dict\n",
    "if test_sensor in sensor_locations:\n",
    "    print(f\"\\nSensor {test_sensor} is in sensor_locations\")\n",
    "    print(f\"Lat/Lon: {sensor_locations[test_sensor]['latitude']}, {sensor_locations[test_sensor]['longitude']}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Sensor {test_sensor} NOT in sensor_locations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Manually trace the nearby sensor calculation for one sensor\n",
    "# from utils.feature_engineering import build_sensor_location_map, compute_closest_sensors\n",
    "\n",
    "# test_sensor = 58666\n",
    "# locations = build_sensor_location_map(all_aq, sensor_locations)\n",
    "# closest_map = compute_closest_sensors(locations, n_closest=3)\n",
    "\n",
    "# print(f\"üîç Closest sensors to {test_sensor}:\")\n",
    "# neighbors = closest_map.get(test_sensor, [])\n",
    "# print(f\"Neighbors: {neighbors}\")\n",
    "\n",
    "# if neighbors:\n",
    "#     # Get neighbor data\n",
    "#     neighbor_df = all_aq[all_aq['sensor_id'].isin(neighbors)][['date', 'pm25_lag_1d']]\n",
    "#     print(f\"\\nüìä Neighbor data (showing last 10):\")\n",
    "#     print(neighbor_df.tail(10))\n",
    "    \n",
    "#     # Group by date\n",
    "#     neighbor_avg = neighbor_df.groupby('date')['pm25_lag_1d'].mean().reset_index()\n",
    "#     print(f\"\\nüìä Neighbor average by date:\")\n",
    "#     print(neighbor_avg.tail(10))\n",
    "    \n",
    "#     # Get sensor data and merge\n",
    "#     sensor_data = all_aq[all_aq['sensor_id'] == test_sensor]\n",
    "#     print(f\"\\nüìä Sensor {test_sensor} data:\")\n",
    "#     print(sensor_data[['sensor_id', 'date', 'pm25_lag_1d']].tail(5))\n",
    "    \n",
    "#     merged = sensor_data.merge(neighbor_avg, on='date', how='left')\n",
    "#     print(f\"\\nüìä After merge:\")\n",
    "#     print(merged[['sensor_id', 'date', 'pm25_lag_1d_x', 'pm25_lag_1d_y']].tail(5))\n",
    "#     print(f\"\\n‚úÖ Column 'pm25_lag_1d_y' exists: {'pm25_lag_1d_y' in merged.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import the fixed module\n",
    "# import importlib\n",
    "# importlib.reload(feature_engineering)\n",
    "\n",
    "# # Re-apply the nearby sensor feature with the fixed function\n",
    "# print(\"üîÑ Re-calculating nearby sensor averages with fixed function...\")\n",
    "# all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)\n",
    "\n",
    "# print(\"\\n‚úÖ Re-calculated nearby sensor averages\")\n",
    "# print(\"\\nüìä Sample data for today:\")\n",
    "# sample = all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_nearby_avg\"]].head(10)\n",
    "# print(sample)\n",
    "\n",
    "# print(f\"\\nüìä Nearby avg stats:\")\n",
    "# print(all_aq[\"pm25_nearby_avg\"].describe())\n",
    "# print(f\"\\nüìä Non-null nearby averages: {all_aq['pm25_nearby_avg'].notna().sum()} / {len(all_aq)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a801bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check today's data more thoroughly\n",
    "# today_data = all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_nearby_avg\"]]\n",
    "# print(f\"üìä Today's data ({today}):\")\n",
    "# print(f\"Total records: {len(today_data)}\")\n",
    "# print(f\"Records with nearby_avg: {today_data['pm25_nearby_avg'].notna().sum()}\")\n",
    "# print(f\"Records without nearby_avg: {today_data['pm25_nearby_avg'].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüîç Sensors still missing nearby_avg on today:\")\n",
    "# missing_nearby = today_data[today_data['pm25_nearby_avg'].isna()]\n",
    "# if len(missing_nearby) > 0:\n",
    "#     print(missing_nearby.head(10))\n",
    "#     print(f\"\\n...and {max(0, len(missing_nearby) - 10)} more\")\n",
    "# else:\n",
    "#     print(\"None! All sensors have nearby averages ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "### 2.6.3. Batch Insert Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:04:06,455 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "‚ö†Ô∏è Connection error on weather batch 1, retrying in 1s...\n",
      "2026-01-28 15:05:06,901 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 15:05:15,728 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/weather_1_offline_fg_materialization/config_1768459788862) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Weather batch 1: 100 records (total: 100/1751)\n",
      "2026-01-28 15:05:15,924 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 2: 100 records (total: 200/1751)\n",
      "2026-01-28 15:05:25,063 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 3: 100 records (total: 300/1751)\n",
      "2026-01-28 15:05:33,752 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 4: 100 records (total: 400/1751)\n",
      "2026-01-28 15:05:42,549 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 5: 100 records (total: 500/1751)\n",
      "2026-01-28 15:05:51,185 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 6: 100 records (total: 600/1751)\n",
      "2026-01-28 15:05:59,957 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 7: 100 records (total: 700/1751)\n",
      "2026-01-28 15:06:08,701 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 8: 100 records (total: 800/1751)\n",
      "2026-01-28 15:06:17,444 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 9: 100 records (total: 900/1751)\n",
      "2026-01-28 15:06:26,143 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 10: 100 records (total: 1000/1751)\n",
      "2026-01-28 15:06:34,802 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 11: 100 records (total: 1100/1751)\n",
      "2026-01-28 15:06:43,509 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 12: 100 records (total: 1200/1751)\n",
      "2026-01-28 15:06:52,498 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 13: 100 records (total: 1300/1751)\n",
      "2026-01-28 15:07:01,470 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 14: 100 records (total: 1400/1751)\n",
      "2026-01-28 15:07:10,511 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 15: 100 records (total: 1500/1751)\n",
      "2026-01-28 15:07:19,196 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 16: 100 records (total: 1600/1751)\n",
      "2026-01-28 15:07:27,934 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 17: 100 records (total: 1700/1751)\n",
      "2026-01-28 15:07:36,665 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 51/51 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 18: 51 records (total: 1751/1751)\n",
      "üå§Ô∏è Total weather inserted: 1751/1751 records\n"
     ]
    }
   ],
   "source": [
    "if not all_weather.empty:\n",
    "    # Convert types to match feature group schema\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    weather_fg_columns = [f.name for f in weather_fg.features]\n",
    "    all_weather = all_weather[weather_fg_columns]\n",
    "    \n",
    "    # Insert in smaller batches to avoid connection issues\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"‚ö†Ô∏è Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather inserted: {total_inserted}/{len(all_weather)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No weather data to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845de6",
   "metadata": {},
   "source": [
    "### 2.6.4. Print Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9844",
   "metadata": {},
   "source": [
    "## 2.7. Inspect Inserted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1986667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Air quality records inserted: 1133\n",
      "\n",
      "üìã Sample air quality data:\n",
      "   sensor_id       date  pm25         city street country  \\\n",
      "0      57421 2026-01-18   5.0  Johannehill   Ubby  Sweden   \n",
      "1      57421 2026-01-19   5.0  Johannehill   Ubby  Sweden   \n",
      "2      57421 2026-01-20   5.0  Johannehill   Ubby  Sweden   \n",
      "3      57421 2026-01-21   5.0  Johannehill   Ubby  Sweden   \n",
      "4      57421 2026-01-22   5.0  Johannehill   Ubby  Sweden   \n",
      "\n",
      "                            aqicn_url  latitude  longitude  pm25_rolling_3d  \\\n",
      "0  https://api.waqi.info/feed/A57421/      62.0       15.0              NaN   \n",
      "1  https://api.waqi.info/feed/A57421/      62.0       15.0              5.0   \n",
      "2  https://api.waqi.info/feed/A57421/      62.0       15.0              5.0   \n",
      "3  https://api.waqi.info/feed/A57421/      62.0       15.0              5.0   \n",
      "4  https://api.waqi.info/feed/A57421/      62.0       15.0              5.0   \n",
      "\n",
      "   pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  pm25_nearby_avg  \n",
      "0          NaN          NaN          NaN              NaN  \n",
      "1          5.0          NaN          NaN         5.333333  \n",
      "2          5.0          5.0          NaN         5.333333  \n",
      "3          5.0          5.0          5.0         5.333333  \n",
      "4          5.0          5.0          5.0         5.333333  \n",
      "\n",
      "üîß Air quality data types:\n",
      "sensor_id                   int32\n",
      "date               datetime64[us]\n",
      "pm25                      float64\n",
      "city                       object\n",
      "street                     object\n",
      "country                    object\n",
      "aqicn_url                  object\n",
      "latitude                  float64\n",
      "longitude                 float64\n",
      "pm25_rolling_3d           float64\n",
      "pm25_lag_1d               float64\n",
      "pm25_lag_2d               float64\n",
      "pm25_lag_3d               float64\n",
      "pm25_nearby_avg           float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Date range:\n",
      "From 2026-01-18 00:00:00 to 2026-01-28 00:00:00\n",
      "\n",
      "üå§Ô∏è Weather records inserted: 1751\n",
      "\n",
      "üìã Sample weather data:\n",
      "         date  sensor_id  temperature_2m_mean  precipitation_sum  \\\n",
      "0  2026-01-18      57421            -4.706000                0.0   \n",
      "1  2026-01-19      57421            -3.526834                0.0   \n",
      "3  2026-01-20      57421            -4.024750                0.0   \n",
      "6  2026-01-21      57421            -3.328917                1.0   \n",
      "10 2026-01-22      57421            -4.656000                0.3   \n",
      "\n",
      "    wind_speed_10m_max  wind_direction_10m_dominant  \n",
      "0             6.480000                   207.441162  \n",
      "1             6.840000                   184.421982  \n",
      "3             9.720000                     9.830655  \n",
      "6             9.720000                    69.557274  \n",
      "10           10.440001                   104.406693  \n",
      "\n",
      "üîß Weather data types:\n",
      "date                           datetime64[ns]\n",
      "sensor_id                               int32\n",
      "temperature_2m_mean                   float64\n",
      "precipitation_sum                     float64\n",
      "wind_speed_10m_max                    float64\n",
      "wind_direction_10m_dominant           float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Unique weather dates:\n",
      "<DatetimeArray>\n",
      "['2026-01-18 00:00:00', '2026-01-19 00:00:00', '2026-01-20 00:00:00',\n",
      " '2026-01-21 00:00:00', '2026-01-22 00:00:00', '2026-01-23 00:00:00',\n",
      " '2026-01-24 00:00:00', '2026-01-25 00:00:00', '2026-01-26 00:00:00',\n",
      " '2026-01-27 00:00:00', '2026-01-28 00:00:00', '2026-01-29 00:00:00',\n",
      " '2026-01-30 00:00:00', '2026-01-31 00:00:00', '2026-02-01 00:00:00',\n",
      " '2026-02-02 00:00:00', '2026-02-03 00:00:00']\n",
      "Length: 17, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "if 'all_aq' in locals() and not all_aq.empty:\n",
    "    print(f\"‚úÖ Air quality records inserted: {len(all_aq)}\")\n",
    "    print(\"\\nüìã Sample air quality data:\")\n",
    "    print(all_aq.head())\n",
    "    print(\"\\nüîß Air quality data types:\")\n",
    "    print(all_aq.dtypes)\n",
    "    print(\"\\nüìÖ Date range:\")\n",
    "    print(f\"From {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "if 'all_weather' in locals() and not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è Weather records inserted: {len(all_weather)}\")\n",
    "    print(\"\\nüìã Sample weather data:\")\n",
    "    print(all_weather.head())\n",
    "    print(\"\\nüîß Weather data types:\")\n",
    "    print(all_weather.dtypes)\n",
    "    print(\"\\nüìÖ Unique weather dates:\")\n",
    "    print(all_weather['date'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
