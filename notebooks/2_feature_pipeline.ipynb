{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6c5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f5bad",
   "metadata": {},
   "source": [
    "### 2.1.2. Load Settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d171fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopsworksSettings initialized!\n",
      "2026-01-26 09:47:35,021 INFO: Initializing external client\n",
      "2026-01-26 09:47:35,022 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-26 09:47:37,158 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Detect environment (local, Jupyter, or Hopsworks Job)\n",
    "# ---------------------------------------------------------\n",
    "RUNNING_IN_HOPSWORKS_JOB = \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "\n",
    "if RUNNING_IN_HOPSWORKS_JOB:\n",
    "    # Running inside a Hopsworks Job ‚Üí Vault secrets available\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    os.environ[\"HOPSWORKS_API_KEY\"] = secrets_api.get_secret(\"HOPSWORKS_API_KEY\").value\n",
    "    os.environ[\"AQICN_API_KEY\"] = secrets_api.get_secret(\"AQICN_API_KEY\").value\n",
    "    os.environ[\"GH_PAT\"] = secrets_api.get_secret(\"GH_PAT\").value\n",
    "    os.environ[\"GH_USERNAME\"] = secrets_api.get_secret(\"GH_USERNAME\").value\n",
    "\n",
    "else:\n",
    "    # Running locally or in Hopsworks Jupyter ‚Üí use .env\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Load Pydantic settings (now environment is ready)\n",
    "# ---------------------------------------------------------\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Login to Hopsworks using the API key\n",
    "# ---------------------------------------------------------\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "### 2.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists at c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa04a",
   "metadata": {},
   "source": [
    "### 2.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.3. Load Sensor Locations from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.15s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.14s) \n",
      "üìã Found 103 sensors in feature store\n",
      "üìç Loaded locations for 103 existing sensors\n"
     ]
    }
   ],
   "source": [
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Build sensor location dictionary: sensor_id -> (lat, lon, city, street, country, aqicn_url)\n",
    "sensor_locations = {}\n",
    "existing_aq_data = air_quality_fg.read()\n",
    "existing_sensors = set(existing_aq_data[\"sensor_id\"].unique())\n",
    "print(f\"üìã Found {len(existing_sensors)} sensors in feature store\")\n",
    "\n",
    "\n",
    "for _, row in existing_aq_data[\n",
    "    [\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\", \"aqicn_url\"]\n",
    "].drop_duplicates(subset=[\"sensor_id\"]).iterrows():\n",
    "    \n",
    "    sensor_locations[row[\"sensor_id\"]] = {\n",
    "        \"latitude\": row[\"latitude\"],\n",
    "        \"longitude\": row[\"longitude\"],\n",
    "        \"city\": row[\"city\"],\n",
    "        \"street\": row[\"street\"],\n",
    "        \"country\": row[\"country\"],\n",
    "        \"aqicn_url\": row[\"aqicn_url\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# # Build location dict\n",
    "# for _, row in existing_aq_data[[\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\", \"aqicn_url\"]].drop_duplicates(subset=[\"sensor_id\"]).iterrows():\n",
    "#     sensor_locations[row[\"sensor_id\"]] = (\n",
    "#         row[\"latitude\"], \n",
    "#         row[\"longitude\"], \n",
    "#         row[\"city\"], \n",
    "#         row[\"street\"], \n",
    "#         row[\"country\"],\n",
    "#         row[\"aqicn_url\"]\n",
    "#     )\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.4. Data Collection\n",
    "Fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "### 2.4.1. Load Historical Air Quality Data (Last 4 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 103 sensor locations.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (5.18s) \n"
     ]
    }
   ],
   "source": [
    "print(f\"üîç Processing {len(sensor_locations)} sensor locations.\")\n",
    "historical_start = today - timedelta(days=4)\n",
    "\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt) \n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(sensor_locations.keys())]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "### 2.4.2. Identify Missing Dates for Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (7.75s) \n",
      "üìÖ Missing dates to backfill: 2026-01-19, 2026-01-20, 2026-01-21, 2026-01-22, 2026-01-23, 2026-01-24, 2026-01-25, 2026-01-26\n"
     ]
    }
   ],
   "source": [
    "existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()\n",
    "\n",
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # Check last 7 days for missing data\n",
    "\n",
    "expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "missing_dates = [d for d in expected_dates if d not in existing_dates]\n",
    "\n",
    "# print(f\"üìÖ Missing dates to backfill: {missing_dates}\")\n",
    "formatted = \", \".join(d.isoformat() for d in missing_dates)\n",
    "print(f\"üìÖ Missing dates to backfill: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "### 2.4.3. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (8.26s) \n"
     ]
    }
   ],
   "source": [
    "# Prepare historical data window\n",
    "historical_cutoff = pd.to_datetime(min(missing_dates)) - pd.Timedelta(days=3)\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "historical = historical [historical[\"date\"] >= historical_cutoff]\n",
    "\n",
    "# Track existing sensor-date pairs\n",
    "existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))\n",
    "\n",
    "# Initialize data containers\n",
    "all_aq_rows = [historical]\n",
    "all_weather_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "### 2.4.4. Fetch Missing Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc65830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching air quality for sensor 60853, 1/103\n",
      "Fetching air quality for sensor 59497, 2/103\n",
      "Fetching air quality for sensor 59650, 3/103\n",
      "Fetching air quality for sensor 112672, 4/103\n",
      "Fetching air quality for sensor 60889, 5/103\n",
      "Fetching air quality for sensor 60076, 6/103\n",
      "Fetching air quality for sensor 58921, 7/103\n",
      "Fetching air quality for sensor 84085, 8/103\n",
      "Fetching air quality for sensor 89584, 9/103\n",
      "Fetching air quality for sensor 198559, 10/103\n",
      "Fetching air quality for sensor 149242, 11/103\n",
      "Fetching air quality for sensor 105325, 12/103\n",
      "Fetching air quality for sensor 78529, 13/103\n",
      "Fetching air quality for sensor 88876, 14/103\n",
      "Fetching air quality for sensor 65272, 15/103\n",
      "Fetching air quality for sensor 77488, 16/103\n",
      "Fetching air quality for sensor 351115, 17/103\n",
      "Fetching air quality for sensor 122302, 18/103\n",
      "Fetching air quality for sensor 196735, 19/103\n",
      "Fetching air quality for sensor 69724, 20/103\n",
      "Fetching air quality for sensor 60859, 21/103\n",
      "Fetching air quality for sensor 65146, 22/103\n",
      "Fetching air quality for sensor 57421, 23/103\n",
      "Fetching air quality for sensor 194215, 24/103\n",
      "Fetching air quality for sensor 82384, 25/103\n",
      "Fetching air quality for sensor 180187, 26/103\n",
      "Fetching air quality for sensor 68167, 27/103\n",
      "Fetching air quality for sensor 129124, 28/103\n",
      "Fetching air quality for sensor 79999, 29/103\n",
      "Fetching air quality for sensor 59593, 30/103\n",
      "Fetching air quality for sensor 462457, 31/103\n",
      "Fetching air quality for sensor 417595, 32/103\n",
      "Fetching air quality for sensor 59410, 33/103\n",
      "Fetching air quality for sensor 249862, 34/103\n",
      "Fetching air quality for sensor 345007, 35/103\n",
      "Fetching air quality for sensor 128095, 36/103\n",
      "Fetching air quality for sensor 70564, 37/103\n",
      "Fetching air quality for sensor 63637, 38/103\n",
      "Fetching air quality for sensor 65104, 39/103\n",
      "Fetching air quality for sensor 65290, 40/103\n",
      "Fetching air quality for sensor 252352, 41/103\n",
      "Fetching air quality for sensor 60535, 42/103\n",
      "Fetching air quality for sensor 79750, 43/103\n",
      "Fetching air quality for sensor 58912, 44/103\n",
      "Fetching air quality for sensor 415030, 45/103\n",
      "Fetching air quality for sensor 65284, 46/103\n",
      "Fetching air quality for sensor 107110, 47/103\n",
      "Fetching air quality for sensor 90676, 48/103\n",
      "Fetching air quality for sensor 163156, 49/103\n",
      "Fetching air quality for sensor 59893, 50/103\n",
      "Fetching air quality for sensor 121810, 51/103\n",
      "Fetching air quality for sensor 60541, 52/103\n",
      "Fetching air quality for sensor 60886, 53/103\n",
      "Fetching air quality for sensor 77446, 54/103\n",
      "Fetching air quality for sensor 59095, 55/103\n",
      "Fetching air quality for sensor 88372, 56/103\n",
      "Fetching air quality for sensor 62566, 57/103\n",
      "Fetching air quality for sensor 494275, 58/103\n",
      "Fetching air quality for sensor 61867, 59/103\n",
      "Fetching air quality for sensor 376954, 60/103\n",
      "Fetching air quality for sensor 191047, 61/103\n",
      "Fetching air quality for sensor 59656, 62/103\n",
      "Fetching air quality for sensor 62848, 63/103\n",
      "Fetching air quality for sensor 407335, 64/103\n",
      "Fetching air quality for sensor 87319, 65/103\n",
      "Fetching air quality for sensor 420664, 66/103\n",
      "Fetching air quality for sensor 409513, 67/103\n",
      "Fetching air quality for sensor 78532, 68/103\n",
      "Fetching air quality for sensor 80773, 69/103\n",
      "Fetching air quality for sensor 250030, 70/103\n",
      "Fetching air quality for sensor 76915, 71/103\n",
      "Fetching air quality for sensor 61714, 72/103\n",
      "Fetching air quality for sensor 69628, 73/103\n",
      "Fetching air quality for sensor 476353, 74/103\n",
      "Fetching air quality for sensor 92683, 75/103\n",
      "Fetching air quality for sensor 112993, 76/103\n",
      "Fetching air quality for sensor 82942, 77/103\n",
      "Fetching air quality for sensor 58909, 78/103\n",
      "Fetching air quality for sensor 60838, 79/103\n",
      "Fetching air quality for sensor 192520, 80/103\n",
      "Fetching air quality for sensor 81505, 81/103\n",
      "Fetching air quality for sensor 65707, 82/103\n",
      "Fetching air quality for sensor 59887, 83/103\n",
      "Fetching air quality for sensor 63646, 84/103\n",
      "Fetching air quality for sensor 59356, 85/103\n",
      "Fetching air quality for sensor 60073, 86/103\n",
      "Fetching air quality for sensor 61045, 87/103\n",
      "Fetching air quality for sensor 61861, 88/103\n",
      "Fetching air quality for sensor 154549, 89/103\n",
      "Fetching air quality for sensor 61420, 90/103\n",
      "Fetching air quality for sensor 404209, 91/103\n",
      "Fetching air quality for sensor 59899, 92/103\n",
      "Fetching air quality for sensor 533086, 93/103\n",
      "Fetching air quality for sensor 113542, 94/103\n",
      "Fetching air quality for sensor 208483, 95/103\n",
      "Fetching air quality for sensor 62968, 96/103\n",
      "Fetching air quality for sensor 474841, 97/103\n",
      "Fetching air quality for sensor 113539, 98/103\n",
      "Fetching air quality for sensor 497266, 99/103\n",
      "Fetching air quality for sensor 58666, 100/103\n",
      "Fetching air quality for sensor 401314, 101/103\n",
      "Fetching air quality for sensor 562600, 102/103\n",
      "Fetching air quality for sensor 556792, 103/103\n",
      "üìä Collected 825 air quality dataframes\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching air quality for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in missing_dates:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            # aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            # Add metadata\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "\n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Air quality for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_aq_rows)} air quality dataframes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf99689",
   "metadata": {},
   "source": [
    "### 2.4.5. Fetch Missing Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aab9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for sensor 60853, 1/103\n",
      "Fetching weather for sensor 59497, 2/103\n",
      "Fetching weather for sensor 59650, 3/103\n",
      "Fetching weather for sensor 112672, 4/103\n",
      "Fetching weather for sensor 60889, 5/103\n",
      "Fetching weather for sensor 60076, 6/103\n",
      "Fetching weather for sensor 58921, 7/103\n",
      "Fetching weather for sensor 84085, 8/103\n",
      "Fetching weather for sensor 89584, 9/103\n",
      "Fetching weather for sensor 198559, 10/103\n",
      "Fetching weather for sensor 149242, 11/103\n",
      "Fetching weather for sensor 105325, 12/103\n",
      "Fetching weather for sensor 78529, 13/103\n",
      "Fetching weather for sensor 88876, 14/103\n",
      "Fetching weather for sensor 65272, 15/103\n",
      "Fetching weather for sensor 77488, 16/103\n",
      "Fetching weather for sensor 351115, 17/103\n",
      "Fetching weather for sensor 122302, 18/103\n",
      "Fetching weather for sensor 196735, 19/103\n",
      "Fetching weather for sensor 69724, 20/103\n",
      "Fetching weather for sensor 60859, 21/103\n",
      "Fetching weather for sensor 65146, 22/103\n",
      "Fetching weather for sensor 57421, 23/103\n",
      "Fetching weather for sensor 194215, 24/103\n",
      "Fetching weather for sensor 82384, 25/103\n",
      "2026-01-26 10:12:16,934 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.61617&longitude=16.55276&start_date=2026-01-22&end_date=2026-01-28&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 180187, 26/103\n",
      "Fetching weather for sensor 68167, 27/103\n",
      "Fetching weather for sensor 129124, 28/103\n",
      "Fetching weather for sensor 79999, 29/103\n",
      "Fetching weather for sensor 59593, 30/103\n",
      "Fetching weather for sensor 462457, 31/103\n",
      "Fetching weather for sensor 417595, 32/103\n",
      "2026-01-26 10:13:44,872 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=58.21217&longitude=11.41877&start_date=2026-01-19&end_date=2026-01-25&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 59410, 33/103\n",
      "Fetching weather for sensor 249862, 34/103\n",
      "2026-01-26 10:14:26,189 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.51926&longitude=15.03979&start_date=2026-01-25&end_date=2026-01-31&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 345007, 35/103\n",
      "Fetching weather for sensor 128095, 36/103\n",
      "2026-01-26 10:14:55,577 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=57.73333&longitude=13.01667&start_date=2026-01-23&end_date=2026-01-29&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "2026-01-26 10:15:09,394 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=57.73333&longitude=13.01667&start_date=2026-01-26&end_date=2026-02-01&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 70564, 37/103\n",
      "Fetching weather for sensor 63637, 38/103\n",
      "Fetching weather for sensor 65104, 39/103\n",
      "2026-01-26 10:15:49,225 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=58.01667&longitude=11.83333&start_date=2026-01-23&end_date=2026-01-29&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 65290, 40/103\n",
      "Fetching weather for sensor 252352, 41/103\n",
      "Fetching weather for sensor 60535, 42/103\n",
      "Fetching weather for sensor 79750, 43/103\n",
      "Fetching weather for sensor 58912, 44/103\n",
      "2026-01-26 10:16:53,089 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.75&longitude=15.43333&start_date=2026-01-20&end_date=2026-01-26&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 415030, 45/103\n",
      "Fetching weather for sensor 65284, 46/103\n",
      "Fetching weather for sensor 107110, 47/103\n",
      "Fetching weather for sensor 90676, 48/103\n",
      "2026-01-26 10:17:47,975 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=58.15807&longitude=12.30476&start_date=2026-01-19&end_date=2026-01-25&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 163156, 49/103\n",
      "Fetching weather for sensor 59893, 50/103\n",
      "Fetching weather for sensor 121810, 51/103\n",
      "2026-01-26 10:18:32,326 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.63306&longitude=13.73306&start_date=2026-01-19&end_date=2026-01-25&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 60541, 52/103\n",
      "Fetching weather for sensor 60886, 53/103\n",
      "Fetching weather for sensor 77446, 54/103\n",
      "Fetching weather for sensor 59095, 55/103\n",
      "Fetching weather for sensor 88372, 56/103\n",
      "Fetching weather for sensor 62566, 57/103\n",
      "Fetching weather for sensor 494275, 58/103\n",
      "Fetching weather for sensor 61867, 59/103\n",
      "Fetching weather for sensor 376954, 60/103\n",
      "Fetching weather for sensor 191047, 61/103\n",
      "Fetching weather for sensor 59656, 62/103\n",
      "2026-01-26 10:21:03,812 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=55.83928&longitude=13.30393&start_date=2026-01-24&end_date=2026-01-30&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 62848, 63/103\n",
      "Fetching weather for sensor 407335, 64/103\n",
      "Fetching weather for sensor 87319, 65/103\n",
      "Fetching weather for sensor 420664, 66/103\n",
      "Fetching weather for sensor 409513, 67/103\n",
      "Fetching weather for sensor 78532, 68/103\n",
      "2026-01-26 10:22:27,214 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=58.93945&longitude=11.1712&start_date=2026-01-26&end_date=2026-02-01&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 80773, 69/103\n",
      "Fetching weather for sensor 250030, 70/103\n",
      "2026-01-26 10:22:56,560 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.51839&longitude=17.91128&start_date=2026-01-24&end_date=2026-01-30&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 76915, 71/103\n",
      "Fetching weather for sensor 61714, 72/103\n",
      "Fetching weather for sensor 69628, 73/103\n",
      "Fetching weather for sensor 476353, 74/103\n",
      "Fetching weather for sensor 92683, 75/103\n",
      "Fetching weather for sensor 112993, 76/103\n",
      "Fetching weather for sensor 82942, 77/103\n",
      "Fetching weather for sensor 58909, 78/103\n",
      "Fetching weather for sensor 60838, 79/103\n",
      "Fetching weather for sensor 192520, 80/103\n",
      "Fetching weather for sensor 81505, 81/103\n",
      "Fetching weather for sensor 65707, 82/103\n",
      "Fetching weather for sensor 59887, 83/103\n",
      "Fetching weather for sensor 63646, 84/103\n",
      "Fetching weather for sensor 59356, 85/103\n",
      "Fetching weather for sensor 60073, 86/103\n",
      "Fetching weather for sensor 61045, 87/103\n",
      "Fetching weather for sensor 61861, 88/103\n",
      "Fetching weather for sensor 154549, 89/103\n",
      "Fetching weather for sensor 61420, 90/103\n",
      "Fetching weather for sensor 404209, 91/103\n",
      "Fetching weather for sensor 59899, 92/103\n",
      "2026-01-26 10:27:23,151 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=56.42233&longitude=15.54029&start_date=2026-01-20&end_date=2026-01-26&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "Fetching weather for sensor 533086, 93/103\n",
      "Fetching weather for sensor 113542, 94/103\n",
      "Fetching weather for sensor 208483, 95/103\n",
      "Fetching weather for sensor 62968, 96/103\n",
      "Fetching weather for sensor 474841, 97/103\n",
      "Fetching weather for sensor 113539, 98/103\n",
      "Fetching weather for sensor 497266, 99/103\n",
      "Fetching weather for sensor 58666, 100/103\n",
      "Fetching weather for sensor 401314, 101/103\n",
      "Fetching weather for sensor 562600, 102/103\n",
      "Fetching weather for sensor 556792, 103/103\n",
      "üìä Collected 824 weather dataframes\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching weather for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in missing_dates:\n",
    "        try:\n",
    "            weather_df = fetchers.get_weather_forecast(\n",
    "                sensor_id=sensor_id,\n",
    "                latitude=meta[\"latitude\"],\n",
    "                longitude=meta[\"longitude\"],\n",
    "                start_date=day,\n",
    "                end_date=day + timedelta(days=6)\n",
    "            )\n",
    "\n",
    "            if weather_df.empty:\n",
    "                continue\n",
    "\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.normalize()\n",
    "            # weather_df[\"date\"] = (\n",
    "            #     pd.to_datetime(weather_df[\"date\"])\n",
    "            #     .dt.normalize()\n",
    "            #     .dt.tz_localize(None)\n",
    "            # )\n",
    "\n",
    "            all_weather_rows.append(weather_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weather for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_weather_rows)} weather dataframes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca1fa",
   "metadata": {},
   "source": [
    "### 2.4.9. Clean and Align Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping empty or invalid df[0]\n",
      "üìã Cleaned 824 air quality dataframes\n",
      "üìã Using base columns (excluding engineered features): 9 columns\n"
     ]
    }
   ],
   "source": [
    "cleaned_aq_rows = []\n",
    "\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "base_cols = [c for c in historical.columns if c not in engineered_cols]\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(base_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align to base columns only (no engineered features yet)\n",
    "    aligned = df.reindex(columns=base_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(base_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical (for base columns only)\n",
    "    for col in base_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "print(f\"üìã Cleaned {len(cleaned_aq_rows)} air quality dataframes\")\n",
    "print(f\"üìã Using base columns (excluding engineered features): {len(base_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235625a",
   "metadata": {},
   "source": [
    "### 2.4.10. Combine and Clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e893b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è Total weather records: 1442\n",
      "üìÖ Weather date range: 2026-01-19 00:00:00 to 2026-02-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if all_weather_rows:\n",
    "    all_weather = pd.concat(all_weather_rows, ignore_index=True)\n",
    "    all_weather = all_weather.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "    all_weather[\"date\"] = pd.to_datetime(all_weather[\"date\"]).dt.tz_localize(None)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_weather = all_weather.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather records: {len(all_weather)}\")\n",
    "    print(f\"üìÖ Weather date range: {all_weather['date'].min()} to {all_weather['date'].max()}\")\n",
    "else:\n",
    "    all_weather = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No weather data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa784d",
   "metadata": {},
   "source": [
    "## 2.5. Combine Data and Add Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd37f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import feature_engineering\n",
    "\n",
    "# locations = feature_engineering.build_sensor_location_map(df, sensor_locations)\n",
    "# print(\"DEBUG LOCATIONS:\", locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea612675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total records after deduplication: 824\n",
      "üìä Unique sensors: 103\n",
      "üìä Date range: 2026-01-19 00:00:00 to 2026-01-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Drop engineered columns from historical data before combining\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "historical_base = historical.drop(columns=engineered_cols, errors=\"ignore\")\n",
    "\n",
    "# Combine data\n",
    "all_aq = pd.concat([historical_base, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "# Remove duplicates: keep the first occurrence of each sensor_id + date combination\n",
    "all_aq = all_aq.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Total records after deduplication: {len(all_aq)}\")\n",
    "print(f\"üìä Unique sensors: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"üìä Date range: {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "# Add engineered features\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "\n",
    "# Pass sensor_locations dict to nearby sensor feature\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780ad9",
   "metadata": {},
   "source": [
    "## 2.6. Insert Data to Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e42e56",
   "metadata": {},
   "source": [
    "### 2.6.1. Batch Insert Air Quality Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No valid rows for 2026-01-19\n",
      "‚ö†Ô∏è No valid rows for 2026-01-20\n",
      "‚ö†Ô∏è No valid rows for 2026-01-21\n",
      "2026-01-26 10:29:47,630 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:29:56,560 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-22\n",
      "2026-01-26 10:29:56,773 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:30:04,770 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-23\n",
      "2026-01-26 10:30:04,992 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:30:12,291 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-24\n",
      "2026-01-26 10:30:12,497 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:30:20,256 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-25\n",
      "2026-01-26 10:30:20,444 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:30:28,882 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 103 rows for 2026-01-26\n"
     ]
    }
   ],
   "source": [
    "for day in missing_dates:\n",
    "    day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "    day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "    day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "    if not day_rows.empty:\n",
    "        # Convert types to match feature group schema\n",
    "        day_rows = day_rows.astype({\n",
    "            \"sensor_id\": \"int32\",\n",
    "            \"pm25\": \"float64\",\n",
    "            \"pm25_lag_1d\": \"float64\",\n",
    "            \"pm25_lag_2d\": \"float64\",\n",
    "            \"pm25_lag_3d\": \"float64\",\n",
    "            \"pm25_rolling_3d\": \"float64\",\n",
    "            \"pm25_nearby_avg\": \"float64\",\n",
    "            \"city\": \"string\",\n",
    "            \"street\": \"string\",\n",
    "            \"country\": \"string\",\n",
    "            \"aqicn_url\": \"string\",\n",
    "            \"latitude\": \"float64\",\n",
    "            \"longitude\": \"float64\",\n",
    "        })\n",
    "        \n",
    "        # Ensure correct column order\n",
    "        fg_columns = [f.name for f in air_quality_fg.features]\n",
    "        day_rows = day_rows[fg_columns]\n",
    "        \n",
    "        air_quality_fg.insert(day_rows)\n",
    "        print(f\"‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid rows for {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78444036",
   "metadata": {},
   "source": [
    "### 2.6.2. Verify Air Quality Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c2a0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sensor_id       date  pm25  pm25_lag_1d  pm25_rolling_3d  pm25_nearby_avg\n",
      "7        57421 2026-01-26  10.0         10.0             10.0        11.333333\n",
      "15       58666 2026-01-26   7.0          7.0              7.0        10.666667\n",
      "23       58909 2026-01-26  13.0         13.0             13.0        20.666667\n",
      "31       58912 2026-01-26   7.0          7.0              7.0        12.666667\n",
      "39       58921 2026-01-26  13.0         13.0             13.0        11.333333\n",
      "..         ...        ...   ...          ...              ...              ...\n",
      "791     494275 2026-01-26   6.0          6.0              6.0         3.666667\n",
      "799     497266 2026-01-26  12.0         12.0             12.0         5.333333\n",
      "807     533086 2026-01-26  14.0         14.0             14.0        11.333333\n",
      "815     556792 2026-01-26  10.0         10.0             10.0        11.000000\n",
      "823     562600 2026-01-26  13.0         13.0             13.0         7.333333\n",
      "\n",
      "[103 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_rolling_3d\", \"pm25_nearby_avg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea2437cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sensor_id       date  pm25                        city  \\\n",
      "6        57421 2026-01-25  10.0                 Johannehill   \n",
      "14       58666 2026-01-25   7.0                      √Ñngeby   \n",
      "22       58909 2026-01-25  13.0                       Slaka   \n",
      "30       58912 2026-01-25   7.0                    H√§gern√§s   \n",
      "38       58921 2026-01-25  13.0  Skarpn√§cks stadsdelsomr√•de   \n",
      "..         ...        ...   ...                         ...   \n",
      "790     494275 2026-01-25   6.0                      Stavre   \n",
      "798     497266 2026-01-25  12.0                  Skellefte√•   \n",
      "806     533086 2026-01-25  14.0                        Berg   \n",
      "814     556792 2026-01-25  10.0                  Norrk√∂ping   \n",
      "822     562600 2026-01-25  13.0                       Solna   \n",
      "\n",
      "                 street country                            aqicn_url  \\\n",
      "6                  Ubby  Sweden   https://api.waqi.info/feed/A57421/   \n",
      "14        Jupitersv√§gen  Sweden   https://api.waqi.info/feed/A58666/   \n",
      "22        Tr√∂skaregatan  Sweden   https://api.waqi.info/feed/A58909/   \n",
      "30           Radarv√§gen  Sweden   https://api.waqi.info/feed/A58912/   \n",
      "38   Karin Larssons v√§g  Sweden   https://api.waqi.info/feed/A58921/   \n",
      "..                  ...     ...                                  ...   \n",
      "790               Z 565  Sweden  https://api.waqi.info/feed/A494275/   \n",
      "798        Mobackav√§gen  Sweden  https://api.waqi.info/feed/A497266/   \n",
      "806        Bj√∂rnsbacken  Sweden  https://api.waqi.info/feed/A533086/   \n",
      "814        Enebymov√§gen  Sweden  https://api.waqi.info/feed/A556792/   \n",
      "822      Enk√∂pingsv√§gen  Sweden  https://api.waqi.info/feed/A562600/   \n",
      "\n",
      "     latitude  longitude  pm25_rolling_3d  pm25_lag_1d  pm25_lag_2d  \\\n",
      "6    62.00000   15.00000             10.0         10.0         10.0   \n",
      "14   59.98333   17.73333              7.0          7.0          7.0   \n",
      "22   58.36667   15.55000             13.0         13.0         13.0   \n",
      "30   59.75000   15.43333              7.0          7.0          7.0   \n",
      "38   62.00000   15.00000             13.0         13.0         13.0   \n",
      "..        ...        ...              ...          ...          ...   \n",
      "790  63.41667   14.13333              6.0          6.0          6.0   \n",
      "798  64.75067   20.95279             12.0         12.0         12.0   \n",
      "806  62.00000   15.00000             14.0         14.0         14.0   \n",
      "814  58.59419   16.18260             10.0         10.0         10.0   \n",
      "822  59.36004   18.00086             13.0         13.0         13.0   \n",
      "\n",
      "     pm25_lag_3d  pm25_nearby_avg  \n",
      "6           10.0        11.333333  \n",
      "14           7.0        10.666667  \n",
      "22          13.0        20.666667  \n",
      "30           7.0        12.666667  \n",
      "38          13.0        11.333333  \n",
      "..           ...              ...  \n",
      "790          6.0         3.666667  \n",
      "798         12.0         5.333333  \n",
      "806         14.0        11.333333  \n",
      "814         10.0        11.000000  \n",
      "822         13.0         7.333333  \n",
      "\n",
      "[103 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today - timedelta(days=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab35d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging nearby sensor feature:\n",
      "Total sensors in all_aq: 103\n",
      "Total sensors in sensor_locations: 103\n",
      "\n",
      "pm25_lag_1d stats for today:\n",
      "count    103.000000\n",
      "mean      11.883495\n",
      "std       18.600180\n",
      "min        0.000000\n",
      "25%        5.000000\n",
      "50%        8.000000\n",
      "75%       13.000000\n",
      "max      151.000000\n",
      "Name: pm25_lag_1d, dtype: float64\n",
      "\n",
      "üîç Checking sensor 58666:\n",
      "    sensor_id       date  pm25  pm25_lag_1d  pm25_nearby_avg\n",
      "11      58666 2026-01-22   7.0          7.0        10.666667\n",
      "12      58666 2026-01-23   7.0          7.0        10.666667\n",
      "13      58666 2026-01-24   7.0          7.0        10.666667\n",
      "14      58666 2026-01-25   7.0          7.0        10.666667\n",
      "15      58666 2026-01-26   7.0          7.0        10.666667\n",
      "\n",
      "Sensor 58666 is in sensor_locations\n",
      "Lat/Lon: 59.98333, 17.73333\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check nearby sensor calculation\n",
    "print(\"üîç Debugging nearby sensor feature:\")\n",
    "print(f\"Total sensors in all_aq: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"Total sensors in sensor_locations: {len(sensor_locations)}\")\n",
    "\n",
    "# Check if pm25_lag_1d has values\n",
    "lag_stats = all_aq[all_aq['date'].dt.date == today]['pm25_lag_1d'].describe()\n",
    "print(f\"\\npm25_lag_1d stats for today:\")\n",
    "print(lag_stats)\n",
    "\n",
    "# Check one sensor specifically\n",
    "test_sensor = 58666\n",
    "print(f\"\\nüîç Checking sensor {test_sensor}:\")\n",
    "sensor_data = all_aq[all_aq['sensor_id'] == test_sensor].tail(5)\n",
    "print(sensor_data[['sensor_id', 'date', 'pm25', 'pm25_lag_1d', 'pm25_nearby_avg']])\n",
    "\n",
    "# Check if this sensor has neighbors in the locations dict\n",
    "if test_sensor in sensor_locations:\n",
    "    print(f\"\\nSensor {test_sensor} is in sensor_locations\")\n",
    "    print(f\"Lat/Lon: {sensor_locations[test_sensor]['latitude']}, {sensor_locations[test_sensor]['longitude']}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Sensor {test_sensor} NOT in sensor_locations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a9f81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Manually trace the nearby sensor calculation for one sensor\n",
    "# from utils.feature_engineering import build_sensor_location_map, compute_closest_sensors\n",
    "\n",
    "# test_sensor = 58666\n",
    "# locations = build_sensor_location_map(all_aq, sensor_locations)\n",
    "# closest_map = compute_closest_sensors(locations, n_closest=3)\n",
    "\n",
    "# print(f\"üîç Closest sensors to {test_sensor}:\")\n",
    "# neighbors = closest_map.get(test_sensor, [])\n",
    "# print(f\"Neighbors: {neighbors}\")\n",
    "\n",
    "# if neighbors:\n",
    "#     # Get neighbor data\n",
    "#     neighbor_df = all_aq[all_aq['sensor_id'].isin(neighbors)][['date', 'pm25_lag_1d']]\n",
    "#     print(f\"\\nüìä Neighbor data (showing last 10):\")\n",
    "#     print(neighbor_df.tail(10))\n",
    "    \n",
    "#     # Group by date\n",
    "#     neighbor_avg = neighbor_df.groupby('date')['pm25_lag_1d'].mean().reset_index()\n",
    "#     print(f\"\\nüìä Neighbor average by date:\")\n",
    "#     print(neighbor_avg.tail(10))\n",
    "    \n",
    "#     # Get sensor data and merge\n",
    "#     sensor_data = all_aq[all_aq['sensor_id'] == test_sensor]\n",
    "#     print(f\"\\nüìä Sensor {test_sensor} data:\")\n",
    "#     print(sensor_data[['sensor_id', 'date', 'pm25_lag_1d']].tail(5))\n",
    "    \n",
    "#     merged = sensor_data.merge(neighbor_avg, on='date', how='left')\n",
    "#     print(f\"\\nüìä After merge:\")\n",
    "#     print(merged[['sensor_id', 'date', 'pm25_lag_1d_x', 'pm25_lag_1d_y']].tail(5))\n",
    "#     print(f\"\\n‚úÖ Column 'pm25_lag_1d_y' exists: {'pm25_lag_1d_y' in merged.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce47c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import the fixed module\n",
    "# import importlib\n",
    "# importlib.reload(feature_engineering)\n",
    "\n",
    "# # Re-apply the nearby sensor feature with the fixed function\n",
    "# print(\"üîÑ Re-calculating nearby sensor averages with fixed function...\")\n",
    "# all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)\n",
    "\n",
    "# print(\"\\n‚úÖ Re-calculated nearby sensor averages\")\n",
    "# print(\"\\nüìä Sample data for today:\")\n",
    "# sample = all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_nearby_avg\"]].head(10)\n",
    "# print(sample)\n",
    "\n",
    "# print(f\"\\nüìä Nearby avg stats:\")\n",
    "# print(all_aq[\"pm25_nearby_avg\"].describe())\n",
    "# print(f\"\\nüìä Non-null nearby averages: {all_aq['pm25_nearby_avg'].notna().sum()} / {len(all_aq)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a801bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check today's data more thoroughly\n",
    "# today_data = all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_nearby_avg\"]]\n",
    "# print(f\"üìä Today's data ({today}):\")\n",
    "# print(f\"Total records: {len(today_data)}\")\n",
    "# print(f\"Records with nearby_avg: {today_data['pm25_nearby_avg'].notna().sum()}\")\n",
    "# print(f\"Records without nearby_avg: {today_data['pm25_nearby_avg'].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüîç Sensors still missing nearby_avg on today:\")\n",
    "# missing_nearby = today_data[today_data['pm25_nearby_avg'].isna()]\n",
    "# if len(missing_nearby) > 0:\n",
    "#     print(missing_nearby.head(10))\n",
    "#     print(f\"\\n...and {max(0, len(missing_nearby) - 10)} more\")\n",
    "# else:\n",
    "#     print(\"None! All sensors have nearby averages ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "### 2.6.3. Batch Insert Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a667f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:30:29,186 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "‚úÖ Weather batch 1: 100 records (total: 100/1442)\n",
      "2026-01-26 10:30:43,730 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:30:51,137 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/weather_1_offline_fg_materialization/config_1768459788862) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Weather batch 2: 100 records (total: 200/1442)\n",
      "2026-01-26 10:30:51,315 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 3: 100 records (total: 300/1442)\n",
      "2026-01-26 10:30:59,887 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 4: 100 records (total: 400/1442)\n",
      "2026-01-26 10:31:08,669 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 5: 100 records (total: 500/1442)\n",
      "2026-01-26 10:31:17,290 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 6: 100 records (total: 600/1442)\n",
      "2026-01-26 10:31:26,234 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 7: 100 records (total: 700/1442)\n",
      "2026-01-26 10:31:34,793 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 8: 100 records (total: 800/1442)\n",
      "2026-01-26 10:31:43,313 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 9: 100 records (total: 900/1442)\n",
      "2026-01-26 10:31:51,783 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 10: 100 records (total: 1000/1442)\n",
      "2026-01-26 10:32:00,010 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 11: 100 records (total: 1100/1442)\n",
      "2026-01-26 10:32:08,620 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 12: 100 records (total: 1200/1442)\n",
      "2026-01-26 10:32:17,241 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 13: 100 records (total: 1300/1442)\n",
      "2026-01-26 10:32:26,012 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 100/100 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Weather batch 14: 100 records (total: 1400/1442)\n",
      "2026-01-26 10:32:34,581 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 42/42 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "‚úÖ Weather batch 15: 42 records (total: 1442/1442)\n",
      "üå§Ô∏è Total weather inserted: 1442/1442 records\n"
     ]
    }
   ],
   "source": [
    "if not all_weather.empty:\n",
    "    # Convert types to match feature group schema\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    weather_fg_columns = [f.name for f in weather_fg.features]\n",
    "    all_weather = all_weather[weather_fg_columns]\n",
    "    \n",
    "    # Insert in smaller batches to avoid connection issues\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"‚ö†Ô∏è Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather inserted: {total_inserted}/{len(all_weather)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No weather data to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845de6",
   "metadata": {},
   "source": [
    "### 2.6.4. Print Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9844",
   "metadata": {},
   "source": [
    "## 2.7. Inspect Inserted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1986667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Air quality records inserted: 824\n",
      "\n",
      "üìã Sample air quality data:\n",
      "   sensor_id       date  pm25         city street country  \\\n",
      "0      57421 2026-01-19  10.0  Johannehill   Ubby  Sweden   \n",
      "1      57421 2026-01-20  10.0  Johannehill   Ubby  Sweden   \n",
      "2      57421 2026-01-21  10.0  Johannehill   Ubby  Sweden   \n",
      "3      57421 2026-01-22  10.0  Johannehill   Ubby  Sweden   \n",
      "4      57421 2026-01-23  10.0  Johannehill   Ubby  Sweden   \n",
      "\n",
      "                            aqicn_url  latitude  longitude  pm25_rolling_3d  \\\n",
      "0  https://api.waqi.info/feed/A57421/      62.0       15.0              NaN   \n",
      "1  https://api.waqi.info/feed/A57421/      62.0       15.0             10.0   \n",
      "2  https://api.waqi.info/feed/A57421/      62.0       15.0             10.0   \n",
      "3  https://api.waqi.info/feed/A57421/      62.0       15.0             10.0   \n",
      "4  https://api.waqi.info/feed/A57421/      62.0       15.0             10.0   \n",
      "\n",
      "   pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  pm25_nearby_avg  \n",
      "0          NaN          NaN          NaN              NaN  \n",
      "1         10.0          NaN          NaN        11.333333  \n",
      "2         10.0         10.0          NaN        11.333333  \n",
      "3         10.0         10.0         10.0        11.333333  \n",
      "4         10.0         10.0         10.0        11.333333  \n",
      "\n",
      "üîß Air quality data types:\n",
      "sensor_id                   int32\n",
      "date               datetime64[us]\n",
      "pm25                      float64\n",
      "city                       object\n",
      "street                     object\n",
      "country                    object\n",
      "aqicn_url                  object\n",
      "latitude                  float64\n",
      "longitude                 float64\n",
      "pm25_rolling_3d           float64\n",
      "pm25_lag_1d               float64\n",
      "pm25_lag_2d               float64\n",
      "pm25_lag_3d               float64\n",
      "pm25_nearby_avg           float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Date range:\n",
      "From 2026-01-19 00:00:00 to 2026-01-26 00:00:00\n",
      "\n",
      "üå§Ô∏è Weather records inserted: 1442\n",
      "\n",
      "üìã Sample weather data:\n",
      "         date  sensor_id  temperature_2m_mean  precipitation_sum  \\\n",
      "0  2026-01-19      57421            -3.526834                0.0   \n",
      "1  2026-01-20      57421            -4.024750                0.0   \n",
      "3  2026-01-21      57421            -3.328917                1.0   \n",
      "6  2026-01-22      57421            -4.656000                0.3   \n",
      "10 2026-01-23      57421            -7.174750                0.2   \n",
      "\n",
      "    wind_speed_10m_max  wind_direction_10m_dominant  \n",
      "0             6.840000                   184.421982  \n",
      "1             9.720000                     9.830655  \n",
      "3             9.720000                    69.557274  \n",
      "6            10.440001                   104.406693  \n",
      "10            8.640000                   101.867371  \n",
      "\n",
      "üîß Weather data types:\n",
      "date                           datetime64[ns]\n",
      "sensor_id                               int32\n",
      "temperature_2m_mean                   float64\n",
      "precipitation_sum                     float64\n",
      "wind_speed_10m_max                    float64\n",
      "wind_direction_10m_dominant           float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Unique weather dates:\n",
      "<DatetimeArray>\n",
      "['2026-01-19 00:00:00', '2026-01-20 00:00:00', '2026-01-21 00:00:00',\n",
      " '2026-01-22 00:00:00', '2026-01-23 00:00:00', '2026-01-24 00:00:00',\n",
      " '2026-01-25 00:00:00', '2026-01-26 00:00:00', '2026-01-27 00:00:00',\n",
      " '2026-01-28 00:00:00', '2026-01-29 00:00:00', '2026-01-30 00:00:00',\n",
      " '2026-01-31 00:00:00', '2026-02-01 00:00:00']\n",
      "Length: 14, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "if 'all_aq' in locals() and not all_aq.empty:\n",
    "    print(f\"‚úÖ Air quality records inserted: {len(all_aq)}\")\n",
    "    print(\"\\nüìã Sample air quality data:\")\n",
    "    print(all_aq.head())\n",
    "    print(\"\\nüîß Air quality data types:\")\n",
    "    print(all_aq.dtypes)\n",
    "    print(\"\\nüìÖ Date range:\")\n",
    "    print(f\"From {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "if 'all_weather' in locals() and not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è Weather records inserted: {len(all_weather)}\")\n",
    "    print(\"\\nüìã Sample weather data:\")\n",
    "    print(all_weather.head())\n",
    "    print(\"\\nüîß Weather data types:\")\n",
    "    print(all_weather.dtypes)\n",
    "    print(\"\\nüìÖ Unique weather dates:\")\n",
    "    print(all_weather['date'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
