{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6c5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f5bad",
   "metadata": {},
   "source": [
    "### 2.1.2. Load Settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "### 2.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa04a",
   "metadata": {},
   "source": [
    "### 2.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.3. Load Sensor Locations from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Build sensor location dictionary: sensor_id -> (lat, lon, city, street, country, aqicn_url)\n",
    "sensor_locations = {}\n",
    "existing_aq_data = air_quality_fg.read()\n",
    "existing_sensors = set(existing_aq_data[\"sensor_id\"].unique())\n",
    "print(f\"üìã Found {len(existing_sensors)} sensors in feature store\")\n",
    "\n",
    "\n",
    "for _, row in existing_aq_data[\n",
    "    [\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\", \"aqicn_url\"]\n",
    "].drop_duplicates(subset=[\"sensor_id\"]).iterrows():\n",
    "    \n",
    "    sensor_locations[row[\"sensor_id\"]] = {\n",
    "        \"latitude\": row[\"latitude\"],\n",
    "        \"longitude\": row[\"longitude\"],\n",
    "        \"city\": row[\"city\"],\n",
    "        \"street\": row[\"street\"],\n",
    "        \"country\": row[\"country\"],\n",
    "        \"aqicn_url\": row[\"aqicn_url\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# # Build location dict\n",
    "# for _, row in existing_aq_data[[\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\", \"aqicn_url\"]].drop_duplicates(subset=[\"sensor_id\"]).iterrows():\n",
    "#     sensor_locations[row[\"sensor_id\"]] = (\n",
    "#         row[\"latitude\"], \n",
    "#         row[\"longitude\"], \n",
    "#         row[\"city\"], \n",
    "#         row[\"street\"], \n",
    "#         row[\"country\"],\n",
    "#         row[\"aqicn_url\"]\n",
    "#     )\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.4. Data Collection\n",
    "Fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "### 2.4.1. Load Historical Air Quality Data (Last 4 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîç Processing {len(sensor_locations)} sensor locations.\")\n",
    "historical_start = today - timedelta(days=4)\n",
    "\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt) \n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(sensor_locations.keys())]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "### 2.4.2. Identify Missing Dates for Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()\n",
    "\n",
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # Check last 7 days for missing data\n",
    "\n",
    "expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "missing_dates = [d for d in expected_dates if d not in existing_dates]\n",
    "\n",
    "# print(f\"üìÖ Missing dates to backfill: {missing_dates}\")\n",
    "formatted = \", \".join(d.isoformat() for d in missing_dates)\n",
    "print(f\"üìÖ Missing dates to backfill: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "### 2.4.3. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare historical data window\n",
    "historical_cutoff = pd.to_datetime(min(missing_dates)) - pd.Timedelta(days=3)\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "historical = historical [historical[\"date\"] >= historical_cutoff]\n",
    "\n",
    "# Track existing sensor-date pairs\n",
    "existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))\n",
    "\n",
    "# Initialize data containers\n",
    "all_aq_rows = [historical]\n",
    "all_weather_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "### 2.4.4. Fetch Missing Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching air quality for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in missing_dates:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            # aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            # Add metadata\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "\n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Air quality for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_aq_rows)} air quality dataframes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf99689",
   "metadata": {},
   "source": [
    "### 2.4.5. Fetch Missing Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching weather for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in missing_dates:\n",
    "        try:\n",
    "            weather_df = fetchers.get_weather_forecast(\n",
    "                sensor_id=sensor_id,\n",
    "                latitude=meta[\"latitude\"],\n",
    "                longitude=meta[\"longitude\"],\n",
    "                start_date=day,\n",
    "                end_date=day + timedelta(days=6)\n",
    "            )\n",
    "\n",
    "            if weather_df.empty:\n",
    "                continue\n",
    "\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.normalize()\n",
    "            # weather_df[\"date\"] = (\n",
    "            #     pd.to_datetime(weather_df[\"date\"])\n",
    "            #     .dt.normalize()\n",
    "            #     .dt.tz_localize(None)\n",
    "            # )\n",
    "\n",
    "            all_weather_rows.append(weather_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weather for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_weather_rows)} weather dataframes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca1fa",
   "metadata": {},
   "source": [
    "### 2.4.9. Clean and Align Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_aq_rows = []\n",
    "\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "base_cols = [c for c in historical.columns if c not in engineered_cols]\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(base_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align to base columns only (no engineered features yet)\n",
    "    aligned = df.reindex(columns=base_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(base_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical (for base columns only)\n",
    "    for col in base_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "print(f\"üìã Cleaned {len(cleaned_aq_rows)} air quality dataframes\")\n",
    "print(f\"üìã Using base columns (excluding engineered features): {len(base_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235625a",
   "metadata": {},
   "source": [
    "### 2.4.10. Combine and Clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e893b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_weather_rows:\n",
    "    all_weather = pd.concat(all_weather_rows, ignore_index=True)\n",
    "    all_weather = all_weather.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "    all_weather[\"date\"] = pd.to_datetime(all_weather[\"date\"]).dt.tz_localize(None)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_weather = all_weather.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather records: {len(all_weather)}\")\n",
    "    print(f\"üìÖ Weather date range: {all_weather['date'].min()} to {all_weather['date'].max()}\")\n",
    "else:\n",
    "    all_weather = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No weather data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa784d",
   "metadata": {},
   "source": [
    "## 2.5. Combine Data and Add Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd37f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import feature_engineering\n",
    "\n",
    "# locations = feature_engineering.build_sensor_location_map(df, sensor_locations)\n",
    "# print(\"DEBUG LOCATIONS:\", locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea612675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop engineered columns from historical data before combining\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "historical_base = historical.drop(columns=engineered_cols, errors=\"ignore\")\n",
    "\n",
    "# Combine data\n",
    "all_aq = pd.concat([historical_base, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "# Remove duplicates: keep the first occurrence of each sensor_id + date combination\n",
    "all_aq = all_aq.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Total records after deduplication: {len(all_aq)}\")\n",
    "print(f\"üìä Unique sensors: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"üìä Date range: {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "# Add engineered features\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "\n",
    "# Pass sensor_locations dict to nearby sensor feature\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780ad9",
   "metadata": {},
   "source": [
    "## 2.6. Insert Data to Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e42e56",
   "metadata": {},
   "source": [
    "### 2.6.1. Batch Insert Air Quality Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in missing_dates:\n",
    "    day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "    day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "    day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "    if not day_rows.empty:\n",
    "        # Convert types to match feature group schema\n",
    "        day_rows = day_rows.astype({\n",
    "            \"sensor_id\": \"int32\",\n",
    "            \"pm25\": \"float64\",\n",
    "            \"pm25_lag_1d\": \"float64\",\n",
    "            \"pm25_lag_2d\": \"float64\",\n",
    "            \"pm25_lag_3d\": \"float64\",\n",
    "            \"pm25_rolling_3d\": \"float64\",\n",
    "            \"pm25_nearby_avg\": \"float64\",\n",
    "            \"city\": \"string\",\n",
    "            \"street\": \"string\",\n",
    "            \"country\": \"string\",\n",
    "            \"aqicn_url\": \"string\",\n",
    "            \"latitude\": \"float64\",\n",
    "            \"longitude\": \"float64\",\n",
    "        })\n",
    "        \n",
    "        # Ensure correct column order\n",
    "        fg_columns = [f.name for f in air_quality_fg.features]\n",
    "        day_rows = day_rows[fg_columns]\n",
    "        \n",
    "        air_quality_fg.insert(day_rows)\n",
    "        print(f\"‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid rows for {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78444036",
   "metadata": {},
   "source": [
    "### 2.6.2. Verify Air Quality Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_rolling_3d\", \"pm25_nearby_avg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2437cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today - timedelta(days=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab35d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check nearby sensor calculation\n",
    "print(\"üîç Debugging nearby sensor feature:\")\n",
    "print(f\"Total sensors in all_aq: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"Total sensors in sensor_locations: {len(sensor_locations)}\")\n",
    "\n",
    "# Check if pm25_lag_1d has values\n",
    "lag_stats = all_aq[all_aq['date'].dt.date == today]['pm25_lag_1d'].describe()\n",
    "print(f\"\\npm25_lag_1d stats for today:\")\n",
    "print(lag_stats)\n",
    "\n",
    "# Check one sensor specifically\n",
    "test_sensor = 58666\n",
    "print(f\"\\nüîç Checking sensor {test_sensor}:\")\n",
    "sensor_data = all_aq[all_aq['sensor_id'] == test_sensor].tail(5)\n",
    "print(sensor_data[['sensor_id', 'date', 'pm25', 'pm25_lag_1d', 'pm25_nearby_avg']])\n",
    "\n",
    "# Check if this sensor has neighbors in the locations dict\n",
    "if test_sensor in sensor_locations:\n",
    "    print(f\"\\nSensor {test_sensor} is in sensor_locations\")\n",
    "    print(f\"Lat/Lon: {sensor_locations[test_sensor]['latitude']}, {sensor_locations[test_sensor]['longitude']}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Sensor {test_sensor} NOT in sensor_locations!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Manually trace the nearby sensor calculation for one sensor\n",
    "# from utils.feature_engineering import build_sensor_location_map, compute_closest_sensors\n",
    "\n",
    "# test_sensor = 58666\n",
    "# locations = build_sensor_location_map(all_aq, sensor_locations)\n",
    "# closest_map = compute_closest_sensors(locations, n_closest=3)\n",
    "\n",
    "# print(f\"üîç Closest sensors to {test_sensor}:\")\n",
    "# neighbors = closest_map.get(test_sensor, [])\n",
    "# print(f\"Neighbors: {neighbors}\")\n",
    "\n",
    "# if neighbors:\n",
    "#     # Get neighbor data\n",
    "#     neighbor_df = all_aq[all_aq['sensor_id'].isin(neighbors)][['date', 'pm25_lag_1d']]\n",
    "#     print(f\"\\nüìä Neighbor data (showing last 10):\")\n",
    "#     print(neighbor_df.tail(10))\n",
    "    \n",
    "#     # Group by date\n",
    "#     neighbor_avg = neighbor_df.groupby('date')['pm25_lag_1d'].mean().reset_index()\n",
    "#     print(f\"\\nüìä Neighbor average by date:\")\n",
    "#     print(neighbor_avg.tail(10))\n",
    "    \n",
    "#     # Get sensor data and merge\n",
    "#     sensor_data = all_aq[all_aq['sensor_id'] == test_sensor]\n",
    "#     print(f\"\\nüìä Sensor {test_sensor} data:\")\n",
    "#     print(sensor_data[['sensor_id', 'date', 'pm25_lag_1d']].tail(5))\n",
    "    \n",
    "#     merged = sensor_data.merge(neighbor_avg, on='date', how='left')\n",
    "#     print(f\"\\nüìä After merge:\")\n",
    "#     print(merged[['sensor_id', 'date', 'pm25_lag_1d_x', 'pm25_lag_1d_y']].tail(5))\n",
    "#     print(f\"\\n‚úÖ Column 'pm25_lag_1d_y' exists: {'pm25_lag_1d_y' in merged.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import the fixed module\n",
    "# import importlib\n",
    "# importlib.reload(feature_engineering)\n",
    "\n",
    "# # Re-apply the nearby sensor feature with the fixed function\n",
    "# print(\"üîÑ Re-calculating nearby sensor averages with fixed function...\")\n",
    "# all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)\n",
    "\n",
    "# print(\"\\n‚úÖ Re-calculated nearby sensor averages\")\n",
    "# print(\"\\nüìä Sample data for today:\")\n",
    "# sample = all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_nearby_avg\"]].head(10)\n",
    "# print(sample)\n",
    "\n",
    "# print(f\"\\nüìä Nearby avg stats:\")\n",
    "# print(all_aq[\"pm25_nearby_avg\"].describe())\n",
    "# print(f\"\\nüìä Non-null nearby averages: {all_aq['pm25_nearby_avg'].notna().sum()} / {len(all_aq)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a801bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check today's data more thoroughly\n",
    "# today_data = all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_nearby_avg\"]]\n",
    "# print(f\"üìä Today's data ({today}):\")\n",
    "# print(f\"Total records: {len(today_data)}\")\n",
    "# print(f\"Records with nearby_avg: {today_data['pm25_nearby_avg'].notna().sum()}\")\n",
    "# print(f\"Records without nearby_avg: {today_data['pm25_nearby_avg'].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüîç Sensors still missing nearby_avg on today:\")\n",
    "# missing_nearby = today_data[today_data['pm25_nearby_avg'].isna()]\n",
    "# if len(missing_nearby) > 0:\n",
    "#     print(missing_nearby.head(10))\n",
    "#     print(f\"\\n...and {max(0, len(missing_nearby) - 10)} more\")\n",
    "# else:\n",
    "#     print(\"None! All sensors have nearby averages ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "### 2.6.3. Batch Insert Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_weather.empty:\n",
    "    # Convert types to match feature group schema\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    weather_fg_columns = [f.name for f in weather_fg.features]\n",
    "    all_weather = all_weather[weather_fg_columns]\n",
    "    \n",
    "    # Insert in smaller batches to avoid connection issues\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"‚ö†Ô∏è Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather inserted: {total_inserted}/{len(all_weather)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No weather data to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845de6",
   "metadata": {},
   "source": [
    "### 2.6.4. Print Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9844",
   "metadata": {},
   "source": [
    "## 2.7. Inspect Inserted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1986667",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'all_aq' in locals() and not all_aq.empty:\n",
    "    print(f\"‚úÖ Air quality records inserted: {len(all_aq)}\")\n",
    "    print(\"\\nüìã Sample air quality data:\")\n",
    "    print(all_aq.head())\n",
    "    print(\"\\nüîß Air quality data types:\")\n",
    "    print(all_aq.dtypes)\n",
    "    print(\"\\nüìÖ Date range:\")\n",
    "    print(f\"From {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "if 'all_weather' in locals() and not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è Weather records inserted: {len(all_weather)}\")\n",
    "    print(\"\\nüìã Sample weather data:\")\n",
    "    print(all_weather.head())\n",
    "    print(\"\\nüîß Weather data types:\")\n",
    "    print(all_weather.dtypes)\n",
    "    print(\"\\nüìÖ Unique weather dates:\")\n",
    "    print(all_weather['date'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
