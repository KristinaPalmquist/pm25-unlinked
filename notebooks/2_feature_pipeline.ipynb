{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\n",
      "HopsworksSettings initialized!\n",
      "2026-01-16 09:19:12,384 INFO: Initializing external client\n",
      "2026-01-16 09:19:12,384 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-16 09:19:25,531 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError  \n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "from confluent_kafka import KafkaException\n",
    "import numpy as np\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings()\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository exists at c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010e645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.3. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.4. Load Metadata from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.99s) \n",
      "üìç Loaded metadata for 103 sensors\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Extract unique sensor metadata\n",
    "metadata_df = aq_data[[\"sensor_id\", \"latitude\", \"longitude\", \"city\", \"street\", \"country\", \"aqicn_url\"]].drop_duplicates(subset=[\"sensor_id\"])\n",
    "print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "metadata_df = metadata_df.set_index(\"sensor_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.5. Data Collection\n",
    "Loop through all sensors to fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8e84",
   "metadata": {},
   "source": [
    "Create a copy of dataframe and set up counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616e383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 103 sensor locations.\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from feature group for nearby sensor calculations\n",
    "metadata_indexed = metadata_df.copy()\n",
    "metadata_indexed.index = metadata_indexed.index.astype(int)\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "\n",
    "print(f\"üîç Processing {len(metadata_indexed)} sensor locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "Load historical Air Quality data for all sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (8.49s) \n"
     ]
    }
   ],
   "source": [
    "historical_start = today - timedelta(days=4)\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        # Include TODAY in historical data (we'll filter it out later per sensor)\n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt)  # Changed < to <=\n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(metadata_indexed.index)]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "Initialize containers for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f41259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_list = []\n",
    "weather_dict = {}  # sensor_id -> weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc25fcc",
   "metadata": {},
   "source": [
    "Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b8037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (15.16s) \n"
     ]
    }
   ],
   "source": [
    "existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d51a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # or however far back you want to check\n",
    "\n",
    "expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "missing_dates = [d for d in expected_dates if d not in existing_dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "Load historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (14.98s) \n"
     ]
    }
   ],
   "source": [
    "historical_cutoff = pd.to_datetime(min(missing_dates)) - pd.Timedelta(days=3)\n",
    "# historical_cutoff = min(missing_dates) - timedelta(days=3)\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "historical = historical [historical[\"date\"] >= historical_cutoff]\n",
    "# historical = historical[historical[\"date\"] >= pd.to_datetime(historical_cutoff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807bdec",
   "metadata": {},
   "source": [
    "Track existing sensor-date pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c5bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c2227",
   "metadata": {},
   "source": [
    "Add historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa013b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aq_rows = [historical]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "Fetch missing sensor-date combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb52a07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sensor 60853, 1/103\n",
      "Processing sensor 59497, 2/103\n",
      "Processing sensor 59650, 3/103\n",
      "Processing sensor 112672, 4/103\n",
      "Processing sensor 60889, 5/103\n",
      "Processing sensor 60076, 6/103\n",
      "Processing sensor 58921, 7/103\n",
      "Processing sensor 84085, 8/103\n",
      "Processing sensor 89584, 9/103\n",
      "Processing sensor 198559, 10/103\n",
      "Processing sensor 149242, 11/103\n",
      "Processing sensor 105325, 12/103\n",
      "Processing sensor 78529, 13/103\n",
      "Processing sensor 88876, 14/103\n",
      "Processing sensor 65272, 15/103\n",
      "Processing sensor 77488, 16/103\n",
      "Processing sensor 351115, 17/103\n",
      "Processing sensor 122302, 18/103\n",
      "Processing sensor 196735, 19/103\n",
      "Processing sensor 69724, 20/103\n",
      "Processing sensor 60859, 21/103\n",
      "Processing sensor 65146, 22/103\n",
      "Processing sensor 57421, 23/103\n",
      "Processing sensor 194215, 24/103\n",
      "Processing sensor 82384, 25/103\n",
      "Processing sensor 180187, 26/103\n",
      "Processing sensor 68167, 27/103\n",
      "Processing sensor 129124, 28/103\n",
      "Processing sensor 79999, 29/103\n",
      "Processing sensor 59593, 30/103\n",
      "Processing sensor 462457, 31/103\n",
      "Processing sensor 417595, 32/103\n",
      "Processing sensor 59410, 33/103\n",
      "Processing sensor 249862, 34/103\n",
      "Processing sensor 345007, 35/103\n",
      "Processing sensor 128095, 36/103\n",
      "Processing sensor 70564, 37/103\n",
      "Processing sensor 63637, 38/103\n",
      "Processing sensor 65104, 39/103\n",
      "Processing sensor 65290, 40/103\n",
      "Processing sensor 252352, 41/103\n",
      "Processing sensor 60535, 42/103\n",
      "Processing sensor 79750, 43/103\n",
      "Processing sensor 58912, 44/103\n",
      "Processing sensor 415030, 45/103\n",
      "Processing sensor 65284, 46/103\n",
      "Processing sensor 107110, 47/103\n",
      "Processing sensor 90676, 48/103\n",
      "Processing sensor 163156, 49/103\n",
      "Processing sensor 59893, 50/103\n",
      "Processing sensor 121810, 51/103\n",
      "Processing sensor 60541, 52/103\n",
      "Processing sensor 60886, 53/103\n",
      "Processing sensor 77446, 54/103\n",
      "Processing sensor 59095, 55/103\n",
      "Processing sensor 88372, 56/103\n",
      "Processing sensor 62566, 57/103\n",
      "Processing sensor 494275, 58/103\n",
      "Processing sensor 61867, 59/103\n",
      "Processing sensor 376954, 60/103\n",
      "Processing sensor 191047, 61/103\n",
      "Processing sensor 59656, 62/103\n",
      "Processing sensor 62848, 63/103\n",
      "Processing sensor 407335, 64/103\n",
      "Processing sensor 87319, 65/103\n",
      "Processing sensor 420664, 66/103\n",
      "Processing sensor 409513, 67/103\n",
      "Processing sensor 78532, 68/103\n",
      "Processing sensor 80773, 69/103\n",
      "Processing sensor 250030, 70/103\n",
      "Processing sensor 76915, 71/103\n",
      "Processing sensor 61714, 72/103\n",
      "Processing sensor 69628, 73/103\n",
      "Processing sensor 476353, 74/103\n",
      "Processing sensor 92683, 75/103\n",
      "Processing sensor 112993, 76/103\n",
      "Processing sensor 82942, 77/103\n",
      "Processing sensor 58909, 78/103\n",
      "Processing sensor 60838, 79/103\n",
      "Processing sensor 192520, 80/103\n",
      "Processing sensor 81505, 81/103\n",
      "Processing sensor 65707, 82/103\n",
      "Processing sensor 59887, 83/103\n",
      "Processing sensor 63646, 84/103\n",
      "Processing sensor 59356, 85/103\n",
      "Processing sensor 60073, 86/103\n",
      "Processing sensor 61045, 87/103\n",
      "Processing sensor 61861, 88/103\n",
      "Processing sensor 154549, 89/103\n",
      "Processing sensor 61420, 90/103\n",
      "Processing sensor 404209, 91/103\n",
      "Processing sensor 59899, 92/103\n",
      "Processing sensor 533086, 93/103\n",
      "Processing sensor 113542, 94/103\n",
      "Processing sensor 208483, 95/103\n",
      "Processing sensor 62968, 96/103\n",
      "Processing sensor 474841, 97/103\n",
      "Processing sensor 113539, 98/103\n",
      "Processing sensor 497266, 99/103\n",
      "Processing sensor 58666, 100/103\n",
      "Processing sensor 401314, 101/103\n",
      "Processing sensor 562600, 102/103\n",
      "Processing sensor 556792, 103/103\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for sensor_id, meta in metadata_df.iterrows():\n",
    "    print(f\"Processing sensor {sensor_id}, {count}/{len(metadata_df)}\")\n",
    "    count += 1\n",
    "    for day in missing_dates:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue  # Already exists in Hopsworks, skip API call\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "            \n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Column names match: True\n"
     ]
    }
   ],
   "source": [
    "cleaned_aq_rows = []\n",
    "expected_cols = historical.columns.tolist()\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(expected_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align columns\n",
    "    aligned = df.reindex(columns=expected_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(expected_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical\n",
    "    for col in expected_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "# print(f\"‚úÖ historical shape: {historical.shape}\")\n",
    "# for i, df in enumerate(cleaned_aq_rows):\n",
    "#     print(f\"‚úÖ cleaned_aq_rows[{i}] shape: {df.shape}\")\n",
    "\n",
    "print(\"üìã Column names match:\", all(df.columns.equals(historical.columns) for df in cleaned_aq_rows))\n",
    "\n",
    "for i, df in enumerate(cleaned_aq_rows):\n",
    "    mismatched = [(col, df[col].dtype, historical[col].dtype)\n",
    "                  for col in df.columns if col in historical.columns and df[col].dtype != historical[col].dtype]\n",
    "    if mismatched:\n",
    "        print(\"üìã Dtype mismatch:\")\n",
    "        print(f\"  df[{i}] mismatches: {mismatched}\")\n",
    "\n",
    "all_aq = pd.concat([historical, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "# Feature engineering\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "metadata_indexed = metadata_indexed.reset_index()\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, metadata_indexed, n_closest=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No valid rows for 2026-01-09\n",
      "2026-01-16 09:38:19,173 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization/executions\n",
      "‚úÖ Inserted 1 rows for 2026-01-10\n",
      "2026-01-16 09:39:01,006 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation failed.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 2/2 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 09:39:23,331 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 2 rows for 2026-01-15\n",
      "2026-01-16 09:39:25,811 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation failed.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 2/2 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 09:39:45,784 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/kristina_titanic/Resources/jobs/air_quality_1_offline_fg_materialization/config_1768459798660) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Inserted 2 rows for 2026-01-16\n"
     ]
    }
   ],
   "source": [
    "for day in missing_dates:\n",
    "    day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "    day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "    day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "    if not day_rows.empty:\n",
    "        # Convert types to match feature group schema\n",
    "        day_rows = day_rows.astype({\n",
    "            \"sensor_id\": \"int32\",\n",
    "            \"pm25\": \"float64\",\n",
    "            \"pm25_lag_1d\": \"float64\",\n",
    "            \"pm25_lag_2d\": \"float64\",\n",
    "            \"pm25_lag_3d\": \"float64\",\n",
    "            \"pm25_rolling_3d\": \"float64\",\n",
    "            \"pm25_nearby_avg\": \"float64\",\n",
    "            \"city\": \"string\",\n",
    "            \"street\": \"string\",\n",
    "            \"country\": \"string\",\n",
    "            \"aqicn_url\": \"string\",\n",
    "            \"latitude\": \"float64\",\n",
    "            \"longitude\": \"float64\",\n",
    "        })\n",
    "        \n",
    "        # Ensure correct column order\n",
    "        fg_columns = [f.name for f in air_quality_fg.features]\n",
    "        day_rows = day_rows[fg_columns]\n",
    "        \n",
    "        air_quality_fg.insert(day_rows)\n",
    "        print(f\"‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid rows for {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8d054",
   "metadata": {},
   "source": [
    "Build a unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50efa10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_aq_rows = []   # raw air quality rows for all sensors\n",
    "# weather_dict = {}  # weather per sensor\n",
    "\n",
    "# for sensor_id, meta in metadata_df.iterrows():\n",
    "#     try:\n",
    "#         # Fetch today's PM2.5\n",
    "#         aq_today_df = fetchers.get_pm25(\n",
    "#             meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "#             meta[\"street\"], today, AQICN_API_KEY\n",
    "#         )\n",
    "\n",
    "#         if aq_today_df.empty or aq_today_df[\"pm25\"].isna().all():\n",
    "#             continue\n",
    "\n",
    "#         # Format\n",
    "#         aq_today_df[\"sensor_id\"] = int(sensor_id)\n",
    "#         aq_today_df[\"pm25\"] = pd.to_numeric(aq_today_df[\"pm25\"], errors=\"coerce\")\n",
    "#         aq_today_df[\"date\"] = pd.to_datetime(aq_today_df[\"date\"]).dt.tz_localize(None)\n",
    "#         aq_today_df = aq_today_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "\n",
    "#         # Add historical rows for this sensor\n",
    "#         if not historical_df.empty:\n",
    "#             hist = historical_df[\n",
    "#                 (historical_df[\"sensor_id\"] == sensor_id) &\n",
    "#                 (historical_df[\"date\"].dt.date < today)\n",
    "#             ]\n",
    "#             if not hist.empty:\n",
    "#                 all_aq_rows.append(hist)\n",
    "\n",
    "#         # Add today's row\n",
    "#         all_aq_rows.append(aq_today_df)\n",
    "\n",
    "#         # Fetch weather once per sensor\n",
    "#         if sensor_id not in weather_dict:\n",
    "#             end_date = today + timedelta(days=7)\n",
    "#             wdf = fetchers.get_weather_forecast(\n",
    "#                 sensor_id, today, end_date, meta[\"latitude\"], meta[\"longitude\"]\n",
    "#             )\n",
    "#             if not wdf.empty:\n",
    "#                 wdf[\"sensor_id\"] = sensor_id\n",
    "#                 wdf[\"date\"] = pd.to_datetime(wdf[\"date\"]).dt.tz_localize(None)\n",
    "#                 weather_dict[sensor_id] = wdf\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16417252",
   "metadata": {},
   "source": [
    "Combine all sensors into one datafram and add engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b66b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all sensors into one dataframe\n",
    "# all_aq = pd.concat(all_aq_rows, ignore_index=True)\n",
    "# all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# # Ensure datetime is clean\n",
    "# all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "# min_date = today - timedelta(days=4)\n",
    "# all_aq = all_aq[all_aq[\"date\"].dt.date >= min_date]\n",
    "\n",
    "# # Apply feature engineering across all sensors\n",
    "# all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "# all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "# all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, metadata_indexed, n_closest=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c2a0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sensor_id       date  pm25  pm25_lag_1d  pm25_rolling_3d  pm25_nearby_avg\n",
      "14       57421 2026-01-16  20.0         17.0        13.666667        40.333333\n",
      "15       57421 2026-01-16  17.0         20.0        19.000000        40.333333\n",
      "19       58666 2026-01-16   7.0          7.0         7.000000              NaN\n",
      "23       58909 2026-01-16  32.0         32.0        32.000000              NaN\n",
      "30       58912 2026-01-16  17.0         18.0        17.666667              NaN\n",
      "..         ...        ...   ...          ...              ...              ...\n",
      "638     494275 2026-01-16   6.0          4.0         4.666667              NaN\n",
      "642     497266 2026-01-16   7.0          7.0         7.000000              NaN\n",
      "646     533086 2026-01-16  46.0         46.0        46.000000              NaN\n",
      "650     556792 2026-01-16  56.0         56.0        56.000000              NaN\n",
      "654     562600 2026-01-16  29.0         29.0        29.000000              NaN\n",
      "\n",
      "[161 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_rolling_3d\", \"pm25_nearby_avg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea2437cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sensor_id       date  pm25  pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  \\\n",
      "12       57421 2026-01-15  20.0          4.0          4.0          4.0   \n",
      "13       57421 2026-01-15  17.0         20.0          4.0          4.0   \n",
      "18       58666 2026-01-15   7.0          7.0          7.0          NaN   \n",
      "22       58909 2026-01-15  32.0         32.0         32.0          NaN   \n",
      "28       58912 2026-01-15  17.0         18.0         17.0         18.0   \n",
      "..         ...        ...   ...          ...          ...          ...   \n",
      "636     494275 2026-01-15   6.0          4.0          6.0          4.0   \n",
      "641     497266 2026-01-15   7.0          7.0          7.0          NaN   \n",
      "645     533086 2026-01-15  46.0         46.0         46.0          NaN   \n",
      "649     556792 2026-01-15  56.0         56.0         56.0          NaN   \n",
      "653     562600 2026-01-15  29.0         29.0         29.0          NaN   \n",
      "\n",
      "     pm25_rolling_3d  pm25_nearby_avg         city          street country  \\\n",
      "12          4.000000        40.333333  Johannehill            Ubby  Sweden   \n",
      "13          9.333333        40.333333  Johannehill            Ubby  Sweden   \n",
      "18          7.000000              NaN       √Ñngeby   Jupitersv√§gen  Sweden   \n",
      "22         32.000000              NaN        Slaka   Tr√∂skaregatan  Sweden   \n",
      "28         17.666667              NaN     H√§gern√§s      Radarv√§gen  Sweden   \n",
      "..               ...              ...          ...             ...     ...   \n",
      "636         4.666667              NaN       Stavre           Z 565  Sweden   \n",
      "641         7.000000              NaN   Skellefte√•    Mobackav√§gen  Sweden   \n",
      "645        46.000000              NaN         Berg    Bj√∂rnsbacken  Sweden   \n",
      "649        56.000000              NaN   Norrk√∂ping    Enebymov√§gen  Sweden   \n",
      "653        29.000000              NaN        Solna  Enk√∂pingsv√§gen  Sweden   \n",
      "\n",
      "                               aqicn_url  latitude  longitude  \n",
      "12    https://api.waqi.info/feed/A57421/  62.00000   15.00000  \n",
      "13    https://api.waqi.info/feed/A57421/  62.00000   15.00000  \n",
      "18    https://api.waqi.info/feed/A58666/  59.98333   17.73333  \n",
      "22    https://api.waqi.info/feed/A58909/  58.36667   15.55000  \n",
      "28    https://api.waqi.info/feed/A58912/  59.75000   15.43333  \n",
      "..                                   ...       ...        ...  \n",
      "636  https://api.waqi.info/feed/A494275/  63.41667   14.13333  \n",
      "641  https://api.waqi.info/feed/A497266/  64.75067   20.95279  \n",
      "645  https://api.waqi.info/feed/A533086/  62.00000   15.00000  \n",
      "649  https://api.waqi.info/feed/A556792/  58.59419   16.18260  \n",
      "653  https://api.waqi.info/feed/A562600/  59.36004   18.00086  \n",
      "\n",
      "[162 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today - timedelta(days=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f6457",
   "metadata": {},
   "source": [
    "Extract todays engineered rows for insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d515eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered rows for today: 2\n"
     ]
    }
   ],
   "source": [
    "today_rows = all_aq[all_aq[\"date\"].dt.date == today].copy()\n",
    "\n",
    "# Drop rows with missing target\n",
    "today_rows = today_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "# Optional: drop rows missing engineered features\n",
    "engineered_cols = [c for c in today_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "today_rows = today_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "print(f\"Engineered rows for today: {len(today_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95d50926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sensor_id, meta in metadata_df.iterrows():\n",
    "#     try:\n",
    "#         # Fetch current air quality\n",
    "#         aq_today_df = fetchers.get_pm25(meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"], \n",
    "#                                        meta[\"street\"], today, AQICN_API_KEY)\n",
    "        \n",
    "#         if aq_today_df.empty or aq_today_df['pm25'].isna().all():\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "        \n",
    "#         # Format air quality data\n",
    "#         aq_today_df[\"sensor_id\"] = int(sensor_id)\n",
    "#         aq_today_df[\"pm25\"] = pd.to_numeric(aq_today_df[\"pm25\"], errors=\"coerce\")\n",
    "#         aq_today_df[\"date\"] = pd.to_datetime(aq_today_df[\"date\"]).dt.tz_localize(None)\n",
    "#         aq_today_df = aq_today_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "        \n",
    "#         # Combine with historical data (last 4 days)\n",
    "#         if not historical_df.empty:\n",
    "#             sensor_historical = historical_df[\n",
    "#                 (historical_df[\"sensor_id\"] == sensor_id) & \n",
    "#                 (historical_df[\"date\"].dt.date < today)\n",
    "#             ]\n",
    "#         else:\n",
    "#             sensor_historical = pd.DataFrame()\n",
    "        \n",
    "#         combined = pd.concat([sensor_historical, aq_today_df], ignore_index=True) if not sensor_historical.empty else aq_today_df\n",
    "#         combined = combined.sort_values(\"date\").reset_index(drop=True)\n",
    "        \n",
    "#         # Add features using historical + todays data\n",
    "#         combined = feature_engineering.add_rolling_window_feature(combined, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#         combined = feature_engineering.add_lagged_features(combined, column=\"pm25\", lags=[1, 2, 3])\n",
    "#         combined = feature_engineering.add_nearby_sensor_feature(combined, metadata_indexed, n_closest=3)\n",
    "        \n",
    "#         # Only filter out future dates if any exist\n",
    "#         combined = combined[combined[\"date\"].dt.date <= today].copy()\n",
    "        \n",
    "#         if combined.empty or combined['pm25'].isna().all():\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "        \n",
    "#         aq_list.append(combined)\n",
    "\n",
    "        \n",
    "#         # Fetch weather for each sensor\n",
    "#         if sensor_id not in weather_dict:\n",
    "#             end_date = today + timedelta(days=7)\n",
    "#             weather_df = fetchers.get_weather_forecast(sensor_id, today, end_date, \n",
    "#                                                       meta[\"latitude\"], meta[\"longitude\"])\n",
    "#             if not weather_df.empty:\n",
    "#                 weather_df[\"sensor_id\"] = sensor_id\n",
    "#                 weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "#                 weather_df = weather_df.dropna(subset=['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max'])\n",
    "#                 weather_dict[sensor_id] = weather_df\n",
    "        \n",
    "#         successful += 1\n",
    "#         if successful % 10 == 0:\n",
    "#             print(f\"‚úÖ Processed {successful}/{len(metadata_df)} sensors\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         failed += 1\n",
    "#         print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}\")\n",
    "#         continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85a2ee",
   "metadata": {},
   "source": [
    "Batch insert Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dea3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aq_list:\n",
    "    all_aq = pd.concat(aq_list, ignore_index=True)\n",
    "    \n",
    "    # Convert types\n",
    "    all_aq = all_aq.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"pm25\": \"float64\",\n",
    "        \"pm25_lag_1d\": \"float64\",\n",
    "        \"pm25_lag_2d\": \"float64\",\n",
    "        \"pm25_lag_3d\": \"float64\",\n",
    "        \"pm25_rolling_3d\": \"float64\",\n",
    "        \"pm25_nearby_avg\": \"float64\",\n",
    "        \"city\": \"string\",\n",
    "        \"street\": \"string\",\n",
    "        \"country\": \"string\",\n",
    "        \"aqicn_url\": \"string\",\n",
    "        \"latitude\": \"float64\",\n",
    "        \"longitude\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    fg_columns = [f.name for f in air_quality_fg.features]\n",
    "    all_aq = all_aq[fg_columns]\n",
    "    \n",
    "    air_quality_fg.insert(all_aq)\n",
    "    print(f\"üìä Inserted {len(all_aq)} air quality records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "Batch insert Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a667f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "if weather_dict:\n",
    "    all_weather = pd.concat(weather_dict.values(), ignore_index=True)\n",
    "    \n",
    "    # Convert types\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Insert in smaller batches\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"‚ö†Ô∏è  Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total inserted: {total_inserted}/{len(all_weather)} weather records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845de6",
   "metadata": {},
   "source": [
    "Print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary: ‚úÖ 0 successful, ‚è≠Ô∏è 0 skipped, ‚ùå 0 failed\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9844",
   "metadata": {},
   "source": [
    "## 2.6. Inspect Inserted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1986667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Air quality records inserted: 655\n",
      "\n",
      "üìã Sample air quality data:\n",
      "   sensor_id       date  pm25  pm25_lag_1d  pm25_lag_2d  pm25_lag_3d  \\\n",
      "0      57421 2026-01-09  20.0          NaN          NaN          NaN   \n",
      "1      57421 2026-01-09  17.0         20.0          NaN          NaN   \n",
      "2      57421 2026-01-10  20.0         17.0         20.0          NaN   \n",
      "3      57421 2026-01-10  17.0         20.0         17.0         20.0   \n",
      "4      57421 2026-01-11   4.0         17.0         20.0         17.0   \n",
      "\n",
      "   pm25_rolling_3d  pm25_nearby_avg         city street country  \\\n",
      "0              NaN        40.000000  Johannehill   Ubby  Sweden   \n",
      "1             20.0        40.000000  Johannehill   Ubby  Sweden   \n",
      "2             18.5        40.333333  Johannehill   Ubby  Sweden   \n",
      "3             19.0        40.333333  Johannehill   Ubby  Sweden   \n",
      "4             18.0              NaN  Johannehill   Ubby  Sweden   \n",
      "\n",
      "                            aqicn_url  latitude  longitude  \n",
      "0  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "1  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "2  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "3  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "4  https://api.waqi.info/feed/A57421/      62.0       15.0  \n",
      "\n",
      "üîß Air quality data types:\n",
      "sensor_id                   int32\n",
      "date               datetime64[us]\n",
      "pm25                      float64\n",
      "pm25_lag_1d               float64\n",
      "pm25_lag_2d               float64\n",
      "pm25_lag_3d               float64\n",
      "pm25_rolling_3d           float64\n",
      "pm25_nearby_avg           float64\n",
      "city                       object\n",
      "street                     object\n",
      "country                    object\n",
      "aqicn_url                  object\n",
      "latitude                  float64\n",
      "longitude                 float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Date range:\n",
      "From 2026-01-09 00:00:00 to 2026-01-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if 'all_aq' in locals() and not all_aq.empty:\n",
    "    print(f\"‚úÖ Air quality records inserted: {len(all_aq)}\")\n",
    "    print(\"\\nüìã Sample air quality data:\")\n",
    "    print(all_aq.head())\n",
    "    print(\"\\nüîß Air quality data types:\")\n",
    "    print(all_aq.dtypes)\n",
    "    print(\"\\nüìÖ Date range:\")\n",
    "    print(f\"From {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "if 'all_weather' in locals() and not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è Weather records inserted: {len(all_weather)}\")\n",
    "    print(\"\\nüìã Sample weather data:\")\n",
    "    print(all_weather.head())\n",
    "    print(\"\\nüîß Weather data types:\")\n",
    "    print(all_weather.dtypes)\n",
    "    print(\"\\nüìÖ Unique weather dates:\")\n",
    "    print(all_weather['date'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
