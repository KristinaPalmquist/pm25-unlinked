{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-09 15:05:51,279 INFO: Initializing external client\n",
      "2026-01-09 15:05:51,280 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-09 15:05:52,142 WARNING: UserWarning: The installed hopsworks client version 4.1.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:05:53,364 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in repo at c:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010e645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.3. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, sensor_metadata_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.4. Load Metadata from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.76s) \n",
      "üìç Loaded metadata for 103 sensors\n"
     ]
    }
   ],
   "source": [
    "metadata_df = sensor_metadata_fg.read()\n",
    "if len(metadata_df) == 0:\n",
    "    print(\"‚ö†Ô∏è No sensor metadata found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "metadata_df = metadata_df.set_index(\"sensor_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.5. Data Collection\n",
    "Loop through all sensors to fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 103 sensor locations...\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (12.23s) \n",
      "‚úÖ Processed 10/103 sensors\n",
      "‚úÖ Processed 20/103 sensors\n",
      "‚úÖ Processed 30/103 sensors\n",
      "2026-01-09 15:07:36,849 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.2913223&longitude=17.9831579&start_date=2026-01-09&end_date=2026-01-16&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "‚úÖ Processed 40/103 sensors\n",
      "2026-01-09 15:07:48,079 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.2343185&longitude=18.1981961&start_date=2026-01-09&end_date=2026-01-16&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "2026-01-09 15:08:03,354 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=57.6787131&longitude=11.9099134&start_date=2026-01-09&end_date=2026-01-16&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "2026-01-09 15:08:20,087 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=58.3879196&longitude=15.5651872&start_date=2026-01-09&end_date=2026-01-16&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "2026-01-09 15:08:31,516 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=64.7547331&longitude=20.880701&start_date=2026-01-09&end_date=2026-01-16&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "‚úÖ Processed 50/103 sensors\n",
      "‚úÖ Processed 60/103 sensors\n",
      "‚úÖ Processed 70/103 sensors\n",
      "2026-01-09 15:09:22,224 WARNING: Retrying (Retry(total=4, connect=5, read=4, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='api.open-meteo.com', port=443): Read timed out. (read timeout=None)\")': /v1/forecast?latitude=59.6919825&longitude=18.6389019&start_date=2026-01-09&end_date=2026-01-16&daily=temperature_2m_mean&daily=precipitation_sum&daily=wind_speed_10m_max&daily=wind_direction_10m_dominant&timezone=UTC&format=flatbuffers\n",
      "‚úÖ Processed 80/103 sensors\n",
      "‚úÖ Processed 90/103 sensors\n",
      "‚úÖ Processed 100/103 sensors\n",
      "2026-01-09 15:10:27,707 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation failed.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1911228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 103/103 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization/executions\n",
      "üìä Inserted 103 air quality records\n",
      "2026-01-09 15:11:17,678 INFO: \t4 expectation(s) included in expectation_suite.\n",
      "Validation failed.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1893863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 816/816 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "üå§Ô∏è Inserted 816 weather records for 102 locations\n",
      "\n",
      "üìä Summary: ‚úÖ 103 successful, ‚è≠Ô∏è 0 skipped, ‚ùå 0 failed\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from feature group for nearby sensor calculations\n",
    "metadata_indexed = metadata_df.copy()\n",
    "metadata_indexed.index = metadata_indexed.index.astype(int)\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "\n",
    "print(f\"üîç Processing {len(metadata_df)} sensor locations...\")\n",
    "\n",
    "# Get historical data once for all sensors\n",
    "historical_start = today - timedelta(days=4)\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= pd.to_datetime(historical_start)) & \n",
    "            (historical_df[\"date\"] < today_dt)\n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        # Only keep sensors that exist in metadata\n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(metadata_indexed.index)]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()\n",
    "\n",
    "# Collect all air quality and weather data\n",
    "aq_list = []\n",
    "weather_dict = {}  # location_id -> weather_df\n",
    "\n",
    "for sensor_id, meta in metadata_df.iterrows():\n",
    "    try:\n",
    "        # Fetch current air quality\n",
    "        aq_today_df = fetchers.get_pm25(meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"], \n",
    "                                       meta[\"street\"], today, AQICN_API_KEY)\n",
    "        \n",
    "        if aq_today_df.empty or aq_today_df['pm25'].isna().all():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Format air quality data\n",
    "        aq_today_df[\"sensor_id\"] = int(sensor_id)\n",
    "        aq_today_df[\"location_id\"] = int(meta[\"location_id\"])\n",
    "        aq_today_df[\"pm25\"] = pd.to_numeric(aq_today_df[\"pm25\"], errors=\"coerce\")\n",
    "        aq_today_df[\"date\"] = pd.to_datetime(aq_today_df[\"date\"]).dt.tz_localize(None)\n",
    "        aq_today_df = aq_today_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "        \n",
    "        # Combine with historical data\n",
    "        sensor_historical = historical_df[historical_df[\"sensor_id\"] == sensor_id] if not historical_df.empty else pd.DataFrame()\n",
    "        combined = pd.concat([sensor_historical, aq_today_df], ignore_index=True) if not sensor_historical.empty else aq_today_df\n",
    "        combined = combined.sort_values(\"date\").reset_index(drop=True)\n",
    "        \n",
    "        # Add features\n",
    "        combined = feature_engineering.add_rolling_window_feature(combined, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "        combined = feature_engineering.add_lagged_features(combined, column=\"pm25\", lags=[1, 2, 3])\n",
    "        combined = feature_engineering.add_nearby_sensor_feature(combined, metadata_indexed, n_closest=3)\n",
    "        \n",
    "        # Keep only today's data\n",
    "        aq_final = combined[combined[\"date\"].dt.date == today].copy()\n",
    "        aq_final = aq_final.dropna(subset=['pm25'])\n",
    "        \n",
    "        if aq_final.empty:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        aq_list.append(aq_final)\n",
    "        \n",
    "        # Fetch weather for this location (once per location)\n",
    "        location_id = int(meta[\"location_id\"])\n",
    "        if location_id not in weather_dict:\n",
    "            end_date = today + timedelta(days=7)\n",
    "            weather_df = fetchers.get_weather_forecast(location_id, today, end_date, \n",
    "                                                      meta[\"latitude\"], meta[\"longitude\"])\n",
    "            if not weather_df.empty:\n",
    "                weather_df[\"location_id\"] = location_id\n",
    "                weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "                weather_df = weather_df.dropna(subset=['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max'])\n",
    "                weather_dict[location_id] = weather_df\n",
    "        \n",
    "        successful += 1\n",
    "        if successful % 10 == 0:\n",
    "            print(f\"‚úÖ Processed {successful}/{len(metadata_df)} sensors\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}\")\n",
    "        continue\n",
    "\n",
    "# Batch insert all air quality data\n",
    "if aq_list:\n",
    "    all_aq = pd.concat(aq_list, ignore_index=True)\n",
    "    \n",
    "    # Convert types\n",
    "    all_aq = all_aq.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"location_id\": \"int32\",\n",
    "        \"pm25\": \"float64\",\n",
    "        \"pm25_lag_1d\": \"float64\",\n",
    "        \"pm25_lag_2d\": \"float64\",\n",
    "        \"pm25_lag_3d\": \"float64\",\n",
    "        \"pm25_rolling_3d\": \"float64\",\n",
    "        \"pm25_nearby_avg\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    fg_columns = [f.name for f in air_quality_fg.features]\n",
    "    all_aq = all_aq[fg_columns]\n",
    "    \n",
    "    air_quality_fg.insert(all_aq)\n",
    "    print(f\"üìä Inserted {len(all_aq)} air quality records\")\n",
    "\n",
    "# Batch insert all weather data\n",
    "if weather_dict:\n",
    "    all_weather = pd.concat(weather_dict.values(), ignore_index=True)\n",
    "    \n",
    "    # Convert types\n",
    "    all_weather = all_weather.astype({\n",
    "        \"location_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    weather_fg.insert(all_weather)\n",
    "    print(f\"üå§Ô∏è Inserted {len(all_weather)} weather records for {len(weather_dict)} locations\")\n",
    "\n",
    "print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc9f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load metadata from feature group for nearby sensor calculations\n",
    "# metadata_indexed = metadata_df.copy()\n",
    "# # Ensure index is int type to match sensor_id values in data\n",
    "# metadata_indexed.index = metadata_indexed.index.astype(int)\n",
    "\n",
    "# successful_sensors = 0\n",
    "# failed_sensors = 0\n",
    "# skipped_sensors = 0\n",
    "# location_weather_uploaded = set()  # Track which location weather we've already uploaded\n",
    "\n",
    "# print(f\"üîç Processing {len(metadata_df)} sensor locations...\")\n",
    "\n",
    "# # Get historical data once for all sensors (for rolling/lag features)\n",
    "# historical_start = today - timedelta(days=4)\n",
    "# try:\n",
    "#     # Read all data (Python env doesn't support column selection)\n",
    "#     historical_df = air_quality_fg.read()\n",
    "#     if not historical_df.empty:\n",
    "#         historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "#         # Convert today to datetime for comparison\n",
    "#         today_dt = pd.to_datetime(today)\n",
    "#         historical_start_dt = pd.to_datetime(historical_start)\n",
    "#         # Filter in pandas instead\n",
    "#         historical_df = historical_df[\n",
    "#             (historical_df[\"date\"] >= historical_start_dt) & (historical_df[\"date\"] < today_dt)\n",
    "#         ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "#         # IMPORTANT: Only keep historical data for sensors that exist in metadata\n",
    "#         existing_sensor_ids = metadata_indexed.index.tolist()\n",
    "#         historical_df = historical_df[historical_df[\"sensor_id\"].isin(existing_sensor_ids)]\n",
    "#     else:\n",
    "#         historical_df = pd.DataFrame()\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "#     historical_df = pd.DataFrame()\n",
    "\n",
    "# for sensor_id, meta in metadata_df.iterrows():\n",
    "#     max_retries = 3\n",
    "    \n",
    "#     for attempt in range(max_retries):\n",
    "#         try:\n",
    "#             # Fetch current air quality\n",
    "#             aq_today_df = fetchers.get_pm25(meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"], \n",
    "#                                            meta[\"street\"], today, AQICN_API_KEY)\n",
    "            \n",
    "#             # Check if we got data\n",
    "#             if aq_today_df.empty or aq_today_df['pm25'].isna().all():\n",
    "#                 print(f\"‚è≠Ô∏è Sensor {sensor_id}: No AQ data available\")\n",
    "#                 skipped_sensors += 1\n",
    "#                 break\n",
    "            \n",
    "#             # Format air quality data\n",
    "#             aq_today_df = aq_today_df.assign(\n",
    "#                 sensor_id=int(sensor_id),\n",
    "#                 location_id=int(meta[\"location_id\"])\n",
    "#             )\n",
    "#             aq_today_df[\"pm25\"] = pd.to_numeric(aq_today_df[\"pm25\"], errors=\"coerce\").astype(\"float64\")\n",
    "#             aq_today_df[\"date\"] = pd.to_datetime(aq_today_df[\"date\"]).dt.tz_localize(None)\n",
    "#             aq_today_df = aq_today_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "            \n",
    "#             # Combine with historical data for this sensor\n",
    "#             sensor_historical = historical_df[historical_df[\"sensor_id\"] == sensor_id] if not historical_df.empty else pd.DataFrame()\n",
    "#             combined = pd.concat([sensor_historical, aq_today_df], ignore_index=True) if not sensor_historical.empty else aq_today_df\n",
    "            \n",
    "#             # Sort by date for proper lag/rolling calculations\n",
    "#             combined = combined.sort_values(\"date\").reset_index(drop=True)\n",
    "            \n",
    "#             # Add features\n",
    "#             combined = feature_engineering.add_rolling_window_feature(combined, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#             combined = feature_engineering.add_lagged_features(combined, column=\"pm25\", lags=[1, 2, 3])\n",
    "            \n",
    "#             # Add nearby sensor feature - only for this sensor's data\n",
    "#             combined = feature_engineering.add_nearby_sensor_feature(combined, metadata_indexed, n_closest=3)\n",
    "            \n",
    "#             # Keep only today's data\n",
    "#             today_dt = pd.to_datetime(today)\n",
    "#             aq_final = combined[combined[\"date\"].dt.date == today].copy()\n",
    "#             aq_final = aq_final.dropna(subset=['pm25'])\n",
    "            \n",
    "#             if aq_final.empty:\n",
    "#                 print(f\"‚è≠Ô∏è Sensor {sensor_id}: No valid data after processing\")\n",
    "#                 skipped_sensors += 1\n",
    "#                 break\n",
    "            \n",
    "#             # Ensure correct data types and column order\n",
    "#             aq_final[\"sensor_id\"] = aq_final[\"sensor_id\"].astype(\"int32\")\n",
    "#             aq_final[\"location_id\"] = aq_final[\"location_id\"].astype(\"int32\")\n",
    "            \n",
    "#             # Get expected columns from feature group\n",
    "#             fg_columns = [f.name for f in air_quality_fg.features]\n",
    "#             aq_final = aq_final[fg_columns]\n",
    "            \n",
    "#             # Insert air quality data immediately\n",
    "#             air_quality_fg.insert(aq_final)\n",
    "            \n",
    "#             # Fetch and upload weather for this location (if not already done)\n",
    "#             location_id = int(meta[\"location_id\"])\n",
    "#             if location_id not in location_weather_uploaded:\n",
    "#                 end_date = today + timedelta(days=7)\n",
    "#                 weather_df = fetchers.get_weather_forecast(location_id, today, end_date, \n",
    "#                                                           meta[\"latitude\"], meta[\"longitude\"])\n",
    "                \n",
    "#                 if not weather_df.empty:\n",
    "#                     weather_df[\"location_id\"] = int(location_id)\n",
    "#                     weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "#                     weather_df = weather_df.dropna(subset=['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max'])\n",
    "                    \n",
    "#                     # Convert to correct types to match schema\n",
    "#                     weather_df[\"location_id\"] = weather_df[\"location_id\"].astype(\"int32\")\n",
    "#                     weather_df[\"temperature_2m_mean\"] = weather_df[\"temperature_2m_mean\"].astype(\"float64\")\n",
    "#                     weather_df[\"precipitation_sum\"] = weather_df[\"precipitation_sum\"].astype(\"float64\")\n",
    "#                     weather_df[\"wind_speed_10m_max\"] = weather_df[\"wind_speed_10m_max\"].astype(\"float64\")\n",
    "#                     weather_df[\"wind_direction_10m_dominant\"] = weather_df[\"wind_direction_10m_dominant\"].astype(\"float64\")\n",
    "                    \n",
    "#                     weather_fg.insert(weather_df)\n",
    "#                     location_weather_uploaded.add(location_id)\n",
    "            \n",
    "#             successful_sensors += 1\n",
    "#             print(f\"‚úÖ Uploaded sensor {sensor_id} (location {location_id})\")\n",
    "#             break  # Success, exit retry loop\n",
    "            \n",
    "#         except requests.exceptions.Timeout as e:\n",
    "#             if attempt < max_retries - 1:\n",
    "#                 wait_time = (attempt + 1) * 5\n",
    "#                 print(f\"‚ö†Ô∏è  Sensor {sensor_id}: Timeout, retrying in {wait_time}s... (attempt {attempt + 1}/{max_retries})\")\n",
    "#                 time.sleep(wait_time)\n",
    "#             else:\n",
    "#                 failed_sensors += 1\n",
    "#                 print(f\"‚ùå Sensor {sensor_id}: Failed after {max_retries} timeout attempts\")\n",
    "#                 break\n",
    "                \n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             if attempt < max_retries - 1:\n",
    "#                 wait_time = (attempt + 1) * 5\n",
    "#                 print(f\"‚ö†Ô∏è  Sensor {sensor_id}: {type(e).__name__}, retrying in {wait_time}s... (attempt {attempt + 1}/{max_retries})\")\n",
    "#                 time.sleep(wait_time)\n",
    "#             else:\n",
    "#                 failed_sensors += 1\n",
    "#                 print(f\"‚ùå Sensor {sensor_id}: Failed after {max_retries} attempts - {type(e).__name__}\")\n",
    "#                 break\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             failed_sensors += 1\n",
    "#             print(f\"‚ùå Sensor {sensor_id}: Unexpected error - {type(e).__name__}: {str(e)[:100]}\")\n",
    "#             import traceback\n",
    "#             print(f\"   {traceback.format_exc()[:300]}\")\n",
    "#             break\n",
    "    \n",
    "#     # Brief pause between sensors\n",
    "#     time.sleep(0.5)\n",
    "\n",
    "# print(f\"\\nüìä Collection Summary:\")\n",
    "# print(f\"   ‚úÖ Successful: {successful_sensors}\")\n",
    "# print(f\"   ‚è≠Ô∏è Skipped (no data): {skipped_sensors}\")\n",
    "# print(f\"   ‚ùå Failed: {failed_sensors}\")\n",
    "# print(f\"   üå§Ô∏è Weather locations uploaded: {len(location_weather_uploaded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640b7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aq_df = pd.concat(aqs)\n",
    "aq_df = pd.concat(aq_list)\n",
    "aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\").astype(\"float64\")\n",
    "aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5e08d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.87s) \n"
     ]
    }
   ],
   "source": [
    "aq_df = pd.concat(aq_list) if aq_list else pd.DataFrame()\n",
    "if not aq_df.empty:\n",
    "    aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\").astype(\"float64\")\n",
    "    aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "    aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "    # Data quality check 1: Remove rows with missing PM2.5 values\n",
    "    initial_count = len(aq_df)\n",
    "    aq_df = aq_df.dropna(subset=['pm25'])\n",
    "    if len(aq_df) < initial_count:\n",
    "        print(f\"üßπ Removed {initial_count - len(aq_df)} rows with missing PM2.5 values\")\n",
    "\n",
    "# Get historical data for rolling window and lagged features\n",
    "historical_start = today - timedelta(days=4)\n",
    "historical_df = pd.DataFrame()\n",
    "\n",
    "# Read historical data from feature group and filter for the last 4 days\n",
    "try:\n",
    "    # cols = [f.name for f in air_quality_fg.features] \n",
    "    # historical_df = air_quality_fg.read(cols)\n",
    "    historical_df = air_quality_fg.read()\n",
    "    historical_df = historical_df[[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & (historical_df[\"date\"] < today_dt)\n",
    "        ]\n",
    "except Exception as e:\n",
    "    print(f\"Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48edaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m combined_df = combined_df.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m combined_df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     combined_df = \u001b[43mfeature_engineering\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_rolling_window_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_days\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpm25\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpm25_rolling_3d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     combined_df = feature_engineering.add_lagged_features(combined_df, column=\u001b[33m\"\u001b[39m\u001b[33mpm25\u001b[39m\u001b[33m\"\u001b[39m, lags=[\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m])\n\u001b[32m     26\u001b[39m     combined_df = feature_engineering.add_nearby_sensor_feature(combined_df, metadata_df.to_dict(\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m), column=\u001b[33m\"\u001b[39m\u001b[33mpm25_lag_1d\u001b[39m\u001b[33m\"\u001b[39m, n_closest=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\utils\\feature_engineering.py:17\u001b[39m, in \u001b[36madd_rolling_window_feature\u001b[39m\u001b[34m(df, window_days, column, new_column)\u001b[39m\n\u001b[32m     15\u001b[39m df = df.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m df = df.sort_values([\u001b[33m\"\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m]).copy()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_column\u001b[49m\u001b[43m]\u001b[49m = (\n\u001b[32m     18\u001b[39m     df.groupby(\u001b[33m\"\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m       .rolling(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_days\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m, on=\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m)[column]\n\u001b[32m     20\u001b[39m       .mean()\n\u001b[32m     21\u001b[39m       .reset_index(level=\u001b[32m0\u001b[39m, drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4091\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4088\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_array([key], value)\n\u001b[32m   4089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4090\u001b[39m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4091\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4300\u001b[39m, in \u001b[36mDataFrame._set_item\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4291\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4292\u001b[39m \u001b[33;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[32m   4293\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4298\u001b[39m \u001b[33;03m    ensure homogeneity.\u001b[39;00m\n\u001b[32m   4299\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4300\u001b[39m     value, refs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4303\u001b[39m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns\n\u001b[32m   4304\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value.ndim == \u001b[32m1\u001b[39m\n\u001b[32m   4305\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value.dtype, ExtensionDtype)\n\u001b[32m   4306\u001b[39m     ):\n\u001b[32m   4307\u001b[39m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[32m   4308\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.is_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:5036\u001b[39m, in \u001b[36mDataFrame._sanitize_column\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   5034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Series):\n\u001b[32m   5035\u001b[39m         value = Series(value)\n\u001b[32m-> \u001b[39m\u001b[32m5036\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_reindex_for_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5038\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[32m   5039\u001b[39m     com.require_length_match(value, \u001b[38;5;28mself\u001b[39m.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:12309\u001b[39m, in \u001b[36m_reindex_for_setitem\u001b[39m\u001b[34m(value, index)\u001b[39m\n\u001b[32m  12305\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m  12306\u001b[39m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[32m  12307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value.index.is_unique:\n\u001b[32m  12308\u001b[39m         \u001b[38;5;66;03m# duplicate axis\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m12309\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m  12311\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m  12312\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mincompatible index of inserted column with frame index\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m  12313\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m  12314\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reindexed_value, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:12304\u001b[39m, in \u001b[36m_reindex_for_setitem\u001b[39m\u001b[34m(value, index)\u001b[39m\n\u001b[32m  12302\u001b[39m \u001b[38;5;66;03m# GH#4107\u001b[39;00m\n\u001b[32m  12303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m12304\u001b[39m     reindexed_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m._values\n\u001b[32m  12305\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m  12306\u001b[39m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[32m  12307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value.index.is_unique:\n\u001b[32m  12308\u001b[39m         \u001b[38;5;66;03m# duplicate axis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4981\u001b[39m, in \u001b[36mSeries.reindex\u001b[39m\u001b[34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[39m\n\u001b[32m   4964\u001b[39m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[32m   4965\u001b[39m     NDFrame.reindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m   4966\u001b[39m     klass=_shared_doc_kwargs[\u001b[33m\"\u001b[39m\u001b[33mklass\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   4979\u001b[39m     tolerance=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4980\u001b[39m ) -> Series:\n\u001b[32m-> \u001b[39m\u001b[32m4981\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4989\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:5521\u001b[39m, in \u001b[36mNDFrame.reindex\u001b[39m\u001b[34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[39m\n\u001b[32m   5518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reindex_multi(axes, copy, fill_value)\n\u001b[32m   5520\u001b[39m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5521\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5522\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m   5523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mreindex\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:5544\u001b[39m, in \u001b[36mNDFrame._reindex_axes\u001b[39m\u001b[34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[39m\n\u001b[32m   5541\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   5543\u001b[39m ax = \u001b[38;5;28mself\u001b[39m._get_axis(a)\n\u001b[32m-> \u001b[39m\u001b[32m5544\u001b[39m new_index, indexer = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\n\u001b[32m   5546\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5548\u001b[39m axis = \u001b[38;5;28mself\u001b[39m._get_axis_number(a)\n\u001b[32m   5549\u001b[39m obj = obj._reindex_with_indexers(\n\u001b[32m   5550\u001b[39m     {axis: [new_index, indexer]},\n\u001b[32m   5551\u001b[39m     fill_value=fill_value,\n\u001b[32m   5552\u001b[39m     copy=copy,\n\u001b[32m   5553\u001b[39m     allow_dups=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   5554\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\krist\\Documents\\GitHub\\pm25-forecast-openmeteo-aqicn\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4434\u001b[39m, in \u001b[36mIndex.reindex\u001b[39m\u001b[34m(self, target, method, level, limit, tolerance)\u001b[39m\n\u001b[32m   4431\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot handle a non-unique multi-index!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_unique:\n\u001b[32m   4433\u001b[39m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4434\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4435\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4436\u001b[39m     indexer, _ = \u001b[38;5;28mself\u001b[39m.get_indexer_non_unique(target)\n",
      "\u001b[31mValueError\u001b[39m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "# dtype_map = {\n",
    "#     \"sensor_id\": \"int32\",\n",
    "#     \"location_id\": \"int32\",\n",
    "#     \"date\": \"datetime64[ns]\",\n",
    "#     \"pm25\": \"float64\",\n",
    "#     \"pm25_lag_1d\": \"float64\",\n",
    "#     \"pm25_lag_2d\": \"float64\",\n",
    "#     \"pm25_lag_3d\": \"float64\",\n",
    "#     \"pm25_rolling_3d\": \"float64\",\n",
    "#     \"pm25_nearby_avg\": \"float64\",\n",
    "# }\n",
    "\n",
    "# # Cast both dataframes\n",
    "# for col, dtype in dtype_map.items():\n",
    "#     if col in historical_df.columns:\n",
    "#         historical_df[col] = historical_df[col].astype(dtype, errors=\"ignore\")\n",
    "#     if col in aq_df.columns:\n",
    "#         aq_df[col] = aq_df[col].astype(dtype, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# combined_df = pd.concat([historical_df, aq_df], ignore_index=True)\n",
    "# combined_df = combined_df.reset_index(drop=True)\n",
    "# if not combined_df.empty:\n",
    "#     combined_df = feature_engineering.add_rolling_window_feature(combined_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#     combined_df = feature_engineering.add_lagged_features(combined_df, column=\"pm25\", lags=[1, 2, 3])\n",
    "#     combined_df = feature_engineering.add_nearby_sensor_feature(combined_df, metadata_df.to_dict('index'), column=\"pm25_lag_1d\", n_closest=3)\n",
    "    \n",
    "#     # Data quality check 2: Clean up NaNs created by feature engineering\n",
    "#     before_cleaning = len(combined_df[combined_df[\"date\"].dt.date == today])\n",
    "    \n",
    "#     # Only keep today's data and remove rows where essential features are NaN\n",
    "#     aq_df = combined_df[combined_df[\"date\"].dt.date == today].copy()\n",
    "    \n",
    "#     # Remove rows where pm25 is still NaN after all processing\n",
    "#     aq_df = aq_df.dropna(subset=['pm25'])\n",
    "    \n",
    "#     after_cleaning = len(aq_df)\n",
    "#     if before_cleaning > after_cleaning:\n",
    "#         print(f\"üßπ Removed {before_cleaning - after_cleaning} rows with NaN values after feature engineering\")\n",
    "    \n",
    "#     print(f\"üìä Final data quality: {len(aq_df)} clean rows ready for feature store\")\n",
    "# else:\n",
    "#     aq_df = pd.DataFrame()\n",
    "#     print(\"‚ö†Ô∏è  No data available for processing\")\n",
    "# aq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.concat(weathers) if weathers else pd.DataFrame()\n",
    "if not weather_df.empty:\n",
    "    weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "    \n",
    "    # Data quality check 3: Remove rows with missing weather data\n",
    "    initial_weather_count = len(weather_df)\n",
    "    weather_df = weather_df.dropna(subset=['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max'])\n",
    "    \n",
    "    # Convert to float32 to match Hopsworks feature group schema\n",
    "    weather_df[\"temperature_2m_mean\"] = weather_df[\"temperature_2m_mean\"].astype(\"float32\")\n",
    "    weather_df[\"precipitation_sum\"] = weather_df[\"precipitation_sum\"].astype(\"float32\")\n",
    "    weather_df[\"wind_speed_10m_max\"] = weather_df[\"wind_speed_10m_max\"].astype(\"float32\")\n",
    "    weather_df[\"wind_direction_10m_dominant\"] = weather_df[\"wind_direction_10m_dominant\"].astype(\"float32\")\n",
    "    \n",
    "    if len(weather_df) < initial_weather_count:\n",
    "        print(f\"üßπ Removed {initial_weather_count - len(weather_df)} rows with missing weather data\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è  Weather data quality: {len(weather_df)} clean weather rows\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No weather data available\")\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation before inserting to feature store\n",
    "if not aq_df.empty and not weather_df.empty:\n",
    "    print(f\"‚úÖ Inserting {len(aq_df)} air quality rows and {len(weather_df)} weather rows to feature store\")\n",
    "    air_quality_fg.insert(aq_df)\n",
    "    weather_fg.insert(weather_df)\n",
    "    print(\"üìÅ Data successfully inserted to feature store\")\n",
    "else:\n",
    "    if aq_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No clean air quality data to insert\")\n",
    "    if weather_df.empty:\n",
    "        print(\"‚ö†Ô∏è  No clean weather data to insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae941b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aq_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(air_quality_fg.read().dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
