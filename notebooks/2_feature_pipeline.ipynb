{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n",
      "HopsworksSettings initialized!\n",
      "2026-01-14 09:06:04,886 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-14 09:06:04,894 INFO: Initializing external client\n",
      "2026-01-14 09:06:04,895 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-14 09:06:07,709 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError  \n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "from confluent_kafka import KafkaException\n",
    "import numpy as np\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata\n",
    "\n",
    "#  Load settings \n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in repo at c:\\Users\\krist\\Documents\\GitHub\\pm25\\notebooks\\pm25-forecast-openmeteo-aqicn\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "010e645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret created successfully, explore it at https://c.app.hopsworks.ai:443/account/secrets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Secret('AQICN_API_KEY', 'PRIVATE')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today()\n",
    "\n",
    "if settings.AQICN_API_KEY is None:\n",
    "    print(\"AQICN_API_KEY missing.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "try:\n",
    "    secret = secrets.get_secret(\"AQICN_API_KEY\")\n",
    "    if secret is not None:\n",
    "        secret.delete()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "secrets.create_secret(\"AQICN_API_KEY\", AQICN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.3. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, sensor_metadata_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.4. Load Metadata from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.65s) \n",
      "üìç Loaded metadata for 103 sensors\n"
     ]
    }
   ],
   "source": [
    "metadata_df = sensor_metadata_fg.read()\n",
    "\n",
    "if len(metadata_df) == 0:\n",
    "    print(\"‚ö†Ô∏è No sensor metadata found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"üìç Loaded metadata for {len(metadata_df)} sensors\")\n",
    "metadata_df = metadata_df.set_index(\"sensor_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.5. Data Collection\n",
    "Loop through all sensors to fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8e84",
   "metadata": {},
   "source": [
    "Create a copy of dataframe and set up counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "616e383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 103 sensor locations.\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from feature group for nearby sensor calculations\n",
    "metadata_indexed = metadata_df.copy()\n",
    "metadata_indexed.index = metadata_indexed.index.astype(int)\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "\n",
    "print(f\"üîç Processing {len(metadata_indexed)} sensor locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "Load historical Air Quality data for all sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (8.17s) \n"
     ]
    }
   ],
   "source": [
    "historical_start = today - timedelta(days=4)\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        # Include TODAY in historical data (we'll filter it out later per sensor)\n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt)  # Changed < to <=\n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(metadata_indexed.index)]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "Initialize containers for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f41259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_list = []\n",
    "weather_dict = {}  # location_id -> weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc25fcc",
   "metadata": {},
   "source": [
    "Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10b8037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (10.57s) \n"
     ]
    }
   ],
   "source": [
    "existing_dates = air_quality_fg.read()[\"date\"].dt.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63d51a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # or however far back you want to check\n",
    "\n",
    "expected_dates = pd.date_range(start=start_date, end=today, freq=\"D\").date\n",
    "missing_dates = [d for d in expected_dates if d not in existing_dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "Load historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (28.66s) \n"
     ]
    }
   ],
   "source": [
    "historical_cutoff = pd.to_datetime(min(missing_dates)) - pd.Timedelta(days=3)\n",
    "# historical_cutoff = min(missing_dates) - timedelta(days=3)\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "historical = historical [historical[\"date\"] >= historical_cutoff]\n",
    "# historical = historical[historical[\"date\"] >= pd.to_datetime(historical_cutoff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807bdec",
   "metadata": {},
   "source": [
    "Track existing sensor-date pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c5bafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c2227",
   "metadata": {},
   "source": [
    "Add historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa013b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aq_rows = [historical]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "Fetch missing sensor-date combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb52a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_id, meta in metadata_df.iterrows():\n",
    "    for day in missing_dates:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue  # Already exists in Hopsworks, skip API call\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"location_id\"] = int(meta[\"location_id\"])\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df = aq_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Column names match: True\n"
     ]
    }
   ],
   "source": [
    "cleaned_aq_rows = []\n",
    "expected_cols = historical.columns.tolist()\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(expected_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align columns\n",
    "    aligned = df.reindex(columns=expected_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(expected_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical\n",
    "    for col in expected_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "# print(f\"‚úÖ historical shape: {historical.shape}\")\n",
    "# for i, df in enumerate(cleaned_aq_rows):\n",
    "#     print(f\"‚úÖ cleaned_aq_rows[{i}] shape: {df.shape}\")\n",
    "\n",
    "print(\"üìã Column names match:\", all(df.columns.equals(historical.columns) for df in cleaned_aq_rows))\n",
    "\n",
    "for i, df in enumerate(cleaned_aq_rows):\n",
    "    mismatched = [(col, df[col].dtype, historical[col].dtype)\n",
    "                  for col in df.columns if col in historical.columns and df[col].dtype != historical[col].dtype]\n",
    "    if mismatched:\n",
    "        print(\"üìã Dtype mismatch:\")\n",
    "        print(f\"  df[{i}] mismatches: {mismatched}\")\n",
    "\n",
    "all_aq = pd.concat([historical, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "# Feature engineering\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, locations_dict, n_closest=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No valid rows for 2026-01-07\n"
     ]
    }
   ],
   "source": [
    "for day in missing_dates:\n",
    "    day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "    day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "    engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "    day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "    if not day_rows.empty:\n",
    "        air_quality_fg.insert(day_rows)\n",
    "        # air_quality_fg.insert(day_rows, storage=\"offline\")\n",
    "        print(f\"‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid rows for {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8d054",
   "metadata": {},
   "source": [
    "Build a unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50efa10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_aq_rows = []   # raw air quality rows for all sensors\n",
    "# weather_dict = {}  # weather per location\n",
    "\n",
    "# for sensor_id, meta in metadata_df.iterrows():\n",
    "#     try:\n",
    "#         # Fetch today's PM2.5\n",
    "#         aq_today_df = fetchers.get_pm25(\n",
    "#             meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "#             meta[\"street\"], today, AQICN_API_KEY\n",
    "#         )\n",
    "\n",
    "#         if aq_today_df.empty or aq_today_df[\"pm25\"].isna().all():\n",
    "#             continue\n",
    "\n",
    "#         # Format\n",
    "#         aq_today_df[\"sensor_id\"] = int(sensor_id)\n",
    "#         aq_today_df[\"location_id\"] = int(meta[\"location_id\"])\n",
    "#         aq_today_df[\"pm25\"] = pd.to_numeric(aq_today_df[\"pm25\"], errors=\"coerce\")\n",
    "#         aq_today_df[\"date\"] = pd.to_datetime(aq_today_df[\"date\"]).dt.tz_localize(None)\n",
    "#         aq_today_df = aq_today_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "\n",
    "#         # Add historical rows for this sensor\n",
    "#         if not historical_df.empty:\n",
    "#             hist = historical_df[\n",
    "#                 (historical_df[\"sensor_id\"] == sensor_id) &\n",
    "#                 (historical_df[\"date\"].dt.date < today)\n",
    "#             ]\n",
    "#             if not hist.empty:\n",
    "#                 all_aq_rows.append(hist)\n",
    "\n",
    "#         # Add today's row\n",
    "#         all_aq_rows.append(aq_today_df)\n",
    "\n",
    "#         # Fetch weather once per location\n",
    "#         loc_id = int(meta[\"location_id\"])\n",
    "#         if loc_id not in weather_dict:\n",
    "#             end_date = today + timedelta(days=7)\n",
    "#             wdf = fetchers.get_weather_forecast(\n",
    "#                 loc_id, today, end_date, meta[\"latitude\"], meta[\"longitude\"]\n",
    "#             )\n",
    "#             if not wdf.empty:\n",
    "#                 wdf[\"location_id\"] = loc_id\n",
    "#                 wdf[\"date\"] = pd.to_datetime(wdf[\"date\"]).dt.tz_localize(None)\n",
    "#                 weather_dict[loc_id] = wdf\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16417252",
   "metadata": {},
   "source": [
    "Combine all sensors into one datafram and add engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15b66b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine all sensors into one dataframe\n",
    "# all_aq = pd.concat(all_aq_rows, ignore_index=True)\n",
    "# all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# # Ensure datetime is clean\n",
    "# all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "# min_date = today - timedelta(days=4)\n",
    "# all_aq = all_aq[all_aq[\"date\"].dt.date >= min_date]\n",
    "\n",
    "# # Apply feature engineering across all sensors\n",
    "# all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "# all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "# all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, metadata_indexed, n_closest=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c2a0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sensor_id       date  pm25  pm25_lag_1d  pm25_rolling_3d  pm25_nearby_avg\n",
      "17      57421 2026-01-14  30.0         30.0        21.200000              NaN\n",
      "18      57421 2026-01-14  30.0         30.0        22.666667              NaN\n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today][[\"sensor_id\", \"date\", \"pm25\", \"pm25_lag_1d\", \"pm25_rolling_3d\", \"pm25_nearby_avg\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea2437cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sensor_id  location_id       date  pm25  pm25_lag_1d  pm25_lag_2d  \\\n",
      "15      57421           42 2026-01-13  30.0          8.0          8.0   \n",
      "16      57421           42 2026-01-13  30.0         30.0          8.0   \n",
      "\n",
      "    pm25_lag_3d  pm25_rolling_3d  pm25_nearby_avg  \n",
      "15         30.0        21.200000              NaN  \n",
      "16          8.0        22.666667              NaN  \n"
     ]
    }
   ],
   "source": [
    "print(all_aq[all_aq[\"date\"].dt.date == today - timedelta(days=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f6457",
   "metadata": {},
   "source": [
    "Extract todays engineered rows for insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d515eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered rows for today: 0\n"
     ]
    }
   ],
   "source": [
    "today_rows = all_aq[all_aq[\"date\"].dt.date == today].copy()\n",
    "\n",
    "# Drop rows with missing target\n",
    "today_rows = today_rows.dropna(subset=[\"pm25\"])\n",
    "\n",
    "# Optional: drop rows missing engineered features\n",
    "engineered_cols = [c for c in today_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "today_rows = today_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "\n",
    "print(f\"Engineered rows for today: {len(today_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95d50926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sensor_id, meta in metadata_df.iterrows():\n",
    "#     try:\n",
    "#         # Fetch current air quality\n",
    "#         aq_today_df = fetchers.get_pm25(meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"], \n",
    "#                                        meta[\"street\"], today, AQICN_API_KEY)\n",
    "        \n",
    "#         if aq_today_df.empty or aq_today_df['pm25'].isna().all():\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "        \n",
    "#         # Format air quality data\n",
    "#         aq_today_df[\"sensor_id\"] = int(sensor_id)\n",
    "#         aq_today_df[\"location_id\"] = int(meta[\"location_id\"])\n",
    "#         aq_today_df[\"pm25\"] = pd.to_numeric(aq_today_df[\"pm25\"], errors=\"coerce\")\n",
    "#         aq_today_df[\"date\"] = pd.to_datetime(aq_today_df[\"date\"]).dt.tz_localize(None)\n",
    "#         aq_today_df = aq_today_df.drop(columns=[\"url\", \"country\", \"city\", \"street\"], errors=\"ignore\")\n",
    "        \n",
    "#         # Combine with historical data (last 4 days)\n",
    "#         if not historical_df.empty:\n",
    "#             sensor_historical = historical_df[\n",
    "#                 (historical_df[\"sensor_id\"] == sensor_id) & \n",
    "#                 (historical_df[\"date\"].dt.date < today)\n",
    "#             ]\n",
    "#         else:\n",
    "#             sensor_historical = pd.DataFrame()\n",
    "        \n",
    "#         combined = pd.concat([sensor_historical, aq_today_df], ignore_index=True) if not sensor_historical.empty else aq_today_df\n",
    "#         combined = combined.sort_values(\"date\").reset_index(drop=True)\n",
    "        \n",
    "#         # Add features using historical + todays data\n",
    "#         combined = feature_engineering.add_rolling_window_feature(combined, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "#         combined = feature_engineering.add_lagged_features(combined, column=\"pm25\", lags=[1, 2, 3])\n",
    "#         combined = feature_engineering.add_nearby_sensor_feature(combined, metadata_indexed, n_closest=3)\n",
    "        \n",
    "#         # Only filter out future dates if any exist\n",
    "#         combined = combined[combined[\"date\"].dt.date <= today].copy()\n",
    "        \n",
    "#         if combined.empty or combined['pm25'].isna().all():\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "        \n",
    "#         aq_list.append(combined)\n",
    "\n",
    "        \n",
    "#         # Fetch weather for each location\n",
    "#         location_id = int(meta[\"location_id\"])\n",
    "#         if location_id not in weather_dict:\n",
    "#             end_date = today + timedelta(days=7)\n",
    "#             weather_df = fetchers.get_weather_forecast(location_id, today, end_date, \n",
    "#                                                       meta[\"latitude\"], meta[\"longitude\"])\n",
    "#             if not weather_df.empty:\n",
    "#                 weather_df[\"location_id\"] = location_id\n",
    "#                 weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"])\n",
    "#                 weather_df = weather_df.dropna(subset=['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max'])\n",
    "#                 weather_dict[location_id] = weather_df\n",
    "        \n",
    "#         successful += 1\n",
    "#         if successful % 10 == 0:\n",
    "#             print(f\"‚úÖ Processed {successful}/{len(metadata_df)} sensors\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         failed += 1\n",
    "#         print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}\")\n",
    "#         continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85a2ee",
   "metadata": {},
   "source": [
    "Batch insert Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dea3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aq_list:\n",
    "    all_aq = pd.concat(aq_list, ignore_index=True)\n",
    "    \n",
    "    # Convert types\n",
    "    all_aq = all_aq.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"location_id\": \"int32\",\n",
    "        \"pm25\": \"float64\",\n",
    "        \"pm25_lag_1d\": \"float64\",\n",
    "        \"pm25_lag_2d\": \"float64\",\n",
    "        \"pm25_lag_3d\": \"float64\",\n",
    "        \"pm25_rolling_3d\": \"float64\",\n",
    "        \"pm25_nearby_avg\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    fg_columns = [f.name for f in air_quality_fg.features]\n",
    "    all_aq = all_aq[fg_columns]\n",
    "    \n",
    "    air_quality_fg.insert(all_aq)\n",
    "    print(f\"üìä Inserted {len(all_aq)} air quality records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "Batch insert Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a667f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "if weather_dict:\n",
    "    all_weather = pd.concat(weather_dict.values(), ignore_index=True)\n",
    "    \n",
    "    # Convert types\n",
    "    all_weather = all_weather.astype({\n",
    "        \"location_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Insert in smaller batches\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"‚ö†Ô∏è  Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total inserted: {total_inserted}/{len(all_weather)} weather records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845de6",
   "metadata": {},
   "source": [
    "Print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74278aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary: ‚úÖ 0 successful, ‚è≠Ô∏è 0 skipped, ‚ùå 0 failed\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìä Summary: ‚úÖ {successful} successful, ‚è≠Ô∏è {skipped} skipped, ‚ùå {failed} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c9844",
   "metadata": {},
   "source": [
    "## 2.6. Inspect Inserted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1986667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Air quality records inserted: 993\n",
      "\n",
      "üìã Sample air quality data:\n",
      "   sensor_id  location_id                date  pm25  pm25_lag_1d  pm25_lag_2d  \\\n",
      "0      57421           42 2026-01-07 00:00:00  24.0          NaN          NaN   \n",
      "1      57421           42 2026-01-08 18:52:20   8.0         24.0          NaN   \n",
      "2      57421           42 2026-01-08 18:52:20   8.0          8.0         24.0   \n",
      "3      57421           42 2026-01-09 00:00:00   4.0          8.0          8.0   \n",
      "4      57421           42 2026-01-09 00:00:00   4.0          4.0          8.0   \n",
      "\n",
      "   pm25_lag_3d  pm25_rolling_3d  pm25_nearby_avg  \n",
      "0          NaN        24.000000              NaN  \n",
      "1          NaN        16.000000              NaN  \n",
      "2          NaN        13.333333              NaN  \n",
      "3         24.0        11.000000              6.5  \n",
      "4          8.0         9.600000              6.5  \n",
      "\n",
      "üîß Air quality data types:\n",
      "sensor_id                   int32\n",
      "location_id                 int32\n",
      "date               datetime64[us]\n",
      "pm25                      float64\n",
      "pm25_lag_1d               float64\n",
      "pm25_lag_2d               float64\n",
      "pm25_lag_3d               float64\n",
      "pm25_rolling_3d           float64\n",
      "pm25_nearby_avg           float64\n",
      "dtype: object\n",
      "\n",
      "üìÖ Date range:\n",
      "From 2026-01-07 00:00:00 to 2026-01-14 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if 'all_aq' in locals() and not all_aq.empty:\n",
    "    print(f\"‚úÖ Air quality records inserted: {len(all_aq)}\")\n",
    "    print(\"\\nüìã Sample air quality data:\")\n",
    "    print(all_aq.head())\n",
    "    print(\"\\nüîß Air quality data types:\")\n",
    "    print(all_aq.dtypes)\n",
    "    print(\"\\nüìÖ Date range:\")\n",
    "    print(f\"From {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "if 'all_weather' in locals() and not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è Weather records inserted: {len(all_weather)}\")\n",
    "    print(\"\\nüìã Sample weather data:\")\n",
    "    print(all_weather.head())\n",
    "    print(\"\\nüîß Weather data types:\")\n",
    "    print(all_weather.dtypes)\n",
    "    print(\"\\nüìÖ Unique weather dates:\")\n",
    "    print(all_weather['date'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
