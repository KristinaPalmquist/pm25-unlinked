{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6c5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: /Users/kristina/Github/pm25-unlinked\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f5bad",
   "metadata": {},
   "source": [
    "### 2.1.2. Load Settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "‚úÖ Loaded .env from: /Users/kristina/Github/pm25-unlinked/.env\n",
      "üîó Using Hopsworks host: eu-west.cloud.hopsworks.ai\n",
      "üîë API key loaded: vcbtKUr5zSFtHtin.PGB...PIVn8Zeqjj\n",
      "2026-02-19 09:44:57,931 INFO: Initializing external client\n",
      "2026-02-19 09:44:57,932 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2026-02-19 09:44:58,125 WARNING: UserWarning: The installed hopsworks client version 4.6.2 may not be compatible with the connected Hopsworks backend version 4.7.1. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.7) by running 'pip install hopsworks==4.7.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 09:44:59,276 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/19575\n",
      "Environment initialized and Hopsworks connected!\n",
      "pm25_sweden\n"
     ]
    }
   ],
   "source": [
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    env_path = os.path.join(root_dir, '.env')\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"‚úÖ Loaded .env from: {env_path}\")\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "HOPSWORKS_ENDPOINT = os.environ.get('HOPSWORKS_ENDPOINT')\n",
    "if HOPSWORKS_ENDPOINT:\n",
    "    HOPSWORKS_HOST = HOPSWORKS_ENDPOINT.replace('https://', '').replace('http://', '')\n",
    "    print(f\"üîó Using Hopsworks host: {HOPSWORKS_HOST}\")\n",
    "else:\n",
    "    HOPSWORKS_HOST = None\n",
    "    print(\"üîó Using default Hopsworks host\")\n",
    "\n",
    "print(f\"üîë API key loaded: {HOPSWORKS_API_KEY[:20]}...{HOPSWORKS_API_KEY[-10:]}\")\n",
    "\n",
    "if HOPSWORKS_HOST:\n",
    "    project = hopsworks.login(host=HOPSWORKS_HOST, api_key_value=HOPSWORKS_API_KEY)\n",
    "else:\n",
    "    project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n",
    "print(project.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "### 2.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª URL: /Users/kristina/Github/pm25-unlinked\n",
      "   ‚úÖ Configured git remote with authentication for pm25-unlinked\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa04a",
   "metadata": {},
   "source": [
    "### 2.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.3. Load Sensor Locations from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (42.02s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (37.36s) \n",
      "üìç Loaded locations for 102 existing sensors\n"
     ]
    }
   ],
   "source": [
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.4. Data Collection\n",
    "Fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "### 2.4.1. Load Historical Air Quality Data (Last 4 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 102 sensor locations.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (36.60s) \n"
     ]
    }
   ],
   "source": [
    "print(f\"üîç Processing {len(sensor_locations)} sensor locations.\")\n",
    "historical_start = today - timedelta(days=4)\n",
    "\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt) \n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(sensor_locations.keys())]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "### 2.4.2. Identify Missing Dates for Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e64d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (36.34s) \n",
      "üìÖ Feature store date range: 2019-12-09 to 2025-12-19\n",
      "üìÖ Total unique dates in store: 2201\n",
      "\n",
      "üîç Checking for missing dates between 2026-02-12 and 2026-02-19\n",
      "   Expected dates: 8\n",
      "   Existing dates in that range: 0\n",
      "   Missing dates: 8\n",
      "\n",
      "üìÖ Dates to fetch: 2026-02-09, 2026-02-10, 2026-02-11, 2026-02-12, 2026-02-13, 2026-02-14, 2026-02-15, 2026-02-16, 2026-02-17, 2026-02-18, 2026-02-19\n",
      "üìÖ Dates to insert: 2026-02-12, 2026-02-13, 2026-02-14, 2026-02-15, 2026-02-16, 2026-02-17, 2026-02-18, 2026-02-19\n"
     ]
    }
   ],
   "source": [
    "aq_data = air_quality_fg.read()\n",
    "existing_dates = pd.to_datetime(aq_data[\"date\"]).dt.date.unique()\n",
    "\n",
    "# Debug: Show what dates exist in the feature store\n",
    "if len(existing_dates) > 0:\n",
    "    print(f\"üìÖ Feature store date range: {min(existing_dates)} to {max(existing_dates)}\")\n",
    "    print(f\"üìÖ Total unique dates in store: {len(existing_dates)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No dates found in feature store\")\n",
    "\n",
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # Check last 7 days for missing data\n",
    "\n",
    "# Generate expected dates and convert to set for faster lookup\n",
    "expected_dates = set(pd.date_range(start=start_date, end=today, freq=\"D\").date)\n",
    "existing_dates_set = set(existing_dates)\n",
    "\n",
    "# Find missing dates\n",
    "original_missing_dates = sorted(list(expected_dates - existing_dates_set))\n",
    "\n",
    "print(f\"\\nüîç Checking for missing dates between {start_date} and {today}\")\n",
    "print(f\"   Expected dates: {len(expected_dates)}\")\n",
    "print(f\"   Existing dates in that range: {len(expected_dates & existing_dates_set)}\")\n",
    "print(f\"   Missing dates: {len(original_missing_dates)}\")\n",
    "\n",
    "# Separate: dates to fetch vs dates to insert\n",
    "dates_to_insert = original_missing_dates.copy()  # Only insert the actual missing dates\n",
    "dates_to_fetch = original_missing_dates.copy()   # Fetch missing dates + buffer\n",
    "\n",
    "# Add 3 buffer days before first missing date to ensure we can calculate lag features\n",
    "if original_missing_dates:\n",
    "    earliest_missing = min(original_missing_dates)\n",
    "    buffer_dates = [earliest_missing - timedelta(days=i) for i in range(1, 4)]\n",
    "    # Only add buffer dates that aren't already in existing_dates\n",
    "    buffer_dates = [d for d in buffer_dates if d not in existing_dates_set]\n",
    "    dates_to_fetch = sorted(buffer_dates + dates_to_fetch)\n",
    "\n",
    "formatted = \", \".join(d.isoformat() for d in dates_to_fetch) if dates_to_fetch else \"None\"\n",
    "insert_formatted = \", \".join(d.isoformat() for d in dates_to_insert) if dates_to_insert else \"None\"\n",
    "print(f\"\\nüìÖ Dates to fetch: {formatted}\")\n",
    "print(f\"üìÖ Dates to insert: {insert_formatted}\")\n",
    "\n",
    "# Exit early if no missing dates\n",
    "if not dates_to_fetch:\n",
    "    print(\"\\n‚úÖ No missing dates found. Feature store is up to date!\")\n",
    "    print(\"   The feature pipeline will continue without fetching new data.\")\n",
    "    dates_to_insert = []\n",
    "    dates_to_fetch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "### 2.4.3. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (36.97s) \n",
      "\n",
      "üìã Preparing to fetch data for 11 dates\n"
     ]
    }
   ],
   "source": [
    "# Load historical data\n",
    "historical = air_quality_fg.read()\n",
    "historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "# Skip if no dates to fetch\n",
    "if not dates_to_fetch:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipping data preparation - no missing dates\")\n",
    "    all_aq_rows = []\n",
    "    all_weather_rows = []\n",
    "else:\n",
    "    print(f\"\\nüìã Preparing to fetch data for {len(dates_to_fetch)} dates\")\n",
    "    \n",
    "    # Filter historical data to relevant window\n",
    "    historical_cutoff = pd.to_datetime(min(dates_to_fetch)) - pd.Timedelta(days=3)\n",
    "    historical = historical[historical[\"date\"] >= historical_cutoff]\n",
    "\n",
    "    # Track existing sensor-date pairs\n",
    "    existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "    existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "    existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))\n",
    "\n",
    "    # Initialize data containers\n",
    "    all_aq_rows = [historical]\n",
    "    all_weather_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "### 2.4.4. Fetch Missing Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc65830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching air quality for sensor 59899, 1/102\n",
      "Fetching air quality for sensor 105325, 2/102\n",
      "Fetching air quality for sensor 196735, 3/102\n",
      "Fetching air quality for sensor 462457, 4/102\n",
      "Fetching air quality for sensor 61045, 5/102\n",
      "Fetching air quality for sensor 497266, 6/102\n",
      "Fetching air quality for sensor 59593, 7/102\n",
      "Fetching air quality for sensor 62566, 8/102\n",
      "Fetching air quality for sensor 59887, 9/102\n",
      "Fetching air quality for sensor 163156, 10/102\n",
      "Fetching air quality for sensor 107110, 11/102\n",
      "Fetching air quality for sensor 63637, 12/102\n",
      "Fetching air quality for sensor 58666, 13/102\n",
      "Fetching air quality for sensor 88876, 14/102\n",
      "Fetching air quality for sensor 65146, 15/102\n",
      "Fetching air quality for sensor 129124, 16/102\n",
      "Fetching air quality for sensor 59893, 17/102\n",
      "Fetching air quality for sensor 87319, 18/102\n",
      "Fetching air quality for sensor 81505, 19/102\n",
      "Fetching air quality for sensor 113542, 20/102\n",
      "Fetching air quality for sensor 409513, 21/102\n",
      "Fetching air quality for sensor 65707, 22/102\n",
      "Fetching air quality for sensor 59356, 23/102\n",
      "Fetching air quality for sensor 61420, 24/102\n",
      "Fetching air quality for sensor 90676, 25/102\n",
      "Fetching air quality for sensor 404209, 26/102\n",
      "Fetching air quality for sensor 88372, 27/102\n",
      "Fetching air quality for sensor 112993, 28/102\n",
      "Fetching air quality for sensor 60076, 29/102\n",
      "Fetching air quality for sensor 401314, 30/102\n",
      "Fetching air quality for sensor 70564, 31/102\n",
      "Fetching air quality for sensor 376954, 32/102\n",
      "Fetching air quality for sensor 89584, 33/102\n",
      "Fetching air quality for sensor 60838, 34/102\n",
      "Fetching air quality for sensor 208483, 35/102\n",
      "Fetching air quality for sensor 420664, 36/102\n",
      "Fetching air quality for sensor 192520, 37/102\n",
      "Fetching air quality for sensor 113539, 38/102\n",
      "Fetching air quality for sensor 82942, 39/102\n",
      "Fetching air quality for sensor 121810, 40/102\n",
      "Fetching air quality for sensor 249862, 41/102\n",
      "Fetching air quality for sensor 59656, 42/102\n",
      "Fetching air quality for sensor 180187, 43/102\n",
      "Fetching air quality for sensor 79750, 44/102\n",
      "Fetching air quality for sensor 198559, 45/102\n",
      "Fetching air quality for sensor 417595, 46/102\n",
      "Fetching air quality for sensor 59497, 47/102\n",
      "Fetching air quality for sensor 149242, 48/102\n",
      "‚ùå Air quality for sensor 149242 on 2026-02-12: ConnectionError\n",
      "Fetching air quality for sensor 351115, 49/102\n",
      "Fetching air quality for sensor 65290, 50/102\n",
      "Fetching air quality for sensor 58912, 51/102\n",
      "Fetching air quality for sensor 61714, 52/102\n",
      "Fetching air quality for sensor 252352, 53/102\n",
      "Fetching air quality for sensor 128095, 54/102\n",
      "Fetching air quality for sensor 77488, 55/102\n",
      "Fetching air quality for sensor 60535, 56/102\n",
      "Fetching air quality for sensor 250030, 57/102\n",
      "Fetching air quality for sensor 65104, 58/102\n",
      "Fetching air quality for sensor 60889, 59/102\n",
      "Fetching air quality for sensor 59410, 60/102\n",
      "Fetching air quality for sensor 82384, 61/102\n",
      "Fetching air quality for sensor 68167, 62/102\n",
      "Fetching air quality for sensor 494275, 63/102\n",
      "Fetching air quality for sensor 58921, 64/102\n",
      "Fetching air quality for sensor 59650, 65/102\n",
      "Fetching air quality for sensor 122302, 66/102\n",
      "Fetching air quality for sensor 92683, 67/102\n",
      "Fetching air quality for sensor 556792, 68/102\n",
      "Fetching air quality for sensor 60886, 69/102\n",
      "Fetching air quality for sensor 415030, 70/102\n",
      "Fetching air quality for sensor 61861, 71/102\n",
      "Fetching air quality for sensor 345007, 72/102\n",
      "Fetching air quality for sensor 76915, 73/102\n",
      "Fetching air quality for sensor 533086, 74/102\n",
      "Fetching air quality for sensor 84085, 75/102\n",
      "Fetching air quality for sensor 69628, 76/102\n",
      "Fetching air quality for sensor 112672, 77/102\n",
      "Fetching air quality for sensor 78529, 78/102\n",
      "Fetching air quality for sensor 60073, 79/102\n",
      "Fetching air quality for sensor 77446, 80/102\n",
      "Fetching air quality for sensor 476353, 81/102\n",
      "Fetching air quality for sensor 407335, 82/102\n",
      "Fetching air quality for sensor 194215, 83/102\n",
      "Fetching air quality for sensor 60859, 84/102\n",
      "Fetching air quality for sensor 57421, 85/102\n",
      "Fetching air quality for sensor 65272, 86/102\n",
      "Fetching air quality for sensor 154549, 87/102\n",
      "Fetching air quality for sensor 58909, 88/102\n",
      "‚ùå Air quality for sensor 58909 on 2026-02-12: ConnectionError\n",
      "Fetching air quality for sensor 79999, 89/102\n",
      "Fetching air quality for sensor 69724, 90/102\n",
      "Fetching air quality for sensor 62848, 91/102\n",
      "Fetching air quality for sensor 80773, 92/102\n",
      "Fetching air quality for sensor 59095, 93/102\n",
      "Fetching air quality for sensor 60853, 94/102\n",
      "‚ùå Air quality for sensor 60853 on 2026-02-09: ConnectionError\n",
      "Fetching air quality for sensor 63646, 95/102\n",
      "Fetching air quality for sensor 60541, 96/102\n",
      "‚ùå Air quality for sensor 60541 on 2026-02-18: ConnectionError\n",
      "Fetching air quality for sensor 474841, 97/102\n",
      "Fetching air quality for sensor 62968, 98/102\n",
      "Fetching air quality for sensor 562600, 99/102\n",
      "‚ùå Air quality for sensor 562600 on 2026-02-16: ConnectionError\n",
      "Fetching air quality for sensor 65284, 100/102\n",
      "Fetching air quality for sensor 78532, 101/102\n",
      "Fetching air quality for sensor 61867, 102/102\n",
      "üìä Collected 1118 air quality dataframes\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching air quality for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in dates_to_fetch:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            # aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            # Add metadata\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "\n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Air quality for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_aq_rows)} air quality dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf99689",
   "metadata": {},
   "source": [
    "### 2.4.5. Fetch Missing Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aab9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for sensor 59899, 1/102\n",
      "Fetching weather for sensor 105325, 2/102\n",
      "Fetching weather for sensor 196735, 3/102\n",
      "Fetching weather for sensor 462457, 4/102\n",
      "Fetching weather for sensor 61045, 5/102\n",
      "Fetching weather for sensor 497266, 6/102\n",
      "Fetching weather for sensor 59593, 7/102\n",
      "Fetching weather for sensor 62566, 8/102\n",
      "Fetching weather for sensor 59887, 9/102\n",
      "Fetching weather for sensor 163156, 10/102\n",
      "Fetching weather for sensor 107110, 11/102\n",
      "Fetching weather for sensor 63637, 12/102\n",
      "Fetching weather for sensor 58666, 13/102\n",
      "Fetching weather for sensor 88876, 14/102\n",
      "Fetching weather for sensor 65146, 15/102\n",
      "Fetching weather for sensor 129124, 16/102\n",
      "Fetching weather for sensor 59893, 17/102\n",
      "Fetching weather for sensor 87319, 18/102\n",
      "Fetching weather for sensor 81505, 19/102\n",
      "Fetching weather for sensor 113542, 20/102\n",
      "Fetching weather for sensor 409513, 21/102\n",
      "Fetching weather for sensor 65707, 22/102\n",
      "Fetching weather for sensor 59356, 23/102\n",
      "Fetching weather for sensor 61420, 24/102\n",
      "Fetching weather for sensor 90676, 25/102\n",
      "Fetching weather for sensor 404209, 26/102\n",
      "Fetching weather for sensor 88372, 27/102\n",
      "Fetching weather for sensor 112993, 28/102\n",
      "Fetching weather for sensor 60076, 29/102\n",
      "Fetching weather for sensor 401314, 30/102\n",
      "Fetching weather for sensor 70564, 31/102\n",
      "Fetching weather for sensor 376954, 32/102\n",
      "Fetching weather for sensor 89584, 33/102\n",
      "Fetching weather for sensor 60838, 34/102\n",
      "Fetching weather for sensor 208483, 35/102\n",
      "Fetching weather for sensor 420664, 36/102\n",
      "Fetching weather for sensor 192520, 37/102\n",
      "Fetching weather for sensor 113539, 38/102\n",
      "Fetching weather for sensor 82942, 39/102\n",
      "Fetching weather for sensor 121810, 40/102\n",
      "Fetching weather for sensor 249862, 41/102\n",
      "Fetching weather for sensor 59656, 42/102\n",
      "Fetching weather for sensor 180187, 43/102\n",
      "Fetching weather for sensor 79750, 44/102\n",
      "Fetching weather for sensor 198559, 45/102\n",
      "Fetching weather for sensor 417595, 46/102\n",
      "Fetching weather for sensor 59497, 47/102\n",
      "Fetching weather for sensor 149242, 48/102\n",
      "Fetching weather for sensor 351115, 49/102\n",
      "Fetching weather for sensor 65290, 50/102\n",
      "Fetching weather for sensor 58912, 51/102\n",
      "Fetching weather for sensor 61714, 52/102\n",
      "Fetching weather for sensor 252352, 53/102\n",
      "Fetching weather for sensor 128095, 54/102\n",
      "Fetching weather for sensor 77488, 55/102\n",
      "Fetching weather for sensor 60535, 56/102\n",
      "Fetching weather for sensor 250030, 57/102\n",
      "Fetching weather for sensor 65104, 58/102\n",
      "Fetching weather for sensor 60889, 59/102\n",
      "Fetching weather for sensor 59410, 60/102\n",
      "Fetching weather for sensor 82384, 61/102\n",
      "Fetching weather for sensor 68167, 62/102\n",
      "Fetching weather for sensor 494275, 63/102\n",
      "Fetching weather for sensor 58921, 64/102\n",
      "Fetching weather for sensor 59650, 65/102\n",
      "Fetching weather for sensor 122302, 66/102\n",
      "Fetching weather for sensor 92683, 67/102\n",
      "Fetching weather for sensor 556792, 68/102\n",
      "Fetching weather for sensor 60886, 69/102\n",
      "Fetching weather for sensor 415030, 70/102\n",
      "Fetching weather for sensor 61861, 71/102\n",
      "Fetching weather for sensor 345007, 72/102\n",
      "Fetching weather for sensor 76915, 73/102\n",
      "Fetching weather for sensor 533086, 74/102\n",
      "Fetching weather for sensor 84085, 75/102\n",
      "Fetching weather for sensor 69628, 76/102\n",
      "Fetching weather for sensor 112672, 77/102\n",
      "Fetching weather for sensor 78529, 78/102\n",
      "Fetching weather for sensor 60073, 79/102\n",
      "Fetching weather for sensor 77446, 80/102\n",
      "Fetching weather for sensor 476353, 81/102\n",
      "Fetching weather for sensor 407335, 82/102\n",
      "Fetching weather for sensor 194215, 83/102\n",
      "Fetching weather for sensor 60859, 84/102\n",
      "Fetching weather for sensor 57421, 85/102\n",
      "Fetching weather for sensor 65272, 86/102\n",
      "Fetching weather for sensor 154549, 87/102\n",
      "Fetching weather for sensor 58909, 88/102\n",
      "Fetching weather for sensor 79999, 89/102\n",
      "Fetching weather for sensor 69724, 90/102\n",
      "Fetching weather for sensor 62848, 91/102\n",
      "Fetching weather for sensor 80773, 92/102\n",
      "Fetching weather for sensor 59095, 93/102\n",
      "Fetching weather for sensor 60853, 94/102\n",
      "Fetching weather for sensor 63646, 95/102\n",
      "Fetching weather for sensor 60541, 96/102\n",
      "Fetching weather for sensor 474841, 97/102\n",
      "Fetching weather for sensor 62968, 98/102\n",
      "Fetching weather for sensor 562600, 99/102\n",
      "Fetching weather for sensor 65284, 100/102\n",
      "Fetching weather for sensor 78532, 101/102\n",
      "Fetching weather for sensor 61867, 102/102\n",
      "üìä Collected 1122 weather dataframes\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching weather for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in dates_to_fetch:\n",
    "        try:\n",
    "            weather_df = fetchers.get_weather_forecast(\n",
    "                sensor_id=sensor_id,\n",
    "                latitude=meta[\"latitude\"],\n",
    "                longitude=meta[\"longitude\"],\n",
    "                start_date=day,\n",
    "                end_date=day + timedelta(days=6)\n",
    "            )\n",
    "\n",
    "            if weather_df.empty:\n",
    "                continue\n",
    "\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            all_weather_rows.append(weather_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weather for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_weather_rows)} weather dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca1fa",
   "metadata": {},
   "source": [
    "### 2.4.9. Clean and Align Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping empty or invalid df[0]\n",
      "üìã Cleaned 1117 air quality dataframes\n",
      "üìã Using base columns (excluding engineered features): 9 columns\n"
     ]
    }
   ],
   "source": [
    "cleaned_aq_rows = []\n",
    "\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "base_cols = [c for c in historical.columns if c not in engineered_cols]\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(base_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align to base columns only (no engineered features yet)\n",
    "    aligned = df.reindex(columns=base_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(base_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical (for base columns only)\n",
    "    for col in base_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "print(f\"üìã Cleaned {len(cleaned_aq_rows)} air quality dataframes\")\n",
    "print(f\"üìã Using base columns (excluding engineered features): {len(base_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235625a",
   "metadata": {},
   "source": [
    "### 2.4.10. Combine and Clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e893b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è Total weather records: 1734\n",
      "üìÖ Weather date range: 2026-02-09 00:00:00 to 2026-02-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if all_weather_rows:\n",
    "    all_weather = pd.concat(all_weather_rows, ignore_index=True)\n",
    "    all_weather = all_weather.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "    all_weather[\"date\"] = pd.to_datetime(all_weather[\"date\"]).dt.tz_localize(None)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_weather = all_weather.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather records: {len(all_weather)}\")\n",
    "    print(f\"üìÖ Weather date range: {all_weather['date'].min()} to {all_weather['date'].max()}\")\n",
    "else:\n",
    "    all_weather = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No weather data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa784d",
   "metadata": {},
   "source": [
    "## 2.5. Combine Data and Add Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea612675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total records after deduplication: 1117\n",
      "üìä Unique sensors: 102\n",
      "üìä Date range: 2026-02-09 00:00:00 to 2026-02-19 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Drop engineered columns from historical data before combining\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "historical_base = historical.drop(columns=engineered_cols, errors=\"ignore\")\n",
    "\n",
    "# Combine data\n",
    "all_aq = pd.concat([historical_base, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "# Remove duplicates: keep the first occurrence of each sensor_id + date combination\n",
    "all_aq = all_aq.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Total records after deduplication: {len(all_aq)}\")\n",
    "print(f\"üìä Unique sensors: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"üìä Date range: {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "# Add engineered features\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "\n",
    "# Pass sensor_locations dict to nearby sensor feature\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780ad9",
   "metadata": {},
   "source": [
    "## 2.6. Insert Data to Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e42e56",
   "metadata": {},
   "source": [
    "### 2.6.1. Insert Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Preparing to insert air quality data for 8 dates\n",
      "\n",
      "   Date 2026-02-12: 100 total rows before filtering\n",
      "   After pm25 filter: 100 rows\n",
      "   ‚ö†Ô∏è  pm25_lag_3d: 1/100 NaN values\n",
      "   After engineered features filter: 99 rows\n",
      "2026-02-19 14:14:51,955 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-13: 102 total rows before filtering\n",
      "   After pm25 filter: 102 rows\n",
      "   After engineered features filter: 102 rows\n",
      "2026-02-19 14:14:53,689 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-14: 102 total rows before filtering\n",
      "   After pm25 filter: 102 rows\n",
      "   After engineered features filter: 102 rows\n",
      "2026-02-19 14:14:55,276 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-15: 102 total rows before filtering\n",
      "   After pm25 filter: 102 rows\n",
      "   After engineered features filter: 102 rows\n",
      "2026-02-19 14:14:56,879 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-16: 101 total rows before filtering\n",
      "   After pm25 filter: 101 rows\n",
      "   After engineered features filter: 101 rows\n",
      "2026-02-19 14:14:58,443 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-17: 102 total rows before filtering\n",
      "   After pm25 filter: 102 rows\n",
      "   After engineered features filter: 102 rows\n",
      "2026-02-19 14:15:00,192 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-18: 101 total rows before filtering\n",
      "   After pm25 filter: 101 rows\n",
      "   After engineered features filter: 101 rows\n",
      "2026-02-19 14:15:02,043 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "   Date 2026-02-19: 102 total rows before filtering\n",
      "   After pm25 filter: 102 rows\n",
      "   After engineered features filter: 102 rows\n",
      "2026-02-19 14:15:03,580 INFO: \t8 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/30733\n",
      "   ‚ùå Error: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.\n",
      "\n",
      "‚úÖ Total air quality inserted: 0 records\n"
     ]
    }
   ],
   "source": [
    "if dates_to_insert:\n",
    "    print(f\"\\nüîç Preparing to insert air quality data for {len(dates_to_insert)} dates\")\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for day in dates_to_insert:\n",
    "        day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "        \n",
    "        # Show what we have before filtering\n",
    "        print(f\"\\n   Date {day}: {len(day_rows)} total rows before filtering\")\n",
    "        \n",
    "        # Filter out rows with missing pm25\n",
    "        day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "        print(f\"   After pm25 filter: {len(day_rows)} rows\")\n",
    "\n",
    "        # Identify engineered feature columns\n",
    "        engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "        \n",
    "        # Check which engineered features have NaN\n",
    "        if not day_rows.empty:\n",
    "            for col in engineered_cols:\n",
    "                nan_count = day_rows[col].isna().sum()\n",
    "                if nan_count > 0:\n",
    "                    print(f\"   ‚ö†Ô∏è  {col}: {nan_count}/{len(day_rows)} NaN values\")\n",
    "        \n",
    "        # Filter out rows with missing engineered features\n",
    "        day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "        print(f\"   After engineered features filter: {len(day_rows)} rows\")\n",
    "\n",
    "        if not day_rows.empty:\n",
    "            # Convert types to match feature group schema\n",
    "            day_rows = day_rows.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"pm25\": \"float64\",\n",
    "                \"pm25_lag_1d\": \"float64\",\n",
    "                \"pm25_lag_2d\": \"float64\",\n",
    "                \"pm25_lag_3d\": \"float64\",\n",
    "                \"pm25_rolling_3d\": \"float64\",\n",
    "                \"pm25_nearby_avg\": \"float64\",\n",
    "                \"city\": \"string\",\n",
    "                \"street\": \"string\",\n",
    "                \"country\": \"string\",\n",
    "                \"aqicn_url\": \"string\",\n",
    "                \"latitude\": \"float64\",\n",
    "                \"longitude\": \"float64\",\n",
    "            })\n",
    "            \n",
    "            # Ensure correct column order\n",
    "            fg_columns = [f.name for f in air_quality_fg.features]\n",
    "            day_rows = day_rows[fg_columns]\n",
    "            \n",
    "            # Insert data to feature group\n",
    "            try:\n",
    "                air_quality_fg.insert(day_rows)\n",
    "                total_inserted += len(day_rows)\n",
    "                print(f\"   ‚úÖ Inserted {len(day_rows)} rows for {day}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No valid rows for {day}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total air quality inserted: {total_inserted} records\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No air quality data to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "### 2.6.2. Insert Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a667f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå§Ô∏è  Preparing to insert 1734 weather records\n",
      "2026-02-19 14:15:05,115 INFO: \t7 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://eu-west.cloud.hopsworks.ai:443/p/19575/fs/13380/fg/28686\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/pm25-unlinked/.venv/lib/python3.12/site-packages/hsfs/core/delta_engine.py:348\u001b[39m, in \u001b[36mDeltaEngine._write_delta_rs_dataset\u001b[39m\u001b[34m(self, dataset, write_options)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeltalake\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeltaTable \u001b[38;5;28;01mas\u001b[39;00m DeltaRsTable\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeltalake\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m write_deltalake \u001b[38;5;28;01mas\u001b[39;00m deltars_write\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'deltalake'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[43mweather_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m         total_inserted += \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m     29\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Weather batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi//batch_size\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records (total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_inserted\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_weather)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/pm25-unlinked/.venv/lib/python3.12/site-packages/hsfs/feature_group.py:3454\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m   3445\u001b[39m     [\n\u001b[32m   3446\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3450\u001b[39m ):\n\u001b[32m   3451\u001b[39m     \u001b[38;5;66;03m# New delta FG allow for change data capture query\u001b[39;00m\n\u001b[32m   3452\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33mdelta.enableChangeDataFeed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3454\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[38;5;66;03m# Compute stats in client if there is no backfill job:\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;66;03m# - spark engine: always compute in client\u001b[39;00m\n\u001b[32m   3468\u001b[39m \u001b[38;5;66;03m# - python engine: only compute if FG is offline only (no backfill job)\u001b[39;00m\n\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/pm25-unlinked/.venv/lib/python3.12/site-packages/hsfs/core/feature_group_engine.py:247\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28mself\u001b[39m._feature_group_api.delete_content(feature_group)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbulk_insert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    256\u001b[39m     ge_report,\n\u001b[32m    257\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/pm25-unlinked/.venv/lib/python3.12/site-packages/hsfs/engine/python.py:1091\u001b[39m, in \u001b[36mEngine.save_dataframe\u001b[39m\u001b[34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[39m\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_group.time_travel_format == \u001b[33m\"\u001b[39m\u001b[33mDELTA\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m         delta_engine_instance = delta_engine.DeltaEngine(\n\u001b[32m   1085\u001b[39m             feature_store_id=feature_group.feature_store_id,\n\u001b[32m   1086\u001b[39m             feature_store_name=feature_group.feature_store_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1089\u001b[39m             spark_session=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1090\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         \u001b[43mdelta_engine_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_delta_fg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidation_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[32m   1098\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy_save_dataframe(\n\u001b[32m   1099\u001b[39m         feature_group,\n\u001b[32m   1100\u001b[39m         dataframe,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1106\u001b[39m         validation_id,\n\u001b[32m   1107\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/pm25-unlinked/.venv/lib/python3.12/site-packages/hsfs/core/delta_engine.py:93\u001b[39m, in \u001b[36mDeltaEngine.save_delta_fg\u001b[39m\u001b[34m(self, dataset, write_options, validation_id)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     90\u001b[39m     _logger.debug(\n\u001b[32m     91\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving Delta dataset using delta-rs to feature group \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._feature_group.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._feature_group.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     fg_commit = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_delta_rs_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m fg_commit.validation_id = validation_id\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_group_api.commit(\u001b[38;5;28mself\u001b[39m._feature_group, fg_commit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/pm25-unlinked/.venv/lib/python3.12/site-packages/hsfs/core/delta_engine.py:352\u001b[39m, in \u001b[36mDeltaEngine._write_delta_rs_dataset\u001b[39m\u001b[34m(self, dataset, write_options)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeltalake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TableNotFoundError\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    353\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDelta Lake (deltalake) and its dependencies are required for non-Spark operations. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInstall \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhops-deltalake\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to enable Delta RS features.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    355\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    356\u001b[39m location = \u001b[38;5;28mself\u001b[39m._get_delta_rs_location()\n\u001b[32m    357\u001b[39m is_polars_df = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Delta Lake (deltalake) and its dependencies are required for non-Spark operations. Install 'hops-deltalake' to enable Delta RS features."
     ]
    }
   ],
   "source": [
    "if not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è  Preparing to insert {len(all_weather)} weather records\")\n",
    "    \n",
    "    # Convert types to match feature group schema\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    weather_fg_columns = [f.name for f in weather_fg.features]\n",
    "    all_weather = all_weather[weather_fg_columns]\n",
    "    \n",
    "    # Insert in smaller batches to avoid connection issues\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"   ‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\")\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"   ‚ö†Ô∏è  Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Failed weather batch {i//batch_size + 1}\")\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"   üíæ Saved to {failed_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total weather inserted: {total_inserted}/{len(all_weather)} records\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No weather data to insert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc96247",
   "metadata": {},
   "source": [
    "## 2.7. Pipeline Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ FEATURE PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Data Summary:\")\n",
    "print(f\"   - Dates processed: {len(dates_to_insert)}\")\n",
    "print(f\"   - Air quality records inserted: {total_inserted if dates_to_insert else 0}\")\n",
    "print(f\"   - Weather records inserted: {len(all_weather) if not all_weather.empty else 0}\")\n",
    "print(f\"\\nüíæ Feature Groups Updated:\")\n",
    "print(f\"   - {air_quality_fg.name} (v{air_quality_fg.version})\")\n",
    "print(f\"   - {weather_fg.name} (v{weather_fg.version})\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
