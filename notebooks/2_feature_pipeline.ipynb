{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc803b",
   "metadata": {},
   "source": [
    "# 2. Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f00db",
   "metadata": {},
   "source": [
    "## 2.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6c5b",
   "metadata": {},
   "source": [
    "### 2.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ace5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f5bad",
   "metadata": {},
   "source": [
    "### 2.1.2. Load Settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e733e1",
   "metadata": {},
   "source": [
    "### 2.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7280cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa04a",
   "metadata": {},
   "source": [
    "### 2.1.3. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33477f76",
   "metadata": {},
   "source": [
    "## 2.2. Get Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792e656",
   "metadata": {},
   "source": [
    "## 2.3. Load Sensor Locations from Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bfe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from air_quality feature group\n",
    "aq_data = air_quality_fg.read()\n",
    "\n",
    "if len(aq_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No air quality data found. Run pipeline 1 (backfill) first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeca83",
   "metadata": {},
   "source": [
    "## 2.4. Data Collection\n",
    "Fetch today's air quality data and weather forecasts, format data to match feature group schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29f776",
   "metadata": {},
   "source": [
    "### 2.4.1. Load Historical Air Quality Data (Last 4 Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîç Processing {len(sensor_locations)} sensor locations.\")\n",
    "historical_start = today - timedelta(days=4)\n",
    "\n",
    "try:\n",
    "    historical_df = air_quality_fg.read()\n",
    "    if not historical_df.empty:\n",
    "        historical_df[\"date\"] = pd.to_datetime(historical_df[\"date\"]).dt.tz_localize(None)\n",
    "        today_dt = pd.to_datetime(today)\n",
    "        historical_start_dt = pd.to_datetime(historical_start)\n",
    "        \n",
    "        historical_df = historical_df[\n",
    "            (historical_df[\"date\"] >= historical_start_dt) & \n",
    "            (historical_df[\"date\"] <= today_dt) \n",
    "        ][[\"date\", \"sensor_id\", \"pm25\"]]\n",
    "        \n",
    "        historical_df = historical_df[historical_df[\"sensor_id\"].isin(sensor_locations.keys())]\n",
    "    else:\n",
    "        historical_df = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error reading historical data: {e}\")\n",
    "    historical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c824a",
   "metadata": {},
   "source": [
    "### 2.4.2. Identify Missing Dates for Backfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = air_quality_fg.read()\n",
    "existing_dates = pd.to_datetime(aq_data[\"date\"]).dt.date.unique()\n",
    "\n",
    "# Debug: Show what dates exist in the feature store\n",
    "if len(existing_dates) > 0:\n",
    "    print(f\"üìÖ Feature store date range: {min(existing_dates)} to {max(existing_dates)}\")\n",
    "    print(f\"üìÖ Total unique dates in store: {len(existing_dates)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No dates found in feature store\")\n",
    "\n",
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=7)  # Check last 7 days for missing data\n",
    "\n",
    "# Generate expected dates and convert to set for faster lookup\n",
    "expected_dates = set(pd.date_range(start=start_date, end=today, freq=\"D\").date)\n",
    "existing_dates_set = set(existing_dates)\n",
    "\n",
    "# Find missing dates\n",
    "original_missing_dates = sorted(list(expected_dates - existing_dates_set))\n",
    "\n",
    "print(f\"\\nüîç Checking for missing dates between {start_date} and {today}\")\n",
    "print(f\"   Expected dates: {len(expected_dates)}\")\n",
    "print(f\"   Existing dates in that range: {len(expected_dates & existing_dates_set)}\")\n",
    "print(f\"   Missing dates: {len(original_missing_dates)}\")\n",
    "\n",
    "# Separate: dates to fetch vs dates to insert\n",
    "dates_to_insert = original_missing_dates.copy()  # Only insert the actual missing dates\n",
    "dates_to_fetch = original_missing_dates.copy()   # Fetch missing dates + buffer\n",
    "\n",
    "# Add 3 buffer days before first missing date to ensure we can calculate lag features\n",
    "if original_missing_dates:\n",
    "    earliest_missing = min(original_missing_dates)\n",
    "    buffer_dates = [earliest_missing - timedelta(days=i) for i in range(1, 4)]\n",
    "    # Only add buffer dates that aren't already in existing_dates\n",
    "    buffer_dates = [d for d in buffer_dates if d not in existing_dates_set]\n",
    "    dates_to_fetch = sorted(buffer_dates + dates_to_fetch)\n",
    "\n",
    "formatted = \", \".join(d.isoformat() for d in dates_to_fetch) if dates_to_fetch else \"None\"\n",
    "insert_formatted = \", \".join(d.isoformat() for d in dates_to_insert) if dates_to_insert else \"None\"\n",
    "print(f\"\\nüìÖ Dates to fetch: {formatted}\")\n",
    "print(f\"üìÖ Dates to insert: {insert_formatted}\")\n",
    "\n",
    "# Exit early if no missing dates\n",
    "if not dates_to_fetch:\n",
    "    print(\"\\n‚úÖ No missing dates found. Feature store is up to date!\")\n",
    "    print(\"   The feature pipeline will continue without fetching new data.\")\n",
    "    dates_to_insert = []\n",
    "    dates_to_fetch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e421b8",
   "metadata": {},
   "source": [
    "### 2.4.3. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if no dates to fetch\n",
    "if not dates_to_fetch:\n",
    "    print(\"‚è≠Ô∏è Skipping data preparation - no missing dates\")\n",
    "    historical = air_quality_fg.read()\n",
    "    historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "    all_aq_rows = []\n",
    "    all_weather_rows = []\n",
    "else:\n",
    "    # Prepare historical data window\n",
    "    historical_cutoff = pd.to_datetime(min(dates_to_fetch)) - pd.Timedelta(days=3)\n",
    "    historical = air_quality_fg.read()\n",
    "    historical[\"date\"] = pd.to_datetime(historical[\"date\"]).dt.tz_localize(None)\n",
    "    historical = historical[historical[\"date\"] >= historical_cutoff]\n",
    "\n",
    "    # Track existing sensor-date pairs\n",
    "    existing = historical[[\"sensor_id\", \"date\"]].copy()\n",
    "    existing[\"date_only\"] = existing[\"date\"].dt.date\n",
    "    existing_keys = set(zip(existing[\"sensor_id\"], existing[\"date_only\"]))\n",
    "\n",
    "    # Initialize data containers\n",
    "    all_aq_rows = [historical]\n",
    "    all_weather_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ba174",
   "metadata": {},
   "source": [
    "### 2.4.4. Fetch Missing Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching air quality for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in dates_to_fetch:\n",
    "        if (sensor_id, day) in existing_keys:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            aq_df = fetchers.get_pm25(\n",
    "                meta[\"aqicn_url\"], meta[\"country\"], meta[\"city\"],\n",
    "                meta[\"street\"], day, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            if aq_df.empty or aq_df[\"pm25\"].isna().all():\n",
    "                continue\n",
    "\n",
    "            aq_df[\"sensor_id\"] = int(sensor_id)\n",
    "            aq_df[\"pm25\"] = pd.to_numeric(aq_df[\"pm25\"], errors=\"coerce\")\n",
    "            # aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            # Add metadata\n",
    "            aq_df[\"city\"] = meta[\"city\"]\n",
    "            aq_df[\"street\"] = meta[\"street\"]\n",
    "            aq_df[\"country\"] = meta[\"country\"]\n",
    "            aq_df[\"aqicn_url\"] = meta[\"aqicn_url\"]\n",
    "            aq_df[\"latitude\"] = meta[\"latitude\"]\n",
    "            aq_df[\"longitude\"] = meta[\"longitude\"]\n",
    "\n",
    "            aq_df = aq_df.drop(columns=[\"url\"], errors=\"ignore\")\n",
    "\n",
    "            all_aq_rows.append(aq_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Air quality for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_aq_rows)} air quality dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf99689",
   "metadata": {},
   "source": [
    "### 2.4.5. Fetch Missing Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "total = len(sensor_locations)\n",
    "\n",
    "for sensor_id, meta in sensor_locations.items():\n",
    "    print(f\"Fetching weather for sensor {sensor_id}, {count}/{total}\")\n",
    "    count += 1\n",
    "\n",
    "    for day in dates_to_fetch:\n",
    "        try:\n",
    "            weather_df = fetchers.get_weather_forecast(\n",
    "                sensor_id=sensor_id,\n",
    "                latitude=meta[\"latitude\"],\n",
    "                longitude=meta[\"longitude\"],\n",
    "                start_date=day,\n",
    "                end_date=day + timedelta(days=6)\n",
    "            )\n",
    "\n",
    "            if weather_df.empty:\n",
    "                continue\n",
    "\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.normalize()\n",
    "\n",
    "            all_weather_rows.append(weather_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weather for sensor {sensor_id} on {day}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"üìä Collected {len(all_weather_rows)} weather dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca1fa",
   "metadata": {},
   "source": [
    "### 2.4.9. Clean and Align Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_aq_rows = []\n",
    "\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "base_cols = [c for c in historical.columns if c not in engineered_cols]\n",
    "\n",
    "for i, df in enumerate(all_aq_rows):\n",
    "    if df.empty or \"pm25\" not in df.columns or df[\"pm25\"].isna().all():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty or invalid df[{i}]\")\n",
    "        continue\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "    # Skip if too few expected columns are present\n",
    "    if len(set(df.columns) & set(base_cols)) < 3:\n",
    "        print(f\"‚ö†Ô∏è Skipping malformed df[{i}] with columns: {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Align to base columns only (no engineered features yet)\n",
    "    aligned = df.reindex(columns=base_cols, fill_value=np.nan)\n",
    "\n",
    "    # Final sanity check\n",
    "    if aligned.shape[1] != len(base_cols):\n",
    "        print(f\"‚ùå Still malformed after alignment: df[{i}] shape={aligned.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Force dtype alignment to match historical (for base columns only)\n",
    "    for col in base_cols:\n",
    "        if col in historical.columns:\n",
    "            try:\n",
    "                aligned[col] = aligned[col].astype(historical[col].dtype, errors=\"raise\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not cast column '{col}' in df[{i}]: {e}\")\n",
    "                continue\n",
    "\n",
    "    cleaned_aq_rows.append(aligned)\n",
    "\n",
    "print(f\"üìã Cleaned {len(cleaned_aq_rows)} air quality dataframes\")\n",
    "print(f\"üìã Using base columns (excluding engineered features): {len(base_cols)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235625a",
   "metadata": {},
   "source": [
    "### 2.4.10. Combine and Clean Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e893b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_weather_rows:\n",
    "    all_weather = pd.concat(all_weather_rows, ignore_index=True)\n",
    "    all_weather = all_weather.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "    all_weather[\"date\"] = pd.to_datetime(all_weather[\"date\"]).dt.tz_localize(None)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_weather = all_weather.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\")\n",
    "    \n",
    "    print(f\"üå§Ô∏è Total weather records: {len(all_weather)}\")\n",
    "    print(f\"üìÖ Weather date range: {all_weather['date'].min()} to {all_weather['date'].max()}\")\n",
    "else:\n",
    "    all_weather = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No weather data collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa784d",
   "metadata": {},
   "source": [
    "## 2.5. Combine Data and Add Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea612675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop engineered columns from historical data before combining\n",
    "engineered_cols = [c for c in historical.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "historical_base = historical.drop(columns=engineered_cols, errors=\"ignore\")\n",
    "\n",
    "# Combine data\n",
    "all_aq = pd.concat([historical_base, *cleaned_aq_rows], ignore_index=True)\n",
    "all_aq = all_aq.sort_values([\"sensor_id\", \"date\"]).reset_index(drop=True)\n",
    "all_aq[\"date\"] = pd.to_datetime(all_aq[\"date\"]).dt.normalize().dt.tz_localize(None)\n",
    "\n",
    "# Remove duplicates: keep the first occurrence of each sensor_id + date combination\n",
    "all_aq = all_aq.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Total records after deduplication: {len(all_aq)}\")\n",
    "print(f\"üìä Unique sensors: {all_aq['sensor_id'].nunique()}\")\n",
    "print(f\"üìä Date range: {all_aq['date'].min()} to {all_aq['date'].max()}\")\n",
    "\n",
    "# Add engineered features\n",
    "all_aq = feature_engineering.add_rolling_window_feature(all_aq, window_days=3)\n",
    "all_aq = feature_engineering.add_lagged_features(all_aq, lags=[1, 2, 3])\n",
    "\n",
    "# Pass sensor_locations dict to nearby sensor feature\n",
    "all_aq = feature_engineering.add_nearby_sensor_feature(all_aq, sensor_locations, n_closest=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780ad9",
   "metadata": {},
   "source": [
    "## 2.6. Insert Data to Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e42e56",
   "metadata": {},
   "source": [
    "### 2.6.1. Insert Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f589c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dates_to_insert:\n",
    "    print(f\"\\nüîç Preparing to insert air quality data for {len(dates_to_insert)} dates\", flush=True)\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for day in dates_to_insert:\n",
    "        day_rows = all_aq[all_aq[\"date\"].dt.date == day].copy()\n",
    "        \n",
    "        # Show what we have before filtering\n",
    "        print(f\"\\n   Date {day}: {len(day_rows)} total rows before filtering\", flush=True)\n",
    "        \n",
    "        # Filter out rows with missing pm25\n",
    "        day_rows = day_rows.dropna(subset=[\"pm25\"])\n",
    "        print(f\"   After pm25 filter: {len(day_rows)} rows\", flush=True)\n",
    "\n",
    "        # Identify engineered feature columns\n",
    "        engineered_cols = [c for c in day_rows.columns if \"lag\" in c or \"rolling\" in c or \"nearby\" in c]\n",
    "        \n",
    "        # Check which engineered features have NaN\n",
    "        if not day_rows.empty:\n",
    "            for col in engineered_cols:\n",
    "                nan_count = day_rows[col].isna().sum()\n",
    "                if nan_count > 0:\n",
    "                    print(f\"   ‚ö†Ô∏è  {col}: {nan_count}/{len(day_rows)} NaN values\", flush=True)\n",
    "        \n",
    "        # Filter out rows with missing engineered features\n",
    "        day_rows = day_rows.dropna(subset=engineered_cols, how=\"any\")\n",
    "        print(f\"   After engineered features filter: {len(day_rows)} rows\", flush=True)\n",
    "\n",
    "        if not day_rows.empty:\n",
    "            # Convert types to match feature group schema\n",
    "            day_rows = day_rows.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"pm25\": \"float64\",\n",
    "                \"pm25_lag_1d\": \"float64\",\n",
    "                \"pm25_lag_2d\": \"float64\",\n",
    "                \"pm25_lag_3d\": \"float64\",\n",
    "                \"pm25_rolling_3d\": \"float64\",\n",
    "                \"pm25_nearby_avg\": \"float64\",\n",
    "                \"city\": \"string\",\n",
    "                \"street\": \"string\",\n",
    "                \"country\": \"string\",\n",
    "                \"aqicn_url\": \"string\",\n",
    "                \"latitude\": \"float64\",\n",
    "                \"longitude\": \"float64\",\n",
    "            })\n",
    "            \n",
    "            # Ensure correct column order\n",
    "            fg_columns = [f.name for f in air_quality_fg.features]\n",
    "            day_rows = day_rows[fg_columns]\n",
    "            \n",
    "            # Insert data to feature group\n",
    "            try:\n",
    "                air_quality_fg.insert(day_rows)\n",
    "                total_inserted += len(day_rows)\n",
    "                print(f\"   ‚úÖ Inserted {len(day_rows)} rows for {day}\", flush=True)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\", flush=True)\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No valid rows for {day}\", flush=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total air quality inserted: {total_inserted} records\", flush=True)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No air quality data to insert\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e41ac3",
   "metadata": {},
   "source": [
    "### 2.6.2. Insert Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_weather.empty:\n",
    "    print(f\"\\nüå§Ô∏è  Preparing to insert {len(all_weather)} weather records\", flush=True)\n",
    "    \n",
    "    # Convert types to match feature group schema\n",
    "    all_weather = all_weather.astype({\n",
    "        \"sensor_id\": \"int32\",\n",
    "        \"temperature_2m_mean\": \"float64\",\n",
    "        \"precipitation_sum\": \"float64\",\n",
    "        \"wind_speed_10m_max\": \"float64\",\n",
    "        \"wind_direction_10m_dominant\": \"float64\",\n",
    "    })\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    weather_fg_columns = [f.name for f in weather_fg.features]\n",
    "    all_weather = all_weather[weather_fg_columns]\n",
    "    \n",
    "    # Insert in smaller batches to avoid connection issues\n",
    "    batch_size = 100\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(all_weather), batch_size):\n",
    "        batch = all_weather.iloc[i:i+batch_size]\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                weather_fg.insert(batch)\n",
    "                total_inserted += len(batch)\n",
    "                print(f\"   ‚úÖ Weather batch {i//batch_size + 1}: {len(batch)} records (total: {total_inserted}/{len(all_weather)})\", flush=True)\n",
    "                break\n",
    "            except (ProtocolError, ConnectionError, TimeoutError, KafkaException) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt\n",
    "                    print(f\"   ‚ö†Ô∏è  Connection error on weather batch {i//batch_size + 1}, retrying in {wait_time}s...\", flush=True)\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Failed weather batch {i//batch_size + 1}\", flush=True)\n",
    "                    failed_file = f\"{root_dir}/failed_weather_batch_{today}_{i}.csv\"\n",
    "                    batch.to_csv(failed_file, index=False)\n",
    "                    print(f\"   üíæ Saved to {failed_file}\", flush=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total weather inserted: {total_inserted}/{len(all_weather)} records\", flush=True)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No weather data to insert\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
