{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1f60",
   "metadata": {},
   "source": [
    "### 1.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c087c8",
   "metadata": {},
   "source": [
    "### 1.1.2. Load settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n",
    "print(project.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "### 1.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f82663",
   "metadata": {},
   "source": [
    "### 1.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6286f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean Up Old Resources (if recreating)\n",
    "\n",
    "# # Delete feature view first (blocks feature group deletion)\n",
    "# print(\"ðŸ—‘ï¸ Step 1: Delete feature view (if exists)...\")\n",
    "# try:\n",
    "#     hopsworks_admin.delete_feature_views(fs, \"air_quality_complete_fv\")\n",
    "# except Exception as e:\n",
    "#     print(f\"   Note: {e}\")\n",
    "\n",
    "# # Delete old feature groups\n",
    "# print(\"\\nðŸ—‘ï¸ Step 2: Delete old feature groups (if exist)...\")\n",
    "# try:\n",
    "#     hopsworks_admin.delete_feature_groups(fs, \"air_quality\")\n",
    "# except Exception as e:\n",
    "#     print(f\"   Note: {e}\")\n",
    "\n",
    "# try:\n",
    "#     hopsworks_admin.delete_feature_groups(fs, \"weather\")\n",
    "# except Exception as e:\n",
    "#     print(f\"   Note: {e}\")\n",
    "\n",
    "# print(\"\\nâœ… Cleanup complete - ready to create fresh feature groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check what backfill is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations_dict(air_quality_fg)\n",
    "existing_sensors = set(sensor_locations.keys())\n",
    "\n",
    "print(f\"ðŸ“‹ Found {len(existing_sensors)} sensors already in feature store\")\n",
    "print(f\"ðŸ“ Loaded locations for {len(sensor_locations)} existing sensors\")\n",
    "\n",
    "total_sensors = len([f for f in dir_list if f.endswith(\".csv\")])\n",
    "remaining = total_sensors - len(existing_sensors)\n",
    "print(f\"ðŸ“Š Total sensors: {total_sensors}, Already processed: {len(existing_sensors)}, Remaining: {remaining}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75457765",
   "metadata": {},
   "source": [
    "## 1.4. Backfill\n",
    "When performed for the first time, might take a long time if many added sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if total_sensors != len(existing_sensors):\n",
    "    print(\"\\nðŸš€ Starting backfill process...\\n\")\n",
    "    # Track processing stats\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    failed_sensors = []  # Track which sensors failed and why\n",
    "\n",
    "    for file in dir_list:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        try:\n",
    "            aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "                file_path, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            sensor_id = int(sensor_id)\n",
    "\n",
    "            # Skip if already processed\n",
    "            if sensor_id in existing_sensors:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Get working feed URL using sensor ID and API token\n",
    "            try:\n",
    "                working_feed_url = fetchers.get_working_feed_url(sensor_id, AQICN_API_KEY)\n",
    "            except Exception as url_err:\n",
    "                print(f\"âš ï¸ Sensor {sensor_id}: Could not resolve feed URL - {url_err}\")\n",
    "                working_feed_url = feed_url  # Fallback to CSV feed_url if resolution fails\n",
    "\n",
    "            # Get coordinates for this sensor\n",
    "            lat, lon = metadata.get_coordinates(city, street, country)\n",
    "            \n",
    "            if lat is None or lon is None:\n",
    "                print(f\"âš ï¸ Sensor {sensor_id}: cannot geocode location\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"Geocoding failed\"))\n",
    "                continue\n",
    "\n",
    "            # Clean and prepare air quality data \n",
    "            aq_df = cleaning.clean_and_append_data(\n",
    "                aq_df_raw, sensor_id, \n",
    "                city=city, street=street, country=country,\n",
    "                latitude=lat, longitude=lon, aqicn_url=working_feed_url\n",
    "            )\n",
    "            aq_df = aq_df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "            # Add lagged and rolling features\n",
    "            aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "            aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "            \n",
    "            # Calculate nearby sensor feature using location dict\n",
    "            if len(sensor_locations) > 0:\n",
    "                aq_df = feature_engineering.add_nearby_sensor_feature(\n",
    "                    aq_df, \n",
    "                    sensor_locations,\n",
    "                    n_closest=3\n",
    "                )\n",
    "            else:\n",
    "                aq_df[\"pm25_nearby_avg\"] = 0.0\n",
    "            \n",
    "            # Date range for weather\n",
    "            end_date = aq_df[\"date\"].max().date()\n",
    "            start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "            # Fetch weather\n",
    "            weather_df = fetchers.get_historical_weather(\n",
    "                sensor_id, start_date, end_date, lat, lon\n",
    "            )\n",
    "            \n",
    "            if weather_df is None or len(weather_df) == 0:\n",
    "                print(f\"âš ï¸ No weather data for sensor {sensor_id}\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"No weather data\"))\n",
    "                continue\n",
    "\n",
    "            # Prepare weather data\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.tz_localize(None)\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df = weather_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"temperature_2m_mean\": \"float64\",\n",
    "                \"precipitation_sum\": \"float64\",\n",
    "                \"wind_speed_10m_max\": \"float64\",\n",
    "                \"wind_direction_10m_dominant\": \"float64\",\n",
    "            })\n",
    "            \n",
    "            # Deduplicate weather by primary key before insert\n",
    "            weather_df = weather_df.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "            # Insert with automatic materialization\n",
    "            weather_fg.insert(weather_df)\n",
    "\n",
    "            # Prepare air quality data\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "            aq_df = aq_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"pm25\": \"float64\",\n",
    "                \"pm25_lag_1d\": \"float64\",\n",
    "                \"pm25_lag_2d\": \"float64\",\n",
    "                \"pm25_lag_3d\": \"float64\",\n",
    "                \"pm25_rolling_3d\": \"float64\",\n",
    "                \"pm25_nearby_avg\": \"float64\",\n",
    "                \"city\": \"string\",\n",
    "                \"street\": \"string\",\n",
    "                \"country\": \"string\",\n",
    "                \"aqicn_url\": \"string\",\n",
    "                \"latitude\": \"float64\",\n",
    "                \"longitude\": \"float64\",\n",
    "            })\n",
    "            \n",
    "            # Final deduplication by primary key before insert\n",
    "            aq_df = aq_df.drop_duplicates(subset=[\"sensor_id\", \"date\"], keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "            # Insert with automatic materialization\n",
    "            air_quality_fg.insert(aq_df)\n",
    "\n",
    "            existing_sensors.add(sensor_id)\n",
    "            \n",
    "            # Add this sensor's location to dict in format expected by feature_engineering\n",
    "            sensor_locations[sensor_id] = {\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"city\": city,\n",
    "                \"street\": street,\n",
    "                \"country\": country\n",
    "            }\n",
    "            \n",
    "            successful += 1\n",
    "            print(f\"âœ… Sensor {sensor_id} ({successful}/{remaining} complete)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, f\"{type(e).__name__}: {str(e)[:100]}\"))\n",
    "            print(f\"âŒ Sensor {sensor_id}: {type(e).__name__}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Backfill complete!\")\n",
    "    print(f\"ðŸ“Š Final Summary:\")\n",
    "    print(f\"   âœ… Successfully processed: {successful}\")\n",
    "    print(f\"   âŒ Failed: {failed}\")\n",
    "    print(f\"   â© Skipped (already processed): {skipped}\")\n",
    "    print(f\"   ðŸ“ˆ Total in feature store: {len(existing_sensors)}/{total_sensors}\")\n",
    "\n",
    "    if len(failed_sensors) > 0:\n",
    "        print(f\"\\nâš ï¸  Failed Sensors Detail:\")\n",
    "        for sid, reason in failed_sensors:\n",
    "            print(f\"   â€¢ Sensor {sid}: {reason}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâœ… All sensors already processed. No backfill needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.5. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.6. Add Validation to Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": 0.0,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": False,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "        kwargs={\"min_value\": 1, \"max_value\": None}\n",
    "    )\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(   \n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Temperature column - allow nulls, should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"min_value\": -80,\n",
    "            \"max_value\": 60,\n",
    "            \"mostly\": 1.0,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Precipitation column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wind column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"min_value\": 0,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "gx.core.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "    kwargs={\"min_value\": 1, \"max_value\": None}\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.7. Create Complete Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    # Select specific columns from weather_fg, excluding 'date' and 'sensor_id' since they're join keys\n",
    "    weather_features = [f.name for f in weather_fg.features if f.name not in ['date', 'sensor_id']]\n",
    "    \n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select(weather_features), on=[\"sensor_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)\n",
    "print(\"\\nâœ… Feature view 'air_quality_complete_fv' created or updated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
