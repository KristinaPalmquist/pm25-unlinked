{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1f60",
   "metadata": {},
   "source": [
    "### 1.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c087c8",
   "metadata": {},
   "source": [
    "### 1.1.2. Load settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52dd936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "HopsworksSettings initialized!\n",
      "2026-01-30 07:25:42,094 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-30 07:25:42,108 INFO: Initializing external client\n",
      "2026-01-30 07:25:42,108 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-30 07:25:43,690 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "Environment initialized and Hopsworks connected!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "### 1.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Already in git repository at c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f82663",
   "metadata": {},
   "source": [
    "### 1.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6286f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Step 1: Delete feature view (if exists)...\n",
      "2026-01-30 07:25:47,244 WARNING: JobWarning: All jobs associated to feature view `air_quality_complete_fv`, version `1` will be removed.\n",
      "\n",
      "Deleted air_quality_complete_fv/1\n",
      "\n",
      "üóëÔ∏è Step 2: Delete old feature groups (if exist)...\n",
      "2026-01-30 07:26:20,539 WARNING: JobWarning: All jobs associated to feature group `air_quality`, version `1` will be removed.\n",
      "\n",
      "   Note: Remote end closed connection without response\n",
      "2026-01-30 07:27:11,353 WARNING: JobWarning: All jobs associated to feature group `weather`, version `1` will be removed.\n",
      "\n",
      "Deleted weather/1\n",
      "\n",
      "‚úÖ Cleanup complete - ready to create fresh feature groups\n"
     ]
    }
   ],
   "source": [
    "# Clean Up Old Resources (if recreating)\n",
    "\n",
    "# Delete feature view first (blocks feature group deletion)\n",
    "print(\"üóëÔ∏è Step 1: Delete feature view (if exists)...\")\n",
    "try:\n",
    "    hopsworks_admin.delete_feature_views(fs, \"air_quality_complete_fv\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: {e}\")\n",
    "\n",
    "# Delete old feature groups\n",
    "print(\"\\nüóëÔ∏è Step 2: Delete old feature groups (if exist)...\")\n",
    "try:\n",
    "    hopsworks_admin.delete_feature_groups(fs, \"air_quality\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: {e}\")\n",
    "\n",
    "try:\n",
    "    hopsworks_admin.delete_feature_groups(fs, \"weather\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete - ready to create fresh feature groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1984891\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1984892\n"
     ]
    }
   ],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check what backfill is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-30 07:27:22,137 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2026-01-30T06:27:22.1054382+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\krist\\Documents\\GitHub\\pm25\\.venv\\Lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_flight.pyx\", line 1745, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow/_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/kristina_titanic_featurestore.db/air_quality_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2026-01-30T06:27:22.1054382+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n",
      "‚ö†Ô∏è Error loading sensor locations: Could not read data using Hopsworks Query Service.\n",
      "üìã Found 0 sensors already in feature store\n",
      "üìç Loaded locations for 0 existing sensors\n",
      "üìä Total sensors: 103, Already processed: 0, Remaining: 103\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations(air_quality_fg)\n",
    "existing_sensors = set(sensor_locations.keys())\n",
    "\n",
    "print(f\"üìã Found {len(existing_sensors)} sensors already in feature store\")\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")\n",
    "\n",
    "total_sensors = len([f for f in dir_list if f.endswith(\".csv\")])\n",
    "remaining = total_sensors - len(existing_sensors)\n",
    "print(f\"üìä Total sensors: {total_sensors}, Already processed: {len(existing_sensors)}, Remaining: {remaining}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75457765",
   "metadata": {},
   "source": [
    "## 1.4. Backfill\n",
    "When performed for the first time, might take a long time if many added sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting backfill process...\n",
      "\n",
      "\n",
      "üîç DEBUG Sensor 105325 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1119/1119 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 105325\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1686, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1686/1686 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 105325: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 107110 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 107110\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1510, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1510/1510 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 107110: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 112672 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 112672\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (2004, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 2004/2004 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 112672: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 112993 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 112993\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (2006, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 2006/2006 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 112993: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 113539 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:00 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 113539\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1391, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1391/1391 | Elapsed Time: 00:02 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 113539: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 113542 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 113542\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (410, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 410/410 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 113542: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 121810 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 121810\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1872, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1872/1872 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 121810: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 122302 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 122302\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1980, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1980/1980 | Elapsed Time: 00:02 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 122302: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 128095 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 128095\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1804, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1804/1804 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 128095: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 129124 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1127/1127 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Preparing AQ insert for Sensor 129124\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1961, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1961/1961 | Elapsed Time: 00:02 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 129124: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 149242 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 149242\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1703, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1703/1703 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 149242: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 154549 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1118/1118 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 154549\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1872, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1872/1872 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 154549: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 163156 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 163156\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1872, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1872/1872 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 163156: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 180187 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1127/1127 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 180187\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (1464, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1464/1464 | Elapsed Time: 00:02 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 180187: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 191047 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n",
      "‚ùå Sensor 191047: ConnectionError: HTTPSConnectionPool(host='c.app.hopsworks.ai', port=443): Max retries exceeded with url: /hopsworks-api/api/project/1279184/featurestores/1265800/featuregroups/1984892/expectationsuite (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002271843B590>: Failed to resolve 'c.app.hopsworks.ai' (Name or service not known: c.app.hopsworks.ai using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 192520. Details: https://api.waqi.info/feed/@192520/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@192520/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A0DF50>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A192520/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A192520/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A4F490>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 194215. Details: https://api.waqi.info/feed/@194215/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@194215/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A4D610>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A194215/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A194215/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A0EFD0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 196735. Details: https://api.waqi.info/feed/@196735/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@196735/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D682E0D0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A196735/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A196735/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69FE590>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 198559. Details: https://api.waqi.info/feed/@198559/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@198559/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A42490>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A198559/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A198559/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69DB390>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 208483. Details: https://api.waqi.info/feed/@208483/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@208483/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6938050>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A208483/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A208483/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6813850>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 249862. Details: https://api.waqi.info/feed/@249862/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@249862/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A4EDD0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A249862/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A249862/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A28650>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 250030. Details: https://api.waqi.info/feed/@250030/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@250030/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D698BCD0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A250030/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A250030/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022718448150>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 252352. Details: https://api.waqi.info/feed/@252352/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@252352/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6843C90>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A252352/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A252352/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A2AB50>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 345007. Details: https://api.waqi.info/feed/@345007/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@345007/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D68FA8D0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A345007/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A345007/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A42ED0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 351115. Details: https://api.waqi.info/feed/@351115/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@351115/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69E1950>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A351115/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A351115/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69F91D0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 376954. Details: https://api.waqi.info/feed/@376954/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@376954/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69E2790>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A376954/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A376954/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022718438E10>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 401314. Details: https://api.waqi.info/feed/@401314/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@401314/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69DBC50>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A401314/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A401314/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D67D5CD0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 404209. Details: https://api.waqi.info/feed/@404209/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@404209/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A48750>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A404209/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A404209/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D692BE10>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 407335. Details: https://api.waqi.info/feed/@407335/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@407335/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69DA950>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A407335/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A407335/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6843150>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 409513. Details: https://api.waqi.info/feed/@409513/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@409513/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A4A150>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A409513/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A409513/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6843B90>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 415030. Details: https://api.waqi.info/feed/@415030/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@415030/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69F86D0>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A415030/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A415030/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69B0550>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "‚ùå Sensor 191047: ValueError: Failed to resolve feed URL for sensor 417595. Details: https://api.waqi.info/feed/@417595/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/@417595/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D6A4BF90>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\")); https://api.waqi.info/feed/A417595/: HTTP error - HTTPSConnectionPool(host='api.waqi.info', port=443): Max retries exceeded with url: /feed/A417595/?token=8e8102ab311cab1eb5ed3c364f065878fbeb942c (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000226D69C3290>: Failed to resolve 'api.waqi.info' (Name or service not known: api.waqi.info using 1 resolver(s))\"))\n",
      "\n",
      "üîç DEBUG Sensor 420664 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 420664\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (754, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 754/754 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 420664: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 462457 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 462457\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (703, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 703/703 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 462457: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 474841 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1126/1126 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 474841\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (586, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 586/586 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 474841: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 476353 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 476353\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (596, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 596/596 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 476353: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 494275 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1104/1104 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 494275\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (497, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 497/497 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 494275: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 497266 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1103/1103 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 497266\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (411, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 411/411 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 497266: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 533086 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 533086\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (246, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 246/246 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 533086: FeatureStoreException: No materialization job was found\n",
      "\n",
      "üîç DEBUG Sensor 556792 - After cleaning:\n",
      "   Columns: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url']\n",
      "   'date' count: 1\n",
      "   After lagged features - 'date' count: 1\n",
      "   After rolling window - 'date' count: 1\n",
      "   After nearby sensor - 'date' count: 1\n",
      "   Final columns before weather: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Weather columns before insert: ['date', 'temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant', 'sensor_id']\n",
      "   Weather 'date' count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1129/1129 | Elapsed Time: 00:01 | Remaining Time: 00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Weather inserted successfully\n",
      "\n",
      "üîç Preparing AQ insert for Sensor 556792\n",
      "   Columns to insert: ['pm25', 'date', 'sensor_id', 'city', 'street', 'country', 'latitude', 'longitude', 'aqicn_url', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_rolling_3d', 'pm25_nearby_avg']\n",
      "   Shape: (111, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 111/111 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sensor 556792: FeatureStoreException: No materialization job was found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if total_sensors != len(existing_sensors):\n",
    "    print(\"\\nüöÄ Starting backfill process...\\n\")\n",
    "    # Track processing stats\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    failed_sensors = []  # Track which sensors failed and why\n",
    "\n",
    "    for file in dir_list:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        try:\n",
    "            aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "                file_path, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            sensor_id = int(sensor_id)\n",
    "\n",
    "            # Skip if already processed\n",
    "            if sensor_id in existing_sensors:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Get working feed URL using sensor ID and API token\n",
    "            try:\n",
    "                working_feed_url = fetchers.get_working_feed_url(sensor_id, AQICN_API_KEY)\n",
    "            except Exception as url_err:\n",
    "                print(f\"‚ö†Ô∏è Sensor {sensor_id}: Could not resolve feed URL - {url_err}\")\n",
    "                working_feed_url = feed_url  # Fallback to CSV feed_url if resolution fails\n",
    "\n",
    "            # Get coordinates for this sensor\n",
    "            lat, lon = metadata.get_coordinates(city, street, country)\n",
    "            \n",
    "            if lat is None or lon is None:\n",
    "                print(f\"‚ö†Ô∏è Sensor {sensor_id}: cannot geocode location\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"Geocoding failed\"))\n",
    "                continue\n",
    "\n",
    "            # Clean and prepare air quality data \n",
    "            aq_df = cleaning.clean_and_append_data(\n",
    "                aq_df_raw, sensor_id, \n",
    "                city=city, street=street, country=country,\n",
    "                latitude=lat, longitude=lon, aqicn_url=working_feed_url\n",
    "            )\n",
    "            aq_df = aq_df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "            # DEBUG: Check for duplicate columns after cleaning\n",
    "            print(f\"\\nüîç DEBUG Sensor {sensor_id} - After cleaning:\", flush=True)\n",
    "            print(f\"   Columns: {aq_df.columns.tolist()}\", flush=True)\n",
    "            print(f\"   'date' count: {aq_df.columns.tolist().count('date')}\", flush=True)\n",
    "            \n",
    "            # Add features one by one, checking for duplicates after each\n",
    "            aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "            print(f\"   After lagged features - 'date' count: {aq_df.columns.tolist().count('date')}\", flush=True)\n",
    "            \n",
    "            aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "            print(f\"   After rolling window - 'date' count: {aq_df.columns.tolist().count('date')}\", flush=True)\n",
    "            \n",
    "            # Calculate nearby sensor feature using location dict\n",
    "            if len(sensor_locations) > 0:\n",
    "                aq_df = feature_engineering.add_nearby_sensor_feature(\n",
    "                    aq_df, \n",
    "                    sensor_locations,\n",
    "                    n_closest=3\n",
    "                )\n",
    "            else:\n",
    "                aq_df[\"pm25_nearby_avg\"] = 0.0\n",
    "            \n",
    "            print(f\"   After nearby sensor - 'date' count: {aq_df.columns.tolist().count('date')}\", flush=True)\n",
    "            print(f\"   Final columns before weather: {aq_df.columns.tolist()}\", flush=True)\n",
    "            \n",
    "            # Date range for weather\n",
    "            end_date = aq_df[\"date\"].max().date()\n",
    "            start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "            # Fetch weather\n",
    "            weather_df = fetchers.get_historical_weather(\n",
    "                sensor_id, start_date, end_date, lat, lon\n",
    "            )\n",
    "            \n",
    "            if weather_df is None or len(weather_df) == 0:\n",
    "                print(f\"‚ö†Ô∏è No weather data for sensor {sensor_id}\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"No weather data\"))\n",
    "                continue\n",
    "\n",
    "            # Prepare weather data\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.tz_localize(None)\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df = weather_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"temperature_2m_mean\": \"float64\",\n",
    "                \"precipitation_sum\": \"float64\",\n",
    "                \"wind_speed_10m_max\": \"float64\",\n",
    "                \"wind_direction_10m_dominant\": \"float64\",\n",
    "            })\n",
    "            \n",
    "            print(f\"   Weather columns before insert: {weather_df.columns.tolist()}\", flush=True)\n",
    "            print(f\"   Weather 'date' count: {weather_df.columns.tolist().count('date')}\", flush=True)\n",
    "            \n",
    "            # Insert without triggering materialization\n",
    "            weather_fg.insert(weather_df, write_options={\"start_offline_materialization\": False})\n",
    "            print(f\"   ‚úÖ Weather inserted successfully\", flush=True)\n",
    "\n",
    "            # Prepare air quality data\n",
    "            print(f\"\\nüîç Preparing AQ insert for Sensor {sensor_id}\", flush=True)\n",
    "\n",
    "            # Ensure date is properly formatted\n",
    "            aq_df[\"date\"] = pd.to_datetime(aq_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "            # Ensure proper dtypes\n",
    "            aq_df = aq_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"pm25\": \"float64\",\n",
    "                \"pm25_lag_1d\": \"float64\",\n",
    "                \"pm25_lag_2d\": \"float64\",\n",
    "                \"pm25_lag_3d\": \"float64\",\n",
    "                \"pm25_rolling_3d\": \"float64\",\n",
    "                \"pm25_nearby_avg\": \"float64\",\n",
    "                \"city\": \"string\",\n",
    "                \"street\": \"string\",\n",
    "                \"country\": \"string\",\n",
    "                \"aqicn_url\": \"string\",\n",
    "                \"latitude\": \"float64\",\n",
    "                \"longitude\": \"float64\",\n",
    "            })\n",
    "            \n",
    "            # Final verification\n",
    "            print(f\"   Columns to insert: {aq_df.columns.tolist()}\", flush=True)\n",
    "            print(f\"   Shape: {aq_df.shape}\", flush=True)\n",
    "            \n",
    "            # Insert without triggering materialization\n",
    "            air_quality_fg.insert(aq_df, write_options={\"start_offline_materialization\": False})\n",
    "\n",
    "            existing_sensors.add(sensor_id)\n",
    "            \n",
    "            # Add this sensor's location to dict for subsequent nearby calculations\n",
    "            sensor_locations[sensor_id] = (lat, lon, city, street, country)\n",
    "            \n",
    "            successful += 1\n",
    "            print(f\"‚úÖ Sensor {sensor_id} ({successful}/{remaining} complete)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, f\"{type(e).__name__}: {str(e)[:100]}\"))\n",
    "            print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüéâ Backfill complete!\")\n",
    "    print(f\"üìä Final Summary:\")\n",
    "    print(f\"   ‚úÖ Successfully processed: {successful}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}\")\n",
    "    print(f\"   ‚è© Skipped (already processed): {skipped}\")\n",
    "    print(f\"   üìà Total in feature store: {len(existing_sensors)}/{total_sensors}\")\n",
    "\n",
    "    if len(failed_sensors) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed Sensors Detail:\")\n",
    "        for sid, reason in failed_sensors:\n",
    "            print(f\"   ‚Ä¢ Sensor {sid}: {reason}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ All sensors already processed. No backfill needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.5. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "# hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.6. Add Validation to Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing expectation suite for FG 'air_quality'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n",
      "Saved expectation suite for FG 'air_quality'.\n",
      "Deleted existing expectation suite for FG 'weather'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n",
      "Saved expectation suite for FG 'weather'.\n"
     ]
    }
   ],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": 0.0,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": False,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "        kwargs={\"min_value\": 1, \"max_value\": None}\n",
    "    )\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(   \n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Temperature column - allow nulls, should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"min_value\": -80,\n",
    "            \"max_value\": 60,\n",
    "            \"mostly\": 1.0,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Precipitation column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wind column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"min_value\": 0,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "gx.core.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "    kwargs={\"min_value\": 1, \"max_value\": None}\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.7. Create Complete Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    # Select specific columns from weather_fg, excluding 'date' and 'sensor_id' since they're join keys\n",
    "    weather_features = [f.name for f in weather_fg.features if f.name not in ['date', 'sensor_id']]\n",
    "    \n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select(weather_features), on=[\"sensor_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "#     query = (\n",
    "#         air_quality_fg.select_all()\n",
    "#         .join(weather_fg.select_all(), on=[\"sensor_id\", \"date\"])\n",
    "#     )\n",
    "\n",
    "#     fv = fs.get_or_create_feature_view(\n",
    "#         name=\"air_quality_complete_fv\",\n",
    "#         version=1,\n",
    "#         query=query,\n",
    "#         labels=[\"pm25\"]\n",
    "#     )\n",
    "\n",
    "#     return fv\n",
    "\n",
    "# air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452bc36",
   "metadata": {},
   "source": [
    "## 1.8. Trigger Offline Feature Store Materialization\n",
    "After backfilling data, materialize the feature groups to populate the offline feature store for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd447480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting materialization jobs for feature groups...\n",
      "2026-01-29 17:39:32,268 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=None) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Air quality feature group materialization started\n",
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "‚úÖ Weather feature group materialization started\n",
      "\n",
      "üìù Note: Materialization jobs run asynchronously. Check Hopsworks UI for status.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Starting materialization jobs for feature groups...\")\n",
    "\n",
    "try:\n",
    "    # Trigger materialization for air quality feature group\n",
    "    air_quality_job = air_quality_fg.materialization_job\n",
    "    air_quality_job.run(await_termination=False)\n",
    "    print(f\"‚úÖ Air quality feature group materialization started\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Air quality materialization: {e}\")\n",
    "\n",
    "try:\n",
    "    # Trigger materialization for weather feature group\n",
    "    weather_job = weather_fg.materialization_job\n",
    "    weather_job.run(await_termination=False)\n",
    "    print(f\"‚úÖ Weather feature group materialization started\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Weather materialization: {e}\")\n",
    "\n",
    "print(\"\\nüìù Note: Materialization jobs run asynchronously. Check Hopsworks UI for status.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_quality_df = air_quality_fg.read()\n",
    "# weather_df = weather_fg.read()\n",
    "\n",
    "# # Extract unique sensor metadata from air quality feature group\n",
    "# metadata_df = air_quality_df[[\"sensor_id\", \"city\", \"street\", \"country\", \"latitude\", \"longitude\"]].drop_duplicates(subset=[\"sensor_id\"])\n",
    "# print(f\"üìç Extracted metadata for {len(metadata_df)} unique sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "# print(\"=\"*40)\n",
    "\n",
    "# print(f\"Shape: {air_quality_df.shape}\")\n",
    "# print(f\"Date range: {air_quality_df['date'].min().date()} to {air_quality_df['date'].max().date()}\")\n",
    "# print(f\"Number of unique sensors: {air_quality_df['sensor_id'].nunique()}\")\n",
    "# print(f\"Countries: {metadata_df['country'].unique()}\")\n",
    "# print(f\"Cities: {metadata_df['city'].nunique()} unique cities\")\n",
    "\n",
    "# print(\"\\nüìä PM2.5 Statistics:\")\n",
    "# print(air_quality_df['pm25'].describe())\n",
    "# print(f\"Missing values: {air_quality_df['pm25'].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüìà Engineered Features Statistics:\")\n",
    "# for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "#     if col in air_quality_df.columns:\n",
    "#         missing = air_quality_df[col].isna().sum()\n",
    "#         print(f\"{col}: {missing} missing values ({missing/len(air_quality_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "# print(\"=\"*40)\n",
    "\n",
    "# print(f\"Shape: {weather_df.shape}\")\n",
    "# print(f\"Date range: {weather_df['date'].min().date()} to {weather_df['date'].max().date()}\")\n",
    "# print(f\"Number of unique sensors: {metadata_df['sensor_id'].nunique()}\")\n",
    "\n",
    "# print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "# for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "#     if col in weather_df.columns:\n",
    "#         print(f\"{col}:\")\n",
    "#         print(f\"  Range: {weather_df[col].min():.2f} to {weather_df[col].max():.2f}, Mean: {weather_df[col].mean():.2f}, Missing: {weather_df[col].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüìç Geographic Coverage:\")\n",
    "# print(f\"Latitude range: {metadata_df['latitude'].min():.3f} to {metadata_df['latitude'].max():.3f}, Longitude range: {metadata_df['longitude'].min():.3f} to {metadata_df['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "# print(\"=\"*40)\n",
    "\n",
    "# # Overall data completeness\n",
    "# sensor_day_counts = air_quality_df.groupby('sensor_id')['date'].count()\n",
    "# total_records = len(air_quality_df)\n",
    "# data_completeness = (1 - air_quality_df['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "# print(f\"üìä Overall Data Quality:\")\n",
    "# print(f\"Total records: {total_records:,}\")\n",
    "# print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "# print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "# print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# # Extreme values summary\n",
    "# extreme_count = (air_quality_df['pm25'] > 100).sum()\n",
    "# very_high_count = (air_quality_df['pm25'] > 50).sum()\n",
    "# print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "# print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "# print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# # Seasonal patterns\n",
    "# if len(air_quality_df) > 0:\n",
    "#     # Create temporary month column without modifying original DataFrame\n",
    "#     temp_months = pd.to_datetime(air_quality_df['date']).dt.month\n",
    "#     monthly_pm25 = air_quality_df.groupby(temp_months)['pm25'].mean()\n",
    "#     print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "#     seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "#     for months, season in seasons.items():\n",
    "#         season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "#         print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
