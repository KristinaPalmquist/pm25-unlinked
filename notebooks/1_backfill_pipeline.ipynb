{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a14877f",
   "metadata": {},
   "source": [
    "# 1. Backfill Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e716b",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1f60",
   "metadata": {},
   "source": [
    "### 1.1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee74632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root dir: c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#  Establish project root directory\n",
    "def find_project_root(start: Path):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "root_dir = find_project_root(Path().absolute())\n",
    "print(\"Project root dir:\", root_dir)\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import hopsworks\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from confluent_kafka import KafkaException\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#  Project imports\n",
    "from utils import cleaning, config, feature_engineering, fetchers, hopsworks_admin, incremental, metadata, visualization\n",
    "\n",
    "today = datetime.today().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c087c8",
   "metadata": {},
   "source": [
    "### 1.1.2. Load settings and Initialize Hopsworks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dd936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "HopsworksSettings initialized!\n",
      "2026-01-28 13:13:35,644 INFO: Initializing external client\n",
      "2026-01-28 13:13:35,646 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-28 13:13:37,357 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279184\n",
      "Environment initialized and Hopsworks connected!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def detect_environment():\n",
    "    if (\n",
    "        \"HOPSWORKS_JOB_ID\" in os.environ\n",
    "        or \"HOPSWORKS_PROJECT_ID\" in os.environ\n",
    "        or \"HOPSWORKS_JOB_NAME\" in os.environ\n",
    "    ):\n",
    "        return \"job\"\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    if cwd.startswith(\"/hopsfs/Jupyter\"):\n",
    "        return \"jupyter\"\n",
    "\n",
    "    return \"local\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\"Detected environment: {env}\")\n",
    "\n",
    "# Load secrets based on environment\n",
    "\n",
    "if env in (\"job\", \"jupyter\"):\n",
    "    project = hopsworks.login()\n",
    "    secrets_api = hopsworks.get_secrets_api()\n",
    "\n",
    "    for key in [\"HOPSWORKS_API_KEY\", \"AQICN_API_KEY\", \"GH_PAT\", \"GH_USERNAME\"]:\n",
    "        os.environ[key] = secrets_api.get_secret(key).value\n",
    "\n",
    "else:\n",
    "    load_dotenv()\n",
    "\n",
    "# Load Pydantic settings\n",
    "\n",
    "settings = config.HopsworksSettings()\n",
    "\n",
    "HOPSWORKS_API_KEY = settings.HOPSWORKS_API_KEY.get_secret_value()\n",
    "AQICN_API_KEY = settings.AQICN_API_KEY.get_secret_value()\n",
    "GITHUB_USERNAME = settings.GH_USERNAME.get_secret_value()\n",
    "\n",
    "# Login to Hopsworks using the API key\n",
    "\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Environment initialized and Hopsworks connected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b972e",
   "metadata": {},
   "source": [
    "### 1.1.3. Repository management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc6c87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Already in git repository at c:\\Users\\krist\\Documents\\GitHub\\pm25\n"
     ]
    }
   ],
   "source": [
    "repo_dir = hopsworks_admin.clone_or_update_repo(GITHUB_USERNAME)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f82663",
   "metadata": {},
   "source": [
    "### 1.1.4. Configure API Keys and Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61d9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "try:\n",
    "    secrets.get_secret(\"AQICN_API_KEY\")\n",
    "except:\n",
    "    secrets.create_secret(\"AQICN_API_KEY\", settings.AQICN_API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1310d",
   "metadata": {},
   "source": [
    "## 1.2. Create Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30e342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg, weather_fg = hopsworks_admin.create_feature_groups(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a6ddc",
   "metadata": {},
   "source": [
    "## 1.3. Check what backfill is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (12.75s) \n",
      "üìã Found 103 sensors already in feature store\n",
      "üìç Loaded locations for 103 existing sensors\n",
      "üìä Total sensors: 103, Already processed: 103, Remaining: 0\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "dir_list = os.listdir(data_dir)\n",
    "\n",
    "sensor_locations = metadata.get_sensor_locations(air_quality_fg)\n",
    "existing_sensors = set(sensor_locations.keys())\n",
    "\n",
    "print(f\"üìã Found {len(existing_sensors)} sensors already in feature store\")\n",
    "print(f\"üìç Loaded locations for {len(sensor_locations)} existing sensors\")\n",
    "\n",
    "total_sensors = len([f for f in dir_list if f.endswith(\".csv\")])\n",
    "remaining = total_sensors - len(existing_sensors)\n",
    "print(f\"üìä Total sensors: {total_sensors}, Already processed: {len(existing_sensors)}, Remaining: {remaining}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75457765",
   "metadata": {},
   "source": [
    "## 1.4. Backfill\n",
    "When performed for the first time, might take a long time if many added sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All sensors already processed. No backfill needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if total_sensors != len(existing_sensors):\n",
    "    print(\"\\nüöÄ Starting backfill process...\\n\")\n",
    "    # Track processing stats\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    skipped = 0\n",
    "    failed_sensors = []  # Track which sensors failed and why\n",
    "\n",
    "    for file in dir_list:\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        try:\n",
    "            aq_df_raw, street, city, country, feed_url, sensor_id = metadata.read_sensor_data(\n",
    "                file_path, AQICN_API_KEY\n",
    "            )\n",
    "\n",
    "            sensor_id = int(sensor_id)\n",
    "\n",
    "            # Skip if already processed\n",
    "            if sensor_id in existing_sensors:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Get working feed URL using sensor ID and API token\n",
    "            try:\n",
    "                working_feed_url = fetchers.get_working_feed_url(sensor_id, AQICN_API_KEY)\n",
    "            except Exception as url_err:\n",
    "                print(f\"‚ö†Ô∏è Sensor {sensor_id}: Could not resolve feed URL - {url_err}\")\n",
    "                working_feed_url = feed_url  # Fallback to CSV feed_url if resolution fails\n",
    "\n",
    "            # Get coordinates for this sensor\n",
    "            lat, lon = metadata.get_coordinates(city, street, country)\n",
    "            \n",
    "            if lat is None or lon is None:\n",
    "                print(f\"‚ö†Ô∏è Sensor {sensor_id}: cannot geocode location\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"Geocoding failed\"))\n",
    "                continue\n",
    "\n",
    "            # Clean and prepare air quality data \n",
    "            aq_df = cleaning.clean_and_append_data(\n",
    "                aq_df_raw, sensor_id, \n",
    "                city=city, street=street, country=country,\n",
    "                latitude=lat, longitude=lon, aqicn_url=working_feed_url\n",
    "            )\n",
    "            aq_df = aq_df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "            # Add features one by one, checking for duplicates after each\n",
    "            aq_df = feature_engineering.add_lagged_features(aq_df, \"pm25\", lags=[1,2,3])\n",
    "            aq_df = feature_engineering.add_rolling_window_feature(aq_df, window_days=3, column=\"pm25\", new_column=\"pm25_rolling_3d\")\n",
    "            \n",
    "            # Calculate nearby sensor feature using location dict\n",
    "            if len(sensor_locations) > 0:\n",
    "                aq_df = feature_engineering.add_nearby_sensor_feature(\n",
    "                    aq_df, \n",
    "                    sensor_locations,  # Pass dict instead of DataFrame\n",
    "                    n_closest=3\n",
    "                )\n",
    "            else:\n",
    "                aq_df[\"pm25_nearby_avg\"] = 0.0\n",
    "            \n",
    "            # Date range for weather\n",
    "            end_date = aq_df[\"date\"].max().date()\n",
    "            start_date = end_date - timedelta(days=365 * 3)\n",
    "\n",
    "            # Fetch weather\n",
    "            weather_df = fetchers.get_historical_weather(\n",
    "                sensor_id, start_date, end_date, lat, lon\n",
    "            )\n",
    "            \n",
    "            if weather_df is None or len(weather_df) == 0:\n",
    "                print(f\"‚ö†Ô∏è No weather data for sensor {sensor_id}\")\n",
    "                failed += 1\n",
    "                failed_sensors.append((sensor_id, \"No weather data\"))\n",
    "                continue\n",
    "\n",
    "            # Prepare weather data\n",
    "            weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]).dt.tz_localize(None)\n",
    "            weather_df[\"sensor_id\"] = int(sensor_id)\n",
    "            weather_df = weather_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"temperature_2m_mean\": \"float64\",\n",
    "                \"precipitation_sum\": \"float64\",\n",
    "                \"wind_speed_10m_max\": \"float64\",\n",
    "                \"wind_direction_10m_dominant\": \"float64\",\n",
    "            })\n",
    "            # Insert without triggering materialization\n",
    "            weather_fg.insert(weather_df, write_options={\"start_offline_materialization\": False})\n",
    "\n",
    "\n",
    "            # Prepare air quality data - DEBUGGING\n",
    "            print(f\"\\nüîç DEBUG: Sensor {sensor_id}\", flush=True)\n",
    "            print(f\"   Columns in aq_df: {aq_df.columns.tolist()}\", flush=True)\n",
    "            print(f\"   'date' count in aq_df: {aq_df.columns.tolist().count('date')}\", flush=True)\n",
    "            \n",
    "            all_fg_columns = [f.name for f in air_quality_fg.features]\n",
    "            print(f\"   Feature group columns: {all_fg_columns}\", flush=True)\n",
    "            print(f\"   'date' in FG columns: {'date' in all_fg_columns}\", flush=True)\n",
    "            \n",
    "            # Check what columns are actually available in aq_df\n",
    "            available_columns = [col for col in all_fg_columns if col in aq_df.columns]\n",
    "            print(f\"   Available columns in aq_df: {available_columns}\", flush=True)\n",
    "            \n",
    "            # Select columns (including date from the original df)\n",
    "            aq_insert_df = aq_df[available_columns].copy()\n",
    "            print(f\"   Columns after selection: {aq_insert_df.columns.tolist()}\", flush=True)\n",
    "            print(f\"   'date' count after selection: {aq_insert_df.columns.tolist().count('date')}\", flush=True)\n",
    "            \n",
    "            # Ensure date is properly formatted\n",
    "            aq_insert_df[\"date\"] = pd.to_datetime(aq_insert_df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "            # Ensure proper dtypes\n",
    "            aq_insert_df = aq_insert_df.astype({\n",
    "                \"sensor_id\": \"int32\",\n",
    "                \"pm25\": \"float64\",\n",
    "                \"pm25_lag_1d\": \"float64\",\n",
    "                \"pm25_lag_2d\": \"float64\",\n",
    "                \"pm25_lag_3d\": \"float64\",\n",
    "                \"pm25_rolling_3d\": \"float64\",\n",
    "                \"pm25_nearby_avg\": \"float64\",\n",
    "                \"city\": \"string\",\n",
    "                \"street\": \"string\",\n",
    "                \"country\": \"string\",\n",
    "                \"aqicn_url\": \"string\",\n",
    "                \"latitude\": \"float64\",\n",
    "                \"longitude\": \"float64\",\n",
    "            })\n",
    "\n",
    "            # Sort and verify\n",
    "            aq_insert_df = aq_insert_df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "            # Final verification\n",
    "            print(f\"   FINAL columns to insert: {aq_insert_df.columns.tolist()}\", flush=True)\n",
    "            print(f\"   FINAL 'date' count: {aq_insert_df.columns.tolist().count('date')}\", flush=True)\n",
    "            print(f\"   FINAL shape: {aq_insert_df.shape}\\n\", flush=True)\n",
    "\n",
    "            # Insert without triggering materialization\n",
    "            air_quality_fg.insert(aq_insert_df, write_options={\"start_offline_materialization\": False})\n",
    "\n",
    "\n",
    "\n",
    "            existing_sensors.add(sensor_id)\n",
    "            \n",
    "            # Add this sensor's location to dict for subsequent nearby calculations\n",
    "            sensor_locations[sensor_id] = (lat, lon, city, street, country)\n",
    "            \n",
    "            successful += 1\n",
    "            print(f\"‚úÖ Sensor {sensor_id} ({successful}/{remaining} complete)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_sensors.append((sensor_id, f\"{type(e).__name__}: {str(e)[:100]}\"))\n",
    "            print(f\"‚ùå Sensor {sensor_id}: {type(e).__name__}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüéâ Backfill complete!\")\n",
    "    print(f\"üìä Final Summary:\")\n",
    "    print(f\"   ‚úÖ Successfully processed: {successful}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}\")\n",
    "    print(f\"   ‚è© Skipped (already processed): {skipped}\")\n",
    "    print(f\"   üìà Total in feature store: {len(existing_sensors)}/{total_sensors}\")\n",
    "\n",
    "    if len(failed_sensors) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed Sensors Detail:\")\n",
    "        for sid, reason in failed_sensors:\n",
    "            print(f\"   ‚Ä¢ Sensor {sid}: {reason}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ All sensors already processed. No backfill needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cb2e3",
   "metadata": {},
   "source": [
    "## 1.5. Update Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hopsworks_admin.update_air_quality_description(air_quality_fg)\n",
    "# hopsworks_admin.update_weather_description(weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bae89d",
   "metadata": {},
   "source": [
    "## 1.6. Add Validation to Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a87d6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing expectation suite for FG 'air_quality'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1952082\n",
      "Saved expectation suite for FG 'air_quality'.\n",
      "Deleted existing expectation suite for FG 'weather'.\n",
      "Attached expectation suite to Feature Group, edit it at https://c.app.hopsworks.ai:443/p/1279184/fs/1265800/fg/1945998\n",
      "Saved expectation suite for FG 'weather'.\n"
     ]
    }
   ],
   "source": [
    "aq_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"aq_expectation_suite\"\n",
    ")\n",
    "\n",
    "# pm25 should be >= 0\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"pm25\",\n",
    "            \"min_value\": 0.0,\n",
    "            \"max_value\": None,\n",
    "            \"strict_min\": False,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# sensor_id + date should be unique (PK)\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_compound_columns_to_be_unique\",\n",
    "        kwargs={\"column_list\": [\"sensor_id\", \"date\"]},\n",
    "    )\n",
    ")\n",
    "\n",
    "# rolling + lag features should be numeric (float or int)\n",
    "for col in [\"pm25_rolling_3d\", \"pm25_lag_1d\", \"pm25_lag_2d\", \"pm25_lag_3d\"]:\n",
    "    aq_expectation_suite.add_expectation(\n",
    "        gx.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "aq_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "        kwargs={\"min_value\": 1, \"max_value\": None}\n",
    "    )\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(air_quality_fg, aq_expectation_suite)\n",
    "\n",
    "\n",
    "weather_expectation_suite = gx.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(   \n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"date\",\n",
    "            \"type_list\": [\"datetime64\", \"Datetime\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Temperature column - allow nulls, should be within physical range\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"min_value\": -80,\n",
    "            \"max_value\": 60,\n",
    "            \"mostly\": 1.0,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"temperature_2m_mean\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Precipitation column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"min_value\": -0.1,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"precipitation_sum\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wind column - should be >= 0, allow nulls\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"min_value\": 0,\n",
    "            \"max_value\": None,\n",
    "            \"mostly\": 1.0,          # allow nulls\n",
    "        },\n",
    "    )\n",
    ")\n",
    "weather_expectation_suite.add_expectation(\n",
    "    gx.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_type_list\",\n",
    "        kwargs={\n",
    "            \"column\": \"wind_speed_10m_max\",\n",
    "            \"type_list\": [\"float64\", \"Float64\", \"Int64\", \"Null\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "gx.core.ExpectationConfiguration(\n",
    "    expectation_type=\"expect_table_row_count_to_be_between\",\n",
    "    kwargs={\"min_value\": 1, \"max_value\": None}\n",
    ")\n",
    "\n",
    "hopsworks_admin.save_or_replace_expectation_suite(weather_fg, weather_expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337de93",
   "metadata": {},
   "source": [
    "## 1.7. Create Complete Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8c195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "    # Select specific columns from weather_fg, excluding 'date' and 'sensor_id' since they're join keys\n",
    "    weather_features = [f.name for f in weather_fg.features if f.name not in ['date', 'sensor_id']]\n",
    "    \n",
    "    query = (\n",
    "        air_quality_fg.select_all()\n",
    "        .join(weather_fg.select(weather_features), on=[\"sensor_id\", \"date\"])\n",
    "    )\n",
    "\n",
    "    fv = fs.get_or_create_feature_view(\n",
    "        name=\"air_quality_complete_fv\",\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"pm25\"]\n",
    "    )\n",
    "\n",
    "    return fv\n",
    "\n",
    "air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_feature_view(fs, air_quality_fg, weather_fg):\n",
    "#     query = (\n",
    "#         air_quality_fg.select_all()\n",
    "#         .join(weather_fg.select_all(), on=[\"sensor_id\", \"date\"])\n",
    "#     )\n",
    "\n",
    "#     fv = fs.get_or_create_feature_view(\n",
    "#         name=\"air_quality_complete_fv\",\n",
    "#         version=1,\n",
    "#         query=query,\n",
    "#         labels=[\"pm25\"]\n",
    "#     )\n",
    "\n",
    "#     return fv\n",
    "\n",
    "# air_quality_fv = create_feature_view(fs, air_quality_fg, weather_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452bc36",
   "metadata": {},
   "source": [
    "## 1.8. Trigger Offline Feature Store Materialization\n",
    "After backfilling data, materialize the feature groups to populate the offline feature store for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd447480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting materialization jobs for feature groups...\n",
      "2026-01-28 13:13:58,857 WARNING: UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1279184/jobs/named/air_quality_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=None) to trigger the materialization job again.\n",
      "\n",
      "‚úÖ Air quality feature group materialization started\n",
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279184/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "‚úÖ Weather feature group materialization started\n",
      "\n",
      "üìù Note: Materialization jobs run asynchronously. Check Hopsworks UI for status.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Starting materialization jobs for feature groups...\")\n",
    "\n",
    "try:\n",
    "    # Trigger materialization for air quality feature group\n",
    "    air_quality_job = air_quality_fg.materialization_job\n",
    "    air_quality_job.run(await_termination=False)\n",
    "    print(f\"‚úÖ Air quality feature group materialization started\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Air quality materialization: {e}\")\n",
    "\n",
    "try:\n",
    "    # Trigger materialization for weather feature group\n",
    "    weather_job = weather_fg.materialization_job\n",
    "    weather_job.run(await_termination=False)\n",
    "    print(f\"‚úÖ Weather feature group materialization started\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Weather materialization: {e}\")\n",
    "\n",
    "print(\"\\nüìù Note: Materialization jobs run asynchronously. Check Hopsworks UI for status.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741db4f",
   "metadata": {},
   "source": [
    "## 1.9. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fb2796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_quality_df = air_quality_fg.read()\n",
    "# weather_df = weather_fg.read()\n",
    "\n",
    "# # Extract unique sensor metadata from air quality feature group\n",
    "# metadata_df = air_quality_df[[\"sensor_id\", \"city\", \"street\", \"country\", \"latitude\", \"longitude\"]].drop_duplicates(subset=[\"sensor_id\"])\n",
    "# print(f\"üìç Extracted metadata for {len(metadata_df)} unique sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9222740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üîç AIR QUALITY DATA EXPLORATION\")\n",
    "# print(\"=\"*40)\n",
    "\n",
    "# print(f\"Shape: {air_quality_df.shape}\")\n",
    "# print(f\"Date range: {air_quality_df['date'].min().date()} to {air_quality_df['date'].max().date()}\")\n",
    "# print(f\"Number of unique sensors: {air_quality_df['sensor_id'].nunique()}\")\n",
    "# print(f\"Countries: {metadata_df['country'].unique()}\")\n",
    "# print(f\"Cities: {metadata_df['city'].nunique()} unique cities\")\n",
    "\n",
    "# print(\"\\nüìä PM2.5 Statistics:\")\n",
    "# print(air_quality_df['pm25'].describe())\n",
    "# print(f\"Missing values: {air_quality_df['pm25'].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüìà Engineered Features Statistics:\")\n",
    "# for col in ['pm25_rolling_3d', 'pm25_lag_1d', 'pm25_lag_2d', 'pm25_lag_3d', 'pm25_nearby_avg']:\n",
    "#     if col in air_quality_df.columns:\n",
    "#         missing = air_quality_df[col].isna().sum()\n",
    "#         print(f\"{col}: {missing} missing values ({missing/len(air_quality_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab388f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üå§Ô∏è WEATHER DATA EXPLORATION\") \n",
    "# print(\"=\"*40)\n",
    "\n",
    "# print(f\"Shape: {weather_df.shape}\")\n",
    "# print(f\"Date range: {weather_df['date'].min().date()} to {weather_df['date'].max().date()}\")\n",
    "# print(f\"Number of unique sensors: {metadata_df['sensor_id'].nunique()}\")\n",
    "\n",
    "# print(\"\\nüå°Ô∏è Weather Statistics:\")\n",
    "# for col in ['temperature_2m_mean', 'precipitation_sum', 'wind_speed_10m_max', 'wind_direction_10m_dominant']:\n",
    "#     if col in weather_df.columns:\n",
    "#         print(f\"{col}:\")\n",
    "#         print(f\"  Range: {weather_df[col].min():.2f} to {weather_df[col].max():.2f}, Mean: {weather_df[col].mean():.2f}, Missing: {weather_df[col].isna().sum()}\")\n",
    "\n",
    "# print(\"\\nüìç Geographic Coverage:\")\n",
    "# print(f\"Latitude range: {metadata_df['latitude'].min():.3f} to {metadata_df['latitude'].max():.3f}, Longitude range: {metadata_df['longitude'].min():.3f} to {metadata_df['longitude'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "129aa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üîó DATA QUALITY & RELATIONSHIPS\")\n",
    "# print(\"=\"*40)\n",
    "\n",
    "# # Overall data completeness\n",
    "# sensor_day_counts = air_quality_df.groupby('sensor_id')['date'].count()\n",
    "# total_records = len(air_quality_df)\n",
    "# data_completeness = (1 - air_quality_df['pm25'].isna().sum() / total_records) * 100\n",
    "\n",
    "# print(f\"üìä Overall Data Quality:\")\n",
    "# print(f\"Total records: {total_records:,}\")\n",
    "# print(f\"Data completeness: {data_completeness:.1f}%\")\n",
    "# print(f\"Days per sensor - Min: {sensor_day_counts.min()}, Median: {sensor_day_counts.median():.0f}, Max: {sensor_day_counts.max()}\")\n",
    "# print(f\"Sensors with <30 days: {(sensor_day_counts < 30).sum()}, >365 days: {(sensor_day_counts > 365).sum()}\")\n",
    "\n",
    "# # Extreme values summary\n",
    "# extreme_count = (air_quality_df['pm25'] > 100).sum()\n",
    "# very_high_count = (air_quality_df['pm25'] > 50).sum()\n",
    "# print(f\"\\n‚ö†Ô∏è Air Quality Levels:\")\n",
    "# print(f\"Extreme readings (>100 Œºg/m¬≥): {extreme_count} ({extreme_count/total_records*100:.1f}%)\")\n",
    "# print(f\"Very high readings (>50 Œºg/m¬≥): {very_high_count} ({very_high_count/total_records*100:.1f}%)\")\n",
    "\n",
    "# # Seasonal patterns\n",
    "# if len(air_quality_df) > 0:\n",
    "#     # Create temporary month column without modifying original DataFrame\n",
    "#     temp_months = pd.to_datetime(air_quality_df['date']).dt.month\n",
    "#     monthly_pm25 = air_quality_df.groupby(temp_months)['pm25'].mean()\n",
    "#     print(f\"\\nüóìÔ∏è Seasonal Patterns (PM2.5 Œºg/m¬≥):\")\n",
    "#     seasons = {(12,1,2): \"Winter\", (3,4,5): \"Spring\", (6,7,8): \"Summer\", (9,10,11): \"Autumn\"}\n",
    "#     for months, season in seasons.items():\n",
    "#         season_avg = monthly_pm25[monthly_pm25.index.isin(months)].mean()\n",
    "#         print(f\"  {season}: {season_avg:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
